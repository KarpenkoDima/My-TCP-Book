# Оценка материала

**Created:** 2/4/2026 22:11:32  
**Updated:** 2/8/2026 11:52:28  
**Exported:** 2/8/2026 12:40:07  
**Link:** [https://claude.ai/chat/0efe4ab6-e9d2-47e6-8b6e-2f1e9c5ce352](https://claude.ai/chat/0efe4ab6-e9d2-47e6-8b6e-2f1e9c5ce352)  

## Prompt:
04.02.2026, 22:11:34

> File: 

Оцени матеиал



## Response:
04.02.2026, 22:12:18

````plaintext
Thought process: Разработал критерии оценки технического материала о Linux сетях.

Пользователь попросил меня оценить материал на русском языке. Это технический материал о том, как работает сетевой стек Linux на низком уровне - от физического уровня приема пакетов до обработки в ядре. Материал называется "Модуль 1: Физика и Ядро (Где реально умирают пакеты)".

Мне нужно оценить этот материал. Я должен учитывать стиль пользователя, который просит меня давать ясные, глубокие объяснения, как учитель, разбивая сложные концепции на части.

Давайте проанализирую материал по нескольким критериям:

1. Техническая точность
2. Глубина изложения
3. Структура и логика
4. Педагогическая ценность
5. Практическая применимость
6. Стиль и подача

Материал выглядит технически продвинутым, написан для аудитории с опытом в системном программировании и сетях. Рассмотрю каждый аспект детально.
````

Этот материал производит сильное впечатление по нескольким причинам, и я хочу разобрать его с разных сторон, чтобы вы понимали его истинную ценность.

## Сильные стороны материала

**Глубина технического погружения** - это действительно редкий уровень детализации. Автор не останавливается на абстракциях вроде "пакет попадает в сеть", а показывает весь путь от электрического сигнала до структур данных в памяти. Объяснение DMA, Ring Buffers, механизма прерываний и структуры sk_buff дает понимание того, что происходит "под капотом" на уровне, который обычно скрыт от разработчиков.

**Практическая применимость** материала впечатляет. Автор не просто рассказывает теорию, но и дает конкретные команды диагностики (ethtool, /proc/interrupts, /proc/softirqs). Это превращает материал из академического в прикладной - вы сразу можете проверить эти концепции на своей системе. Раздел с практическими заданиями особенно ценен, потому что он заставляет читателя перейти от пассивного чтения к активному исследованию.

**Структура повествования** логична и последовательна. Автор ведет читателя по пути пакета - от момента прибытия на физический порт через DMA, прерывания, SoftIRQ к конечным структурам данных. Это создает ментальную модель, которую легко удержать в голове.

## Области, требующие осторожности

**Тон и стиль** материала весьма агрессивен ("Это ложь", "Hardcore Engineering"). Для некоторых аудиторий это может быть мотивирующим, но для других - отталкивающим. Материал позиционирует себя как нечто для "настоящих профессионалов", что может создать психологический барьер для тех, кто только начинает погружаться в эту тему.

**Предполагаемый уровень знаний** очень высок. Автор предполагает, что читатель уже знаком с концепциями прерываний, PCIe, кэш-иерархии CPU, и может интерпретировать вывод системных команд. Для человека без системного бэкграунда многие объяснения могут остаться непонятными несмотря на детализацию.

**Баланс между широтой и глубиной** иногда нарушается. Например, раздел о kernel bypass (DPDK/XDP) вводит совершенно новые концепции в конце модуля, когда читатель уже перегружен информацией. Возможно, это стоило бы вынести в отдельный модуль или дать больше контекста о том, когда эти технологии действительно нужны.

## Техническая точность

С технической точки зрения материал точен, насколько я могу судить о современном Linux networking stack. Описание NAPI, interrupt coalescing, RSS/RPS/RFS соответствует действительности. Автор правильно указывает на trade-offs - например, что RPS имеет большой overhead из-за межпроцессорного взаимодействия, или что аппаратный LRO может ломать заголовки.

Особенно ценно упоминание реальных проблем производительности - cache thrashing при неправильной настройке affinity, memory pressure от аллокации sk_buff, interrupt storm на высоких скоростях. Это те вещи, которые действительно убивают производительность в продакшене.

## Чего не хватает

Материалу не хватает **визуализации**. Диаграммы, показывающие поток данных от NIC через DMA в память, схемы работы RSS с несколькими очередями, или временные диаграммы обработки прерываний значительно улучшили бы понимание.

Также отсутствуют **конкретные примеры проблем и их решений**. Например: "Мы видели, что один core загружен на 100% в ksoftirqd. Мы настроили RSS так-то и получили такой-то результат". Кейс-стади сделали бы материал более убедительным.

## Общая оценка

Это **отличный материал для среднего или продвинутого уровня**, для людей, которые уже имеют опыт работы с Linux networking и хотят понять, что происходит на более глубоком уровне. Материал особенно полезен для:

- Системных инженеров, настраивающих высоконагруженные сервисы
- Разработчиков сетевого ПО, которым нужно понимать bottlenecks
- Специалистов по производительности, ищущих источники задержек

Для новичков в системном программировании этот материал будет слишком сложным без дополнительного контекста. Им нужно было бы сначала изучить основы работы операционных систем, прерываний, и базовую архитектуру TCP/IP.

Если бы я ставил оценку по десятибалльной шкале для целевой аудитории (опытные инженеры), я бы дал **8 из 10** - высокая техническая ценность, практичность и глубина, но с некоторыми шероховатостями в подаче и структуре.



## Prompt:
04.02.2026, 22:32:50

> File: 

https://www.waveform.com/tools/bufferbloat?test-id=8c4c7c64-79cf-4ace-8a3f-a980e5c78c6c



## Response:
04.02.2026, 22:33:58

````plaintext
Thought process: Analyzed networking module quality and prepared constructive feedback.

Пользователь прислал второй модуль материала о сетевых технологиях и ссылку на тест bufferbloat. Он просит оценить материал (судя по контексту первого сообщения). 

Давайте посмотрим на ссылку и сам материал:

Модуль 2 посвящен Bufferbloat и Congestion Control. Это продолжение первого модуля о низкоуровневой обработке пакетов в Linux.

Мне нужно:
1. Оценить качество второго модуля
2. Возможно, сравнить его с первым модулем
3. Дать конструктивную критику

Особенности второго модуля:
- Фокус на проблеме bufferbloat
- Объяснение TCP BBR vs CUBIC
- Практические решения (AQM, SQM, FQ_CoDel, CAKE)
- Интеграция с практическим тестом (Waveform)
- Домашнее задание с конкретной целью

Стиль остался таким же - агрессивно-мотивирующий, но более прикладной.

У пользователя есть style preference - он хочет глубокие объяснения, как учитель, разбивая на части, с примерами, в прозе.

Мне нужно оценить этот материал, сравнить с первым модулем, и дать полезную обратную связь.
````

Второй модуль представляет собой интересную эволюцию курса, и я вижу несколько важных изменений по сравнению с первым материалом, которые стоит обсудить детально.

## Изменение фокуса и подхода

Если первый модуль был сосредоточен на "как работает машина" (железо, ядро, драйверы), то второй модуль переходит к "зачем это важно для пользователя". Это умный педагогический ход. Автор начал с фундамента - показал механику обработки пакетов на уровне железа и ядра. Теперь он поднимается на уровень выше и демонстрирует, как эти низкоуровневые механизмы влияют на то, что видит конечный пользователь - на задержки в играх, на скачки пинга при загрузке файлов.

Особенно удачна интеграция реального теста от пользователя. Вместо абстрактного объяснения bufferbloat автор привязывается к конкретным цифрам, которые человек только что увидел на своем соединении. Это создает непосредственную связь между теорией и практикой, что сильно повышает мотивацию разобраться в материале.

## Качество объяснений

Аналогия с очередью в супермаркете работает превосходно. Она берет абстрактную концепцию буферизации пакетов и превращает ее в нечто, что каждый испытывал на себе. Когда автор говорит "буфер - это место, где пакеты ждут своей смерти", он создает яркий образ, который остается в памяти. Сравнение CUBIC с "слепым водителем, который жмет газ до столкновения" против BBR как "адаптивного круиз-контроля" дает интуитивное понимание разницы алгоритмов без погружения в математические формулы.

Однако здесь есть и слабость. Автор объясняет *что* делает BBR ("измеряет скорость потока, измеряет дистанцию, держит идеальную дистанцию"), но не объясняет *как* именно это работает на практике. Что значит "измеряет Max BW"? Как TCP-соединение может узнать максимальную пропускную способность канала без заполнения буферов? Это ключевой момент BBR, и его отсутствие оставляет концептуальную дыру в понимании.

## Практическая применимость

Второй модуль значительно сильнее первого в плане практической применимости для обычного пользователя. Команды для включения BBR предельно просты и работают на любом современном Linux-сервере. Объяснение SQM с конкретной целью (получить A+ в тесте Waveform) дает четкую метрику успеха. Это гораздо конкретнее, чем "изучите вывод команд" из первого модуля.

Особенно ценен трюк с искусственным ограничением скорости до девяноста пяти процентов от максимальной. Это контринтуитивная идея - намеренно замедлить соединение, чтобы улучшить его работу. Автор объясняет логику ("вы должны стать бутылочным горлышком искусственно"), но мог бы развить эту мысль глубже. Почему именно девяносто пять процентов? Что происходит, если поставить восемьдесят процентов или девяносто восемь? Как найти оптимальное значение для своего соединения?

## Структурные проблемы

Переход от BBR к AQM кажется несколько резким. Автор говорит "если вы не контролируете сервер", но не объясняет, почему это типичная ситуация. Большинство людей действительно не контролируют серверы, с которыми они общаются (Netflix, игровые серверы, веб-сайты), поэтому решение на стороне роутера зачастую единственное доступное. Это стоило бы подчеркнуть сильнее в начале раздела.

Эволюция алгоритмов управления очередями (Drop Tail → RED → CoDel) дана очень кратко. Для понимания того, почему CoDel революционен, полезно было бы объяснить, почему предыдущие подходы не работали. Что такое "глобальная синхронизация TCP"? Почему RED сложно настраивать? Эти детали помогли бы оценить прогресс.

## Глубина против доступности

Второй модуль более доступен, чем первый. Он требует меньше предварительных знаний о работе операционной системы. Человек с базовым пониманием сетей может понять концепцию bufferbloat и применить решения. Это хорошо для расширения аудитории, но может разочаровать тех, кто ожидал продолжения "hardcore engineering" с погружением в код ядра и структуры данных.

Материал не показывает, например, как FQ_CoDel реализован в ядре, какие структуры данных он использует для разделения потоков, как именно работает хеширование для определения принадлежности пакета к потоку. Для продвинутой аудитории, которая осилила первый модуль, это могло бы быть интересно.

## Отсутствующие детали

Модуль не затрагивает важные нюансы. Например, что происходит, когда BBR встречается с классическим CUBIC на одном узком канале? Есть ли проблемы справедливости (fairness)? Известно, что ранние версии BBR могли быть агрессивными по отношению к другим потокам. Это практически важно, потому что в реальном интернете ваше BBR-соединение будет конкурировать с множеством CUBIC-соединений.

Также не обсуждается взаимодействие BBR с SQM. Если на сервере включен BBR (который старается не создавать очереди), а на роутере включен SQM (который активно управляет очередями), как они работают вместе? Дополняют ли друг друга или возникает конфликт логик?

## Домашнее задание

Практическое задание сформулировано хорошо - есть четкая цель (A+ в тесте) и метрика (сравнение Loaded Latency до и после). Однако задание предполагает, что у читателя есть Linux-роутер или OpenWrt, что не очень типично для домашних пользователей. Было бы полезно дать альтернативные инструкции для тех, у кого обычный потребительский роутер, или объяснить, как проверить эффект BBR на стороне клиента при работе с серверами, которые его поддерживают.

## Сравнение с первым модулем

Первый модуль был более однородным - он держался на одном уровне абстракции (ядро и железо) и последовательно вел читателя через путь пакета. Второй модуль прыгает между уровнями - от TCP-алгоритмов на отправителе к очередям на роутере, от теории к практике, от Google BBR к домашнему OpenWrt. Это делает его менее цельным, но более практически полезным.

Тон второго модуля стал мягче. Меньше "это ложь" и "главный враг", больше конструктивных объяснений и решений. Это хорошо, потому что материал перешел от диагностики проблем к их решению, и агрессивный тон здесь был бы менее уместен.

## Итоговая оценка

Если первый модуль я оценил на восемь из десяти для его целевой аудитории, то второй модуль я бы оценил на семь с половиной из десяти, но для более широкой аудитории. Он теряет немного в глубине и технической строгости, но приобретает в практической применимости и доступности. Это разумный компромисс, если цель курса - дать людям инструменты для улучшения их сетей, а не воспитать ядерных разработчиков.

Материал выиграл бы от добавления диаграмм, показывающих поведение очередей при разных алгоритмах, и от более глубокого объяснения того, как BBR измеряет пропускную способность без заполнения буферов. Также полезно было бы обсудить ограничения и trade-offs каждого подхода более явно.

В целом, это солидное продолжение курса, которое успешно соединяет низкоуровневые знания из первого модуля с высокоуровневыми решениями пользовательских проблем.



## Prompt:
04.02.2026, 22:37:14

перепиши так чтобы до 10/10 (это для продвинутых материал)



## Response:
04.02.2026, 22:40:22

````plaintext
Thought process: Architected advanced technical deepening with kernel-level mechanisms and mathematical rigor.

Пользователь хочет, чтобы я переписал второй модуль так, чтобы он достиг уровня 10/10 для продвинутой аудитории. Это означает, что нужно:

1. Углубить техническую детализацию до уровня первого модуля
2. Объяснить внутренние механизмы BBR - как именно он измеряет BW и RTT
3. Добавить детали реализации FQ_CoDel на уровне ядра
4. Обсудить проблемы fairness, взаимодействие BBR с CUBIC
5. Показать структуры данных и алгоритмы
6. Сохранить практичность, но добавить глубину понимания "почему"
7. Добавить больше диагностических команд и внутренностей
8. Объяснить математику и логику за алгоритмами
9. Обсудить trade-offs более явно
10. Добавить больше деталей о том, как это работает на уровне кода ядра

При этом нужно сохранить стиль "hardcore engineering" из первого модуля и использовать стиль письма из userStyle - полные предложения, prose, объяснения как учитель.
````

# Модуль 2: Bufferbloat и Congestion Control (Война за очередь)

*«Буфер — это место, где пакеты ждут своей смерти, а латентность — это время их агонии».*

В первом модуле мы разобрали, как ядро обрабатывает входящий пакет от момента DMA до `sk_buff`. Теперь мы займемся обратным направлением и гораздо более сложной проблемой: как TCP-отправитель решает, **когда** выстрелить следующий пакет в сеть, не зная точно, что происходит на другом конце провода.

## Часть 2.1: Анатомия Bufferbloat — Где умирает отзывчивость

Ваш тест Waveform показал классический симптом: латентность под нагрузкой взлетает до сотен миллисекунд. Это не случайность и не «плохой провайдер». Это фундаментальное следствие того, как работает TCP.

### Почему буферы вообще существуют

Представьте, что пакеты приходят на роутер со скоростью 10 Гбит/с, а исходящий канал всего 1 Гбит/с. Без буфера каждый десятый пакет будет дропаться мгновенно. TCP увидит потери, упадет в congestion avoidance, пропускная способность рухнет. Буфер сглаживает эти микровсплески трафика, позволяя временно накопить пакеты и отправить их по мере освобождения канала.

Проблема в том, что производители железа решили: если один мегабайт буфера — хорошо, то сто мегабайт — в сто раз лучше. Это логическая ошибка. В реальности огромные буферы превращают микросекундные всплески в секундные очереди. Ваш ICMP-пакет (пинг) встает в конец очереди за мегабайтами вашего же торрент-трафика и ждет своей участи.

### Математика задержки: Little's Law

Существует фундаментальная теорема теории очередей, которую нужно понять интуитивно. Средняя задержка в системе равна количеству элементов в системе, деленному на скорость обработки: `Latency = Queue_Size / Service_Rate`.

Если у вас буфер на десять мегабайт и канал на десять мегабит в секунду, в худшем случае пакет проведет в очереди восемь секунд. Да, восемь секунд задержки, хотя физическая скорость света до сервера и обратно это десятки миллисекунд. Буфер съел всю отзывчивость.

## Часть 2.2: TCP Congestion Control — Игра вслепую

TCP работает в условиях неопределенности. Отправитель не знает точную пропускную способность канала, не знает размер буферов по пути, не видит очереди. Ему приходится **угадывать**, используя единственный сигнал — подтверждения (ACK) от получателя.

### CUBIC: Алгоритм эпохи глупых буферов

CUBIC (дефолт в Linux до недавнего времени) работает по принципу аддитивного увеличения и мультипликативного уменьшения. Он увеличивает окно перегрузки (congestion window) по кубической функции до тех пор, пока не увидит потерю пакета. Потеря интерпретируется как «я переполнил буфер», окно режется пополам, и процесс начинается заново.

Звучит разумно, но есть фундаментальная проблема. CUBIC использует потерю пакета как сигнал перегрузки. Это означает, что он **должен** заполнить все буферы по пути до отказа, чтобы получить этот сигнал. Если буфер огромный (сто мегабайт на вашем домашнем роутере), CUBIC с радостью забьет его полностью, прежде чем понять, что надо остановиться.

Давайте посмотрим на код. В ядре Linux функция `cubictcp_cong_avoid` увеличивает окно так:

```c
// Упрощенная логика CUBIC
if (tcp_in_slow_start(tp)) {
    // Slow start: экспоненциальный рост
    tcp_slow_start(tp);
} else {
    // Congestion avoidance: кубический рост
    // W(t) = C(t - K)³ + W_max
    // где K - время с последней потери
    u32 delta = bictcp_clock() - ca->epoch_start;
    u64 offs = (delta * delta * delta) >> BICTCP_PARAM_SHIFT;
    
    cwnd = ca->last_max_cwnd + offs;
}
```

Обратите внимание: здесь нет ни одной строчки кода, которая смотрела бы на RTT (Round Trip Time). CUBIC слеп к задержке. Он видит только одно: дропнулся пакет или нет. Это означает, что он будет накачивать сеть данными, пока RTT не взлетит до небес от переполненных буферов, и лишь потеря пакета заставит его остановиться.

### Почему loss-based подход устарел

В девяностых годах, когда создавался TCP Reno (предок CUBIC), буферы были маленькими (килобайты), а каналы медленными (килобиты в секунду). Потеря пакета действительно означала перегрузку. Сегодня у вас гигабитный канал и мегабайтные буферы. Потеря означает «ты уже давно создал чудовищную очередь, вот тебе знак остановиться». Это реакция на симптом, а не на причину.

## Часть 2.3: BBR — Революция через измерение

Google BBR (Bottleneck Bandwidth and Round-trip propagation time) переворачивает логику. Вместо того чтобы ждать потери, BBR активно **измеряет** две величины: максимальную пропускную способность канала и минимальное время туда-обратно. Зная эти два числа, BBR вычисляет оптимальное количество данных, которое должно находиться в пути одновременно: `BDP = BtlBw × RTprop`.

BDP (Bandwidth-Delay Product) — это объем данных, который «помещается» в канал. Представьте трубу: ее диаметр это пропускная способность, длина это задержка. BDP это объем воды, который нужен, чтобы заполнить трубу от конца до конца. Ни больше, ни меньше. Если вы отправите больше — излишек скопится в очереди (буфере). Если меньше — канал будет простаивать.

### Как BBR измеряет BtlBw (Bottleneck Bandwidth)

Это гениально просто и невероятно сложно одновременно. BBR отслеживает скорость доставки данных (delivery rate), измеряя, сколько байт было подтверждено (ACK) за единицу времени. Ключевая идея: максимальная наблюдаемая скорость доставки и есть пропускная способность узкого места.

Почему? Если вы отправляете данные быстрее, чем пропускная способность, излишек идет в буфер, а скорость доставки остается постоянной (равной пропускной способности канала). Если вы отправляете медленнее, скорость доставки равна скорости отправки. Максимум из этих измерений за последние десять RTT и есть оценка BtlBw.

В коде ядра это выглядит примерно так:

```c
// bbr_update_bw: обновление оценки пропускной способности
static void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)
{
    struct bbr *bbr = inet_csk_ca(sk);
    u64 bw;

    // Вычисляем скорость доставки за этот RTT
    bw = (u64)rs->delivered * BW_UNIT;
    do_div(bw, rs->interval_us);
    
    // Сохраняем в скользящее окно максимумов
    if (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {
        bbr_lt_bw_sample(sk, bw);
        minmax_running_max(&bbr->bw, BBR_BW_RTTS, 
                          bbr->rtt_cnt, bw);
    }
}
```

Функция `minmax_running_max` ведет скользящее окно максимальных значений за последние десять RTT. Это фильтрует шум и дает устойчивую оценку.

### Как BBR измеряет RTprop (Round-trip propagation time)

Это проще. BBR ведет скользящее окно **минимальных** значений RTT за последние десять секунд. Минимум важен, потому что он соответствует состоянию «канал пуст, нет очередей». Любое RTT выше минимума означает, что где-то по пути копится очередь.

```c
// bbr_update_min_rtt: обновление оценки минимального RTT
static void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)
{
    struct bbr *bbr = inet_csk_ca(sk);
    bool filter_expired;

    // Обновляем минимум, если замерили меньше
    if (rs->rtt_us >= 0 && 
        (rs->rtt_us <= bbr->min_rtt_us || !bbr->min_rtt_us)) {
        bbr->min_rtt_us = rs->rtt_us;
        bbr->min_rtt_stamp = tcp_jiffies32;
    }
    
    // Сбрасываем оценку каждые 10 секунд
    filter_expired = after(tcp_jiffies32,
                          bbr->min_rtt_stamp + BBR_MIN_RTT_WIN_SEC * HZ);
    if (filter_expired) {
        bbr->min_rtt_us = rs->rtt_us;
        bbr->min_rtt_stamp = tcp_jiffies32;
    }
}
```

Сброс каждые десять секунд критичен. Если маршрут изменится (например, BGP перекинул вас на другой путь с большей задержкой), BBR должен это заметить и обновить оценку.

### Pacing: Сердце BBR

Зная BDP, BBR устанавливает целевое окно перегрузки равным `cwnd = BDP × gain`, где gain это коэффициент (обычно единица в установившемся режиме, но BBR периодически его меняет для зондирования). Но простого установления окна недостаточно. Нужен **pacing** — контроль скорости отправки пакетов.

BBR вычисляет интервал между пакетами: `pacing_interval = packet_size / (BtlBw × pacing_gain)`. Ядро использует планировщик пакетов `fq` (Fair Queueing), который откладывает отправку каждого пакета на точно рассчитанное время. Это превращает взрывной (bursty) трафик в равномерный поток.

Без `fq` BBR работает в режиме совместимости, где он полагается на стандартный byte-counting подход, и эффективность падает драматически. Именно поэтому в настройках мы всегда ставим:

```bash
net.core.default_qdisc=fq
net.ipv4.tcp_congestion_control=bbr
```

Планировщик `fq` не просто откладывает пакеты. Он ведет отдельную очередь для каждого потока (используя хеш от пятерки: src_ip, dst_ip, src_port, dst_port, protocol) и использует календарную очередь (timing wheel) для эффективного планирования. Это означает, что BBR-поток не может задавить другие потоки на том же интерфейсе.

### Проблема справедливости: BBR vs CUBIC

Ранние версии BBR (v1) были агрессивны. Когда BBR-поток встречался с CUBIC-потоком на одном узком канале, BBR мог забрать непропорционально большую долю пропускной способности. Причина в том, что BBR не снижает скорость при появлении очереди (пока не видит потерь), а CUBIC снижает. CUBIC отступает, создавая пространство, которое BBR жадно заполняет.

BBR v2 (доступен в ядрах пять точка восемнадцать и выше) решает эту проблему через механизм `ECN` (Explicit Congestion Notification) и явное обнаружение очередей. Когда BBR v2 замечает рост RTT выше минимума, он интерпретирует это как сигнал «я создаю очередь» и снижает пейсинг. Это делает его дружелюбнее к legacy-трафику.

Включить BBR v2:

```bash
echo 2 > /sys/module/tcp_bbr/parameters/version
```

## Часть 2.4: AQM — Умные очереди на роутере

Теперь переместимся на другую сторону проблемы. Даже если у вас идеальный BBR на сервере, домашний роутер или роутер провайдера все равно будет буферизировать пакеты. Если буфер глупый (Drop Tail), мы получим bufferbloat независимо от TCP-алгоритма.

### Drop Tail: Почему это катастрофа

Классический Drop Tail просто копит пакеты в FIFO-очередь до переполнения, потом дропает новые. Это вызывает несколько патологий:

Первая — TCP Global Synchronization. Когда буфер переполняется, дропаются пакеты нескольких потоков одновременно. Все эти потоки видят потерю, режут окно перегрузки в одно и то же время, скорость падает у всех синхронно. Потом все одновременно начинают разгоняться. Получается волнообразное использование канала: то пусто, то переполнено.

Вторая — Lock-out problem. Агрессивный поток может занять весь буфер своими пакетами, не оставив места для пакетов других потоков. Интерактивный SSH или DNS-запрос будут задушены массовой загрузкой.

### RED: Первая попытка быть умнее

Random Early Detection пытался решить синхронизацию через вероятностный дроп. Когда очередь заполнена наполовину, он начинает случайно дропать пакеты с возрастающей вероятностью. Это рассинхронизирует TCP-потоки.

Проблема RED в том, что он требует тонкой настройки порогов (`min_th`, `max_th`, `max_p`), которые зависят от пропускной способности канала и типичного RTT. Неправильная настройка делает его хуже Drop Tail. Он был слишком сложен для операторов и практически умер.

### CoDel: Революция через время, а не объем

Kathleen Nichols и Van Jacobson из Cisco предложили радикально иную идею. Вместо того чтобы измерять **размер** очереди в байтах или пакетах, измерять **время**, которое пакет проводит в очереди.

Логика CoDel проста до гениальности. Пакет попадает в очередь, и к нему прикрепляется timestamp. Когда пакет выходит из очереди, мы вычисляем sojourn time (время пребывания). Если оно превышает target (по умолчанию пять миллисекунд) и очередь не пустеет за interval (сто миллисекунд), CoDel начинает дропать пакеты.

Почему это работает? Потому что короткие всплески (микросекундные burst) не вызовут дропа — пакеты не успеют пролежать пять миллисекунд. Но устойчивая перегрузка, когда пакеты стоят в очереди долго, будет обнаружена и подавлена.

В коде ядра это реализовано в `net/sched/sch_codel.c`:

```c
static struct sk_buff *codel_dequeue(struct Qdisc *sch)
{
    struct codel_sched_data *q = qdisc_priv(sch);
    struct sk_buff *skb;
    codel_time_t now;

    skb = dequeue(sch); // Берем пакет из очереди
    if (!skb)
        return NULL;

    now = codel_get_time();
    // Вычисляем, сколько пакет пролежал
    sojourn_time = now - codel_get_enqueue_time(skb);

    if (sojourn_time < q->params.target || 
        sch->qstats.backlog <= q->params.mtu) {
        // Очередь хорошая, сбрасываем счетчик
        q->vars.dropping = false;
    } else {
        // Очередь плохая, решаем дропать
        if (!q->vars.dropping) {
            q->vars.dropping = true;
            q->vars.drop_next = now + q->params.interval;
        } else if (now >= q->vars.drop_next) {
            // Время дропа настало
            drop(skb);
            q->vars.count++;
            // Следующий дроп раньше (ускоряем дропы)
            q->vars.drop_next = control_law(q, now);
        }
    }
    
    return skb;
}
```

Ключевой момент — `control_law`. Он вычисляет, когда дропнуть следующий пакет. Чем дольше очередь остается переполненной, тем чаще дропы. Это адаптивная обратная связь, которая быстро снижает перегрузку.

### FQ_CoDel: Справедливость плюс контроль задержки

CoDel решает проблему bufferbloat, но не решает проблему справедливости между потоками. FQ_CoDel комбинирует Fair Queueing с CoDel.

Алгоритм создает хеш-таблицу очередей (по умолчанию одна тысяча двадцать четыре бакета). Каждый входящий пакет хешируется по пятерке (src, dst, ports, proto), и попадает в свою очередь. Планировщик обходит очереди по round-robin, беря по одному пакету из каждой непустой очереди. Внутри каждой очереди работает CoDel, дропающий пакеты при переполнении.

Результат: жирный поток (торрент) получает свою очередь, тонкий поток (SSH) получает свою. Round-robin гарантирует, что SSH-пакет не будет ждать за тысячами торрент-пакетов. Он проскочит вперед на следующей итерации планировщика.

Код в `net/sched/sch_fq_codel.c` показывает структуру:

```c
struct fq_codel_sched_data {
    struct fq_codel_flow *flows;  // Массив очередей
    u32 flows_cnt;                 // Количество бакетов (1024)
    u32 quantum;                   // Сколько байт взять из очереди за раз
    u32 drop_batch_size;
    struct codel_params cparams;   // Параметры CoDel
    // ...
};

static struct sk_buff *fq_codel_dequeue(struct Qdisc *sch)
{
    // Находим следующую непустую очередь (round-robin)
    flow = fq_codel_next_flow(q);
    if (!flow)
        return NULL;
    
    // Применяем CoDel к этой очереди
    skb = codel_dequeue(sch, flow, &q->cparams, &flow->cvars, now);
    
    return skb;
}
```

### CAKE: FQ_CoDel на стероидах

CAKE (Common Applications Kept Enhanced) это следующая эволюция, созданная Jonathan Morton специально для домашних роутеров. Он добавляет несколько критичных фич:

Первое — NAT-aware hashing. Обычный FQ_CoDel хеширует по IP-адресам и портам. Но за домашним роутером сотня устройств имеют один внешний IP (NAT). FQ_CoDel увидит все потоки как один. CAKE использует conntrack ядра, чтобы отслеживать реальные внутренние адреса и разделять потоки правильно.

Второе — ATM compensation. Многие DSL-каналы используют ATM (Asynchronous Transfer Mode) на физическом уровне, который добавляет оверхед к каждому пакету (пять байт на каждые сорок восемь байт полезной нагрузки). CAKE может учитывать это, корректируя расчет пропускной способности.

Третье — Triple-Isolate mode. CAKE может изолировать не только по потокам, но и по хостам (src IP), и по приоритету. Это означает, что один хост не может забить весь канал, даже если откроет тысячу соединений.

Настройка CAKE в OpenWrt:

```bash
tc qdisc replace dev eth1 root cake bandwidth 95mbit \
    nat dual-dsthost ingress wash ack-filter
```

Здесь `bandwidth` это девяносто пять процентов от вашей реальной скорости провайдера (критично важно!), `nat` включает NAT-aware режим, `dual-dsthost` изолирует по хостам в обоих направлениях, `ingress` указывает на входящий канал, `wash` очищает DSCP метки от провайдера, `ack-filter` фильтрует избыточные ACK для экономии пропускной способности.

## Часть 2.5: Почему искусственное ограничение скорости работает

Это контринтуитивно, но критично важно понять. Вы платите за сто мегабит, а я говорю поставить лимит в девяносто пять. Почему?

Причина в том, что буферизация происходит в точке перегрузки, в узком месте. Если узкое место это канал провайдера (сто мегабит), то очередь будет копиться в оборудовании провайдера, где у вас тупой Drop Tail буфер на сто мегабайт. Вы не можете контролировать эту очередь.

Но если вы искусственно создадите узкое место на своем роутере (девяносто пять мегабит), то очередь будет копиться у вас, где работает умный CAKE или FQ_CoDel. Пять процентов пропускной способности это цена за управление. Вы обмениваете пять мегабит максимальной скорости на стабильную, низкую латентность.

Математически это работает так. Трафик приходит к вам со средней скоростью R_avg, с всплесками до R_peak. Если R_peak > channel_capacity, пакеты будут буферизироваться. Вопрос в том, где. Установив лимит чуть ниже channel_capacity, вы гарантируете, что буферизация произойдет на вашем умном qdisc, а не на глупом буфере провайдера.

Оптимальное значение зависит от стабильности канала. Если провайдер гарантирует ровно сто мегабит всегда, можно поставить девяносто восемь. Если канал колеблется (например, DOCSIS-кабель), нужен больший запас — девяносто или даже восемьдесят пять процентов.

## Часть 2.6: Взаимодействие BBR и AQM

Когда BBR работает на сервере, а CAKE на вашем роутере, они дополняют друг друга идеально. BBR старается не создавать очередь, отправляя данные ровно на скорости BDP. CAKE следит, чтобы если очередь все же возникла (например, от множества потоков), она оставалась маленькой и справедливой.

Если BBR неправильно оценит пропускную способность (например, из-за изменения маршрута), CAKE поймает излишек трафика и даст сигнал через дроп. BBR увидит потерю (не от переполнения глубокого буфера, а от активного управления очереди) и быстро скорректирует оценку.

Это симбиоз. BBR минимизирует нагрузку на AQM, а AQM страхует от ошибок BBR и обеспечивает справедливость между множеством потоков.

## Практическое задание (Hardcore Edition)

Теперь настоящая работа для продвинутых.

**Задача один:** Включите BBR v2 на вашем Linux-сервере (или виртуалке). Запустите iperf3 тест с сервером, который использует CUBIC. Одновременно мониторьте RTT через ping. Сравните поведение: как быстро BBR заполняет канал, как меняется RTT под нагрузкой, видите ли вы разницу в fairness между v1 и v2.

**Задача два:** Установите на роутер OpenWrt и настройте CAKE на wan-интерфейсе. Критично важно: найдите правильное значение bandwidth через бинарный поиск. Начните с девяноста процентов, запустите тест Waveform. Если видите loaded latency выше десяти миллисекунд, снижайте на пять процентов. Если пропускная способность упала сильно (больше десяти процентов), увеличивайте на два процента. Ваша цель: Grade A плюс с минимальной потерей throughput.

**Задача три:** Погрузитесь в статистику CAKE. Команда `tc -s qdisc show dev eth1` покажет счетчики дропов, backlog, overlimits. Запустите тяжелую загрузку (торрент плюс стрим плюс игра) и смотрите, как CAKE распределяет пропускную способность. Видите ли вы равномерное распределение дропов между потоками?

**Задача четыре:** Изучите разницу между FQ_CoDel и CAKE в реальной нагрузке. Настройте один роутер с `tc qdisc add dev eth1 root fq_codel`, другой с CAKE. Запустите идентичные тесты на обоих. Зафиксируйте разницу в loaded latency для случая с NAT (много устройств за роутером). CAKE должен показать значительное преимущество из-за NAT-aware хеширования.

**Задача пять (для экстремалов):** Соберите ядро Linux с патчами tracepoint для BBR. Используйте eBPF и bpftrace для логирования состояния BBR в реальном времени:

```bash
bpftrace -e 'kprobe:tcp_bbr_main { 
    printf("BBR: mode=%d cwnd=%d pacing_rate=%d\n",
           ((struct bbr*)arg0)->mode,
           ((struct sock*)arg1)->sk_pacing_rate); 
}'
```

Это позволит вам видеть, как BBR переключается между режимами (STARTUP, DRAIN, PROBE_BW, PROBE_RTT) в реальном времени во время передачи данных.

---

**Следующий модуль:** Мы спустимся в `tc` (Traffic Control) и разберем, как именно ядро планирует отправку пакетов, что такое qdisc tree, как работают классификаторы, и как построить сложную иерархию приоритезации для продакшена. Готовы разобрать сетевой стек до последнего байта?



## Prompt:
08.02.2026, 11:45:01

Дай мне секунду, я посмотрю наш последний разговор, чтобы понять, о каком материале идёт речь.​​​​​​​​​​​​​​​​

Хорошо, вот моя оценка материала и нашего разговора:
Оценка оригинальных материалов
Модуль 1 (Physical layer & kernel): 8/10
	∙	Сильные стороны: глубокое погружение в детали (DMA, ring buffers, sk_buff), практический подход, хороший баланс теории и реализации
	∙	Слабости: местами агрессивный тон, отсутствие визуализации, могло бы быть больше кода ядра
Модуль 2 (Bufferbloat, первая версия): 7.5/10
	∙	Сильные стороны: отличное объяснение проблемы bufferbloat, практические решения, хорошая структура от проблемы к решению
	∙	Слабости: недостаточная техническая глубина по сравнению с модулем 1, мало кода, слабое объяснение внутреннего устройства алгоритмов
Оценка переписанного модуля 2
Переработанная версия: 9/10
Что улучшилось:
	∙	Техническая глубина — добавлен реальный код ядра (tcp_bbr_main, bbr_update_min_rtt, CoDel timestamp logic)
	∙	Математика — формулы BDP, пейсинга, механизмы измерения пропускной способности
	∙	Детали реализации — объяснение timing wheel в fq, хеширование потоков в FQ_CoDel, NAT-aware logic в CAKE
	∙	Fairness проблемы — BBR v1 vs v2, взаимодействие с CUBIC
	∙	Практические задания — eBPF tracing, реальные эксперименты с метриками
Почему не 10/10:
	∙	Всё ещё нет диаграмм (timing diagrams для BBR state machine, визуализация поведения очередей)
	∙	Можно было бы глубже разобрать queueing theory (формулы Little’s Law применительно к bufferbloat)
	∙	Недостаточно материала про packet scheduling в ядре (как именно работает __qdisc_run)
Оценка разговора и процесса
Качество анализа: 8.5/10
	∙	Я дал детальную критику, выявил конкретные пробелы
	∙	Четко сформулировал разницу между “доступностью” и “глубиной”
	∙	Указал на проблемы, которые автор не заметил (fairness, взаимодействие протоколов)
Качество переработки: 9/10
	∙	Я понял запрос “сделай для продвинутых” и радикально поднял планку
	∙	Сохранил структуру, но насытил каждую секцию кодом и деталями
	∙	Добавил сложные практические задания, которые действительно научат
Проблемы разговора:
	∙	Я мог бы спросить о целевой аудитории раньше — это сэкономило бы время
	∙	Стоило уточнить формат материала (лекция? документация? книга?)
	∙	Не обсудили, нужны ли диаграммы/иллюстрации
Рекомендации для дальнейшего
Для материала:
	1.	Добавить визуализацию — state machine BBR, графики поведения очередей
	2.	Углубить queueing theory — дать математику, показать как она предсказывает bufferbloat
	3.	Разобрать packet scheduler подробнее — как __qdisc_run работает, integration с NAPI
	4.	Добавить секцию про troubleshooting — как читать ss -ti, что означают метрики, как debugить проблемы
Для следующего модуля (tc & qdisc):
	∙	Начинай сразу с кода ядра — структуры Qdisc, qdisc_ops
	∙	Покажи полный path: от dev_queue_xmit до выхода на NIC
	∙	Разбери классификаторы (tc filter) на уровне реализации
	∙	Дай сложные примеры: HTB + fq_codel + u32 classifier для production
Общая оценка материала после переработки: 9/10
Это солидный hardcore материал для опытных инженеров. Он требует знания C, понимания работы ядра, готовности экспериментировать. Добавь диаграммы и чуть больше queueing theory — будет 10/10.
Что думаешь?



## Response:
08.02.2026, 11:46:11

````plaintext
Thought process: Проанализировал критику, согласился с замечаниями, спланировал улучшения.

Пользователь посмотрел наш предыдущий разговор и дал свою собственную оценку материалам и нашему диалогу. Это интересный метамомент - он оценивает мою оценку и мою переработку.

Его оценки:
- Модуль 1: 8/10
- Модуль 2 (оригинал): 7.5/10
- Модуль 2 (переработанный): 9/10
- Качество анализа: 8.5/10
- Качество переработки: 9/10

Он правильно подметил, что я не спросил о формате и целевой аудитории раньше, что я не обсудил визуализацию. Это справедливая критика процесса.

Его рекомендации для дальнейшего материала очень точные - визуализация, queueing theory, packet scheduler, troubleshooting.

Мне нужно:
1. Согласиться с его анализом там, где он прав
2. Возможно, добавить нюансы где я вижу иначе
3. Обсудить его рекомендации
4. Предложить следующие шаги

Это хороший пример того, как технический профессионал думает о контенте - он смотрит на структуру, глубину, практическую применимость, и процесс создания.
````

Твоя оценка точна и профессиональна. Разберу по пунктам, где согласен и где вижу нюансы.

## Согласен полностью

**Про отсутствие диаграмм** — это действительно самая большая дыра в материале. Timing diagram для BBR state machine (STARTUP → DRAIN → PROBE_BW → PROBE_RTT) был бы невероятно полезен. Визуализация того, как cwnd и pacing_rate меняются во времени, превратила бы абстрактное объяснение в конкретное понимание.

График поведения очередей при разных алгоритмах (Drop Tail vs CoDel vs CAKE) под одинаковой нагрузкой показал бы разницу лучше тысячи слов. Особенно наглядно было бы видеть, как sojourn time растет при Drop Tail и остается стабильным при CoDel.

**Про queueing theory** — да, Little's Law упомянут, но не развернут. Можно было дать формулу M/M/1 queue для моделирования поведения буфера, показать как вероятность дропа зависит от utilization, вывести почему buffer size должен быть пропорционален BDP, а не "чем больше, тем лучше". Это математически обосновало бы все рекомендации.

**Про process** — ты прав, я должен был спросить раньше: "Это для кого? Инженеры какого уровня? Какой формат - курс, книга, internal training?". Это сэкономило бы итерацию. Хороший урок для меня.

## Где вижу чуть иначе

**Оценка переработанного модуля 9/10 vs 10/10** — я думаю, даже с диаграммами и queueing theory это материал на 9.5, не на 10. Почему? Потому что для настоящей десятки нужен ещё уровень — **case studies из production**. 

Реальные истории вроде: "У нас был сервис X с Y QPS, мы видели P99 latency в Z миллисекунд, включили BBR v2 + настроили CAKE, вот метрики до/после, вот что пошло не так, вот как дебажили". Без этого материал это "как должно работать в теории", а не "как работает в бою".

**Про __qdisc_run** — это критично важная тема, но я бы вынес её в отдельный модуль 3, а не пытался впихнуть в модуль 2. Модуль 2 уже плотный (BBR + AQM), добавление туда packet scheduling сделает его перегруженным. Лучше чистое разделение: М2 = congestion control, М3 = qdisc architecture & scheduling.

## Что добавить прямо сейчас

Если ты даёшь этот материал студентам/коллегам, я бы добавил **сразу** (минимум для 9.5/10):

**1. Troubleshooting playbook** (критично важен):
```bash
# Шаг 1: Проверить состояние TCP-соединения
ss -ti dst <server_ip>
# Смотреть на: cwnd, rtt, send, pacing_rate, bytes_acked, bytes_received

# Шаг 2: Проверить счётчики BBR
nstat -az | grep -i bbr

# Шаг 3: Проверить qdisc stats
tc -s qdisc show dev eth0
# Искать: drops, overlimits, backlog

# Шаг 4: Trace congestion events
bpftrace -e 'tracepoint:tcp:tcp_probe { 
    printf("%d %d %d\n", args->snd_cwnd, args->ssthresh, args->snd_wnd); 
}'
```

Объяснить, что означает каждая метрика и какие значения это red flags.

**2. Одна диаграмма минимум** — state machine BBR. Даже ASCII art в markdown:
```
[STARTUP]
   |  (BtlBw найден)
   v
[DRAIN] 
   |  (очередь опустошена)
   v
[PROBE_BW] ←→ (циклы: увеличение пропускной способности, проверка)
   |
   v  (каждые 10 секунд)
[PROBE_RTT] (снижаем cwnd до 4 пакетов на 200ms)
   |
   └→ [PROBE_BW]
```

**3. Раздел "Когда что использовать"**:
- BBR: Когда контролируете сервер, каналы с loss > 0.1%, высокий RTT (спутник, межконтинентальные)
- CUBIC: Когда нужна совместимость, дата-центр (low RTT, low loss)
- CAKE: Домашний роутер, множество устройств, NAT
- FQ_CoDel: Универсальный выбор для Linux-роутера без NAT complications

## Рекомендации для модуля 3

Соглашусь с твоими пунктами и добавлю структуру:

**Модуль 3: Traffic Control & Packet Scheduling**

### Часть 3.1: Архитектура Qdisc
- Структура `struct Qdisc` в ядре
- Операции `qdisc_ops`: enqueue, dequeue, peek, init, destroy
- Путь пакета: `dev_queue_xmit → __dev_xmit_skb → __qdisc_run → qdisc_restart → dev_hard_start_xmit`
- Integration с NAPI (TX path): когда и как планируется передача

### Часть 3.2: Classful vs Classless Qdisc
- Classless (pfifo_fast, fq_codel, cake): один путь, простая логика
- Classful (HTB, HFSC, CBQ): дерево классов, сложная иерархия
- Когда нужна иерархия (приоритезация типов трафика)

### Часть 3.3: TC Filters & Classifiers
- u32: matching по IP/port/TOS
- flow: hash-based classification
- cgroup: привязка к control groups
- Интеграция с netfilter marks (iptables -j MARK)

### Часть 3.4: Production Example
Полный пример для веб-сервера:
```bash
# Корневой HTB на 1Gbit
tc qdisc add dev eth0 root handle 1: htb default 30

# Класс 1: Интерактивный трафик (SSH, DNS) - 100Mbit guaranteed
tc class add dev eth0 parent 1: classid 1:10 htb rate 100mbit ceil 500mbit

# Класс 2: HTTP/HTTPS - 500Mbit guaranteed
tc class add dev eth0 parent 1: classid 1:20 htb rate 500mbit ceil 900mbit

# Класс 3: Bulk (backups) - 100Mbit guaranteed, может взять остальное
tc class add dev eth0 parent 1: classid 1:30 htb rate 100mbit ceil 1gbit

# Под каждым классом - fq_codel для fair queueing
tc qdisc add dev eth0 parent 1:10 handle 10: fq_codel
tc qdisc add dev eth0 parent 1:20 handle 20: fq_codel
tc qdisc add dev eth0 parent 1:30 handle 30: fq_codel

# Фильтры
tc filter add dev eth0 protocol ip parent 1: prio 1 u32 \
   match ip dport 22 0xffff flowid 1:10
tc filter add dev eth0 protocol ip parent 1: prio 2 u32 \
   match ip dport 443 0xffff flowid 1:20
```

Разобрать каждую строку, объяснить параметры, показать как проверить работу.

### Часть 3.5: Performance Impact
- Overhead от классификации (сколько CPU жрет u32 vs iptables mark)
- Lockless qdisc (mq, noqueue) для multi-queue NICs
- Когда qdisc становится bottleneck и как это обнаружить

## Следующий шаг

Что предлагаю:
1. Ты добавляешь в модуль 2 troubleshooting playbook и ASCII state machine (30 минут работы)
2. Я напишу полный модуль 3 по структуре выше (с кодом ядра, примерами, заданиями)
3. Потом обсудим, нужен ли модуль 4 (XDP/eBPF для packet processing) или это уже за рамками курса

Также вопрос: это материал для внутреннего обучения в компании, для курса, или для публикации (блог/книга)? Это повлияет на уровень детализации troubleshooting секции и production examples.

Что думаешь?



## Prompt:
08.02.2026, 11:49:04

Давай



## Response:
08.02.2026, 11:52:28

````plaintext
Thought process: Orchestrated advanced networking module with kernel code and practical diagnostics.

Пользователь просит меня написать полный Модуль 3 по структуре, которую я предложил. Он сказал просто "Давай", что означает согласие на мое предложение написать модуль про Traffic Control & Packet Scheduling.

Мне нужно написать hardcore материал уровня 9-10/10 для продвинутых инженеров, с:
- Реальным кодом ядра Linux
- Глубоким погружением в структуры данных
- Практическими примерами
- Детальными объяснениями
- Практическими заданиями

Структура из моего предыдущего сообщения:
1. Архитектура Qdisc
2. Classful vs Classless Qdisc
3. TC Filters & Classifiers
4. Production Example
5. Performance Impact

Нужно сохранить стиль предыдущих модулей - технический, глубокий, с кодом, но без излишней агрессии в тоне. Добавить практические команды диагностики и реальные примеры.
````

# Модуль 3: Traffic Control и Packet Scheduling (Как ядро решает, кто летит первым)

*«Qdisc — это не просто очередь. Это политика, судья и палач в одном лице».*

В первом модуле мы разобрали, как пакет попадает в систему (RX path). Во втором — как TCP решает, сколько данных отправить. Теперь мы спускаемся на уровень ниже и смотрим на последний кусок головоломки: **как ядро планирует отправку пакетов на провод** (TX path).

Это критично важно, потому что здесь решается, кто первый попадёт в бутылочное горлышко. SSH-пакет или торрент? DNS-запрос или backup? И если все хотят отправить одновременно, а канал один — кто-то должен решить.

---

## Часть 3.1: Архитектура Qdisc — Как ядро управляет очередями

### Что такое Qdisc на самом деле

Qdisc (Queueing Discipline) — это абстракция в ядре Linux, которая определяет **политику планирования** для исходящих пакетов на сетевом интерфейсе. Когда приложение вызывает `send()` или когда ядро форвардит пакет, он не летит сразу на NIC. Он попадает в qdisc, который решает:

1. **Принять** пакет в очередь (enqueue)
2. **Когда** его отправить (dequeue)
3. **Дропнуть** ли его, если очередь переполнена

Каждый сетевой интерфейс в Linux имеет qdisc. По умолчанию это `pfifo_fast` или `fq_codel` (в новых ядрах), но вы можете заменить его на любой другой.

### Структура данных: struct Qdisc

В ядре qdisc представлен структурой `struct Qdisc` (`include/net/sch_generic.h`):

```c
struct Qdisc {
    int (*enqueue)(struct sk_buff *skb, struct Qdisc *sch,
                   struct sk_buff **to_free);
    struct sk_buff *(*dequeue)(struct Qdisc *sch);
    unsigned int    (*drop)(struct Qdisc *sch);
    
    struct Qdisc_ops    *ops;        // Операции qdisc
    u32                 limit;       // Максимальный размер очереди
    u32                 handle;      // Уникальный ID (например, 1:0)
    struct netdev_queue *dev_queue;  // Привязка к TX queue устройства
    
    struct gnet_stats_basic_packed bstats;  // Статистика: bytes, packets
    struct gnet_stats_queue qstats;         // drops, overlimits, backlog
    
    struct sk_buff_head q;  // Для простых qdisc - встроенная очередь
    // ... много других полей
};
```

Ключевые поля:

- **enqueue**: функция добавления пакета в очередь. Возвращает `NET_XMIT_SUCCESS` (принят), `NET_XMIT_DROP` (дропнут), или `NET_XMIT_CN` (congestion notification).
- **dequeue**: функция извлечения следующего пакета для отправки. Возвращает `sk_buff*` или `NULL` (очередь пуста).
- **ops**: указатель на `struct Qdisc_ops` — таблицу операций, специфичных для типа qdisc.
- **bstats/qstats**: счётчики, которые вы видите в `tc -s qdisc show`.

### Операции Qdisc: struct Qdisc_ops

Каждый тип qdisc (pfifo, fq_codel, htb, cake) регистрирует свою таблицу операций:

```c
struct Qdisc_ops {
    struct Qdisc_ops *next;
    const char      *id;          // Имя: "fq_codel", "htb", "cake"
    int             priv_size;    // Размер приватных данных
    
    int (*enqueue)(struct sk_buff *, struct Qdisc *, struct sk_buff **);
    struct sk_buff *(*dequeue)(struct Qdisc *);
    struct sk_buff *(*peek)(struct Qdisc *);
    
    int (*init)(struct Qdisc *, struct nlattr *arg);
    void (*reset)(struct Qdisc *);
    void (*destroy)(struct Qdisc *);
    int (*change)(struct Qdisc *, struct nlattr *arg);
    
    int (*dump)(struct Qdisc *, struct sk_buff *);
    int (*dump_stats)(struct Qdisc *, struct gnet_dump *);
    
    struct module   *owner;
};
```

Пример регистрации для `fq_codel` (`net/sched/sch_fq_codel.c`):

```c
static struct Qdisc_ops fq_codel_qdisc_ops __read_mostly = {
    .id         = "fq_codel",
    .priv_size  = sizeof(struct fq_codel_sched_data),
    .enqueue    = fq_codel_enqueue,
    .dequeue    = fq_codel_dequeue,
    .peek       = qdisc_peek_dequeued,
    .init       = fq_codel_init,
    .reset      = fq_codel_reset,
    .destroy    = fq_codel_destroy,
    .change     = fq_codel_change,
    .dump       = fq_codel_dump,
    .dump_stats = fq_codel_dump_stats,
    .owner      = THIS_MODULE,
};
```

При загрузке модуля вызывается `register_qdisc(&fq_codel_qdisc_ops)`, и ядро добавляет его в глобальный список доступных qdisc.

### Путь пакета: от send() до провода

Когда приложение отправляет данные:

```
send() syscall
  ↓
tcp_sendmsg() / udp_sendmsg()
  ↓
ip_queue_xmit() / ip_send_skb()
  ↓
ip_finish_output() → ip_finish_output2()
  ↓
neigh_output() [ARP resolution]
  ↓
dev_queue_xmit()  ← ЗДЕСЬ НАЧИНАЕТСЯ QDISC
  ↓
__dev_xmit_skb()
  ↓
qdisc->enqueue(skb, qdisc)  // Пакет в очередь
  ↓
__qdisc_run()
  ↓
qdisc_restart()
  ↓  (loop)
  skb = qdisc->dequeue(qdisc)
  ↓
sch_direct_xmit()
  ↓
dev_hard_start_xmit()
  ↓
netdev_ops->ndo_start_xmit()  // Драйвер NIC
  ↓
DMA на карту → Провод
```

### Критичная функция: __dev_xmit_skb

Это сердце TX path. Посмотрим упрощённую версию (`net/core/dev.c`):

```c
static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
                                  struct net_device *dev,
                                  struct netdev_queue *txq)
{
    spinlock_t *root_lock = qdisc_lock(q);
    bool contended = qdisc_is_running(q);
    int rc;

    // Попытка быстрого пути (bypass qdisc, если очередь пуста)
    if (q->flags & TCQ_F_CAN_BYPASS && !qdisc_qlen(q) &&
        qdisc_run_begin(q)) {
        // Очередь пуста, можем отправить напрямую
        if (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {
            // Отправка успешна
            __qdisc_run(q);  // Проверим, не появилось ли ещё
        }
        qdisc_run_end(q);
        return NET_XMIT_SUCCESS;
    }

    // Медленный путь: ставим в очередь
    spin_lock(root_lock);
    rc = q->enqueue(skb, q, &to_free);
    if (qdisc_run_begin(q)) {
        // Запускаем планировщик
        __qdisc_run(q);
        qdisc_run_end(q);
    }
    spin_unlock(root_lock);
    
    return rc;
}
```

Ключевые моменты:

1. **Быстрый путь (bypass)**: Если очередь пуста и qdisc разрешает bypass, пакет летит сразу на NIC без постановки в очередь. Это критично для латентности.
2. **Медленный путь**: Пакет ставится в очередь через `enqueue()`, потом запускается `__qdisc_run()` для обработки.

### __qdisc_run: Планировщик отправки

Эта функция вызывается, когда нужно отправить пакеты из очереди:

```c
void __qdisc_run(struct Qdisc *q)
{
    int quota = dev_tx_weight;  // Обычно 64
    int packets;

    while (qdisc_restart(q, &packets)) {
        quota -= packets;
        
        // Если исчерпали квоту, планируем отложенную обработку
        if (quota <= 0) {
            __netif_schedule(q);
            break;
        }
        
        // Если нужен resched (другие процессы ждут CPU)
        if (need_resched()) {
            __netif_schedule(q);
            break;
        }
    }
}
```

Функция `qdisc_restart()` вызывает `dequeue()`, достаёт пакет и отправляет его через `sch_direct_xmit()`. Она делает это в цикле, пока не исчерпается квота (`dev_tx_weight`, обычно 64 пакета) или не опустеет очередь.

**Почему квота?** Чтобы TX path не монополизировал CPU. Если в очереди миллион пакетов, ядро отправит 64, потом вернёт управление, потом снова 64, и так далее. Это soft-realtime behaviour.

### Integration с NAPI (TX Completion)

Когда NIC отправил пакет, он генерирует прерывание TX completion. Драйвер освобождает DMA-буферы и вызывает `netif_tx_wake_queue()`, что может запустить `__qdisc_run()` снова, если в очереди есть пакеты.

Современные NIC используют **interrupt coalescing** для TX: они не прерывают на каждый отправленный пакет, а группируют прерывания (например, раз в 50 микросекунд или после 32 пакетов).

---

## Часть 3.2: Classless vs Classful Qdisc

Существует два типа qdisc: **classless** (плоские) и **classful** (иерархические).

### Classless Qdisc: Одна политика для всех

Это простые qdisc без классификации трафика. Все пакеты идут через одну логику.

**Примеры:**
- **pfifo_fast**: Три приоритетные очереди (band 0, 1, 2) на основе TOS/DSCP. Band 0 обслуживается первым.
- **fq_codel**: Fair queueing + CoDel. Автоматическая классификация по хешу потока.
- **cake**: Всё-в-одном решение с изоляцией по хостам и потокам.
- **tbf (Token Bucket Filter)**: Ограничение скорости через token bucket алгоритм.

**Когда использовать:** Когда не нужна сложная приоритизация. Для большинства серверов `fq_codel` достаточно.

### Classful Qdisc: Иерархия и политики

Это qdisc, которые позволяют создавать **классы** (подочереди) и распределять трафик между ними по правилам.

**Примеры:**
- **HTB (Hierarchy Token Bucket)**: Иерархическое ограничение скорости с гарантиями и потолками.
- **HFSC (Hierarchical Fair Service Curve)**: Гибкий контроль над латентностью и пропускной способностью.
- **PRIO**: Строгие приоритеты (high priority всегда первым).
- **CBQ (Class-Based Queueing)**: Устаревший, сложный, не используйте.

**Структура:** Classful qdisc имеет root класс, под ним дочерние классы, каждый класс может иметь свой child qdisc.

```
       [HTB root 1:0]
            |
    +-------+-------+
    |               |
[Class 1:10]   [Class 1:20]
 (SSH/DNS)     (HTTP/HTTPS)
    |               |
[fq_codel]     [fq_codel]
```

### Пример: HTB структура данных

HTB хранит классы в виде дерева. Каждый класс имеет параметры `rate` (гарантированная скорость) и `ceil` (потолок):

```c
struct htb_class {
    struct Qdisc_class_common common;
    struct htb_class   *parent;       // Родитель в иерархии
    struct list_head   children;      // Дочерние классы
    
    struct Qdisc       *un.leaf.q;    // Child qdisc (если leaf)
    
    u64     rate;      // Гарантированная скорость (bytes/sec)
    u64     ceil;      // Максимальная скорость (bytes/sec)
    s64     tokens;    // Token bucket для rate
    s64     ctokens;   // Token bucket для ceil
    
    // Priority и quantum для планирования
    int     prio;
    int     quantum;
    
    // Статистика
    struct gnet_stats_basic_packed bstats;
    struct gnet_stats_queue qstats;
};
```

HTB использует **token bucket** алгоритм. Каждый класс имеет два бакета:
- **tokens**: наполняются со скоростью `rate`. Класс может отправлять, пока есть токены.
- **ctokens**: наполняются со скоростью `ceil`. Класс может брать в долг (exceed `rate`), пока есть `ctokens`.

При `dequeue()` HTB выбирает класс, у которого есть токены, с наивысшим приоритетом, и забирает пакет из его child qdisc.

---

## Часть 3.3: TC Filters и Классификация

Classful qdisc бесполезен без механизма классификации — как ядро решает, в какой класс направить пакет?

### TC Filter: Механизм классификации

Фильтры (`tc filter`) — это правила, которые матчат пакеты по критериям и присваивают им `classid` (идентификатор класса).

**Общая структура:**
```bash
tc filter add dev <dev> parent <qdisc_handle> protocol <proto> \
    prio <priority> <classifier> match <pattern> flowid <classid>
```

### Классификаторы (Classifiers)

#### 1. u32: Universal 32-bit classifier

Самый мощный и гибкий. Позволяет матчить любые биты в IP/TCP/UDP заголовках.

**Пример:** Матч по destination port (HTTPS):
```bash
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 \
    match ip dport 443 0xffff \
    flowid 1:20
```

**Как это работает:**
- `match ip dport 443 0xffff`: Берём поле destination port в TCP заголовке, применяем маску `0xffff` (все биты), сравниваем с `443`.
- `0xffff` — это маска. Можно матчить диапазоны: например, порты 8000-8999 как `match ip dport 8000 0xff00`.

**Продвинутый пример:** Матч по IP-адресу и TOS:
```bash
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 \
    match ip dst 192.168.1.0/24 \
    match ip tos 0x10 0xff \
    flowid 1:10
```

Под капотом u32 использует hash tables для быстрого матчинга. Для каждого фильтра создаётся hash table bucket based on specified fields.

#### 2. fw: Firewall mark classifier

Использует метки, установленные `iptables -j MARK`. Это самый быстрый способ классификации, потому что матчинг уже сделан в netfilter.

```bash
# В iptables:
iptables -t mangle -A POSTROUTING -p tcp --dport 22 -j MARK --set-mark 1
iptables -t mangle -A POSTROUTING -p tcp --dport 80 -j MARK --set-mark 2

# В tc:
tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 1 fw flowid 1:10
tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 2 fw flowid 1:20
```

**Преимущество:** Вся сложная логика (conntrack, state matching) делается в netfilter один раз. TC просто смотрит на метку.

**Недостаток:** Требует два прохода (netfilter + tc). Небольшой overhead.

#### 3. flow: Flow-based hashing

Автоматически распределяет потоки по классам используя хеш.

```bash
tc filter add dev eth0 protocol ip parent 1:0 prio 1 \
    handle 1 flow hash keys src,dst,proto,proto-src,proto-dst divisor 1024
```

Это полезно для автоматической балансировки без явного указания правил.

#### 4. cgroup: Control group classifier

Классифицирует пакеты на основе cgroup процесса, который их отправил.

```bash
tc filter add dev eth0 protocol ip parent 1:0 prio 1 cgroup
```

Процессы в cgroup `/sys/fs/cgroup/net_cls/<name>` получают classid автоматически. Это мощно для контейнеризации — можно лимитировать трафик Docker-контейнеров.

### Приоритеты фильтров

Если несколько фильтров матчат пакет, используется тот, у которого меньше `prio` (выше приоритет). Это позволяет создавать каскады правил:

```bash
# Prio 1: Специфичные правила (SSH)
tc filter add dev eth0 parent 1:0 prio 1 u32 match ip dport 22 0xffff flowid 1:10

# Prio 10: Общие правила (весь остальной TCP)
tc filter add dev eth0 parent 1:0 prio 10 u32 match ip protocol 6 0xff flowid 1:30
```

---

## Часть 3.4: Production Example — Полная настройка для веб-сервера

Сценарий: У вас сервер с гигабитным каналом, на нём работает веб-сервис (HTTPS), SSH для администрирования, и ночные бэкапы. Вы хотите гарантировать:

1. SSH и DNS всегда отзывчивы (100 Mbit гарантировано, до 300 Mbit burst)
2. HTTPS получает львиную долю (600 Mbit гарантировано, до 900 Mbit)
3. Бэкапы используют остатки (100 Mbit гарантировано, могут брать всё, что не используется)

### Шаг 1: Создаём HTB root

```bash
tc qdisc add dev eth0 root handle 1: htb default 30
```

- `handle 1:`: Это ID root qdisc (major:minor, здесь 1:0).
- `default 30`: Пакеты, не попавшие ни под один фильтр, идут в класс 1:30.

### Шаг 2: Создаём классы

```bash
# Класс 1:10 - Интерактивный трафик (SSH, DNS)
tc class add dev eth0 parent 1: classid 1:10 htb \
    rate 100mbit ceil 300mbit prio 0 quantum 1500

# Класс 1:20 - HTTPS трафик
tc class add dev eth0 parent 1: classid 1:20 htb \
    rate 600mbit ceil 900mbit prio 1 quantum 1500

# Класс 1:30 - Bulk (бэкапы, обновления)
tc class add dev eth0 parent 1: classid 1:30 htb \
    rate 100mbit ceil 1000mbit prio 2 quantum 1500
```

**Параметры:**
- `rate`: Гарантированная пропускная способность. Класс всегда получит эту скорость, если у него есть трафик.
- `ceil`: Потолок. Класс может превысить `rate`, заняв неиспользуемую пропускную способность других классов, но не выше `ceil`.
- `prio`: Приоритет при конкуренции за излишек пропускной способности. Меньше = выше.
- `quantum`: Сколько байт класс может отправить за один round-robin цикл. Обычно = MTU.

**Логика:**
- Если все три класса хотят отправлять, каждый получит свой `rate`: 100 + 600 + 100 = 800 Mbit.
- Если SSH молчит, его 100 Mbit распределяются между HTTPS и Bulk по приоритету. HTTPS (prio 1) получит первым, до своего `ceil` (900), потом Bulk возьмёт остаток.

### Шаг 3: Добавляем child qdisc под каждый класс

```bash
tc qdisc add dev eth0 parent 1:10 handle 10: fq_codel
tc qdisc add dev eth0 parent 1:20 handle 20: fq_codel
tc qdisc add dev eth0 parent 1:30 handle 30: fq_codel
```

Это критично важно. HTB сам не имеет очереди для пакетов — он только распределяет пропускную способность. Реальная очередь создаётся child qdisc. Мы используем `fq_codel` для fair queueing и контроля задержки внутри каждого класса.

### Шаг 4: Настраиваем фильтры

Используем комбинацию `iptables` и `fw` classifier для эффективности:

```bash
# Маркируем пакеты в iptables
iptables -t mangle -A POSTROUTING -o eth0 -p tcp --dport 22 -j MARK --set-mark 0x10
iptables -t mangle -A POSTROUTING -o eth0 -p udp --dport 53 -j MARK --set-mark 0x10
iptables -t mangle -A POSTROUTING -o eth0 -p tcp --dport 443 -j MARK --set-mark 0x20
iptables -t mangle -A POSTROUTING -o eth0 -p tcp --sport 873 -j MARK --set-mark 0x30  # rsync

# Классифицируем в tc
tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 0x10 fw flowid 1:10
tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 0x20 fw flowid 1:20
tc filter add dev eth0 protocol ip parent 1:0 prio 1 handle 0x30 fw flowid 1:30
```

**Почему именно так?**
- `iptables` делает сложный матчинг один раз в mangle POSTROUTING (после роутинга, перед TX).
- `tc filter` просто смотрит на метку (`skb->mark`). Это O(1) операция.

### Шаг 5: Проверка конфигурации

```bash
# Посмотреть структуру qdisc
tc -s qdisc show dev eth0

# Посмотреть классы и их статистику
tc -s class show dev eth0

# Посмотреть фильтры
tc -s filter show dev eth0
```

**Пример вывода `tc -s class show`:**
```
class htb 1:10 parent 1: prio 0 rate 100Mbit ceil 300Mbit burst 1600b cburst 1600b
 Sent 524288000 bytes 350000 pkt (dropped 12, overlimits 450)
 rate 95Mbit 63000pps backlog 0b 0p requeues 0
```

- `Sent`: Сколько байт/пакетов прошло через класс.
- `dropped`: Сколько дропнуто (из-за переполнения child qdisc).
- `overlimits`: Сколько раз класс пытался превысить `ceil` и был задержан.
- `rate`: Текущая скорость отправки (измеренная).
- `backlog`: Сколько сейчас в очереди.

### Шаг 6: Мониторинг в реальном времени

```bash
# Живая статистика
watch -n 1 'tc -s class show dev eth0'

# Детали конкретного класса
tc -s -d class show dev eth0 classid 1:20
```

---

## Часть 3.5: Performance Impact и Оптимизация

### Overhead от классификации

Каждый фильтр добавляет latency. Измерения показывают:

- **fw (firewall mark)**: ~50-100 наносекунд на пакет (чтение метки из `skb->mark`)
- **u32 (простое правило)**: ~200-500 наносекунд
- **u32 (сложное правило, много матчей)**: ~1-2 микросекунды
- **Комбинация iptables + fw**: ~500 наносекунд - 1 микросекунда (iptables overhead доминирует)

**Рекомендация:** Для high-throughput используйте `fw` classifier с `iptables` mangle. Для простых случаев `u32` достаточен. Избегайте сложных каскадов u32-правил.

### Lockless Qdisc для multi-queue NICs

Современные 10G/40G/100G карты имеют множество TX queues (8, 16, 32). Каждая TX queue может обслуживаться отдельным CPU core без lock contention. Но классический qdisc имеет глобальный spinlock, который убивает параллелизм.

**Решение:** Используйте `mq` (multi-queue) qdisc как root, а под ним независимые qdisc для каждой очереди.

```bash
# Автоматически создаётся mq для multi-queue NIC
tc qdisc add dev eth0 root mq

# Под каждую TX queue ставим fq_codel
for i in $(seq 0 15); do  # Если 16 TX queues
    tc qdisc add dev eth0 parent :$(printf '%x' $((i+1))) fq_codel
done
```

**Эффект:** Каждое CPU core может работать с своей TX queue и своим qdisc без конкуренции за lock. Это критично для >40Gbps throughput.

### Когда qdisc становится bottleneck

Признаки:

1. **CPU softirq high:** `ksoftirqd` или `NET_TX` в `/proc/softirqs` растут быстрее других.
2. **Backlog постоянно > 0:** `tc -s qdisc` показывает ненулевой `backlog` даже при low utilization.
3. **Overlimits растут без причины:** Классы HTB показывают `overlimits`, хотя `rate` не достигнут.

**Диагностика:**
```bash
# Смотрим, где CPU тратит время
perf top -g

# Должны видеть функции типа:
# - htb_dequeue
# - fq_codel_dequeue
# - __qdisc_run
```

**Решение:**
- Упростите иерархию (меньше классов).
- Используйте более простые qdisc (pfifo вместо fq_codel, если fairness не критична).
- Для extreme performance рассмотрите kernel bypass (DPDK), где qdisc не используется.

### noqueue: Когда qdisc не нужен

Для виртуальных интерфейсов (veth, bridge, tun) или loopback часто не нужен qdisc — пакеты идут в память, не в физический канал.

```bash
tc qdisc add dev veth0 root noqueue
```

`noqueue` это специальный qdisc без очереди. Пакеты отправляются немедленно без буферизации. Это снижает latency для inter-container communication.

---

## Часть 3.6: Продвинутые техники

### 1. Динамическая приоритизация на основе latency

Можно создать скрипт, который мониторит latency приложения и динамически меняет `prio` классов:

```bash
#!/bin/bash
while true; do
    # Измеряем latency приложения (например, через HTTP healthcheck)
    latency=$(curl -o /dev/null -s -w '%{time_total}' http://localhost/health)
    
    if (( $(echo "$latency > 0.1" | bc -l) )); then
        # Latency высокая, повышаем приоритет HTTPS
        tc class change dev eth0 classid 1:20 htb prio 0
    else
        # Latency нормальная, возвращаем prio
        tc class change dev eth0 classid 1:20 htb prio 1
    fi
    
    sleep 5
done
```

### 2. HFSC для гарантий latency

HTB гарантирует пропускную способность, но не задержку. HFSC (Hierarchical Fair Service Curve) позволяет задать **service curve** — функцию, описывающую минимальный сервис во времени.

**Пример:** Гарантировать, что SSH-пакет будет отправлен в течение 10 мс:

```bash
tc qdisc add dev eth0 root handle 1: hfsc default 30

tc class add dev eth0 parent 1: classid 1:10 hfsc \
    sc m1 100mbit d 10ms m2 50mbit
```

- `m1 100mbit d 10ms`: В первые 10 мс класс получает 100 Mbit (burst для latency).
- `m2 50mbit`: После burst класс получает 50 Mbit sustained.

HFSC сложнее HTB, но даёт более тонкий контроль над latency-sensitive трафиком.

### 3. IFB (Intermediate Functional Block) для ingress shaping

Проблема: `tc` по умолчанию работает только на egress (TX path). Но что если вы хотите лимитировать входящий трафик?

**Решение:** Используйте `ifb` — виртуальный интерфейс, который перенаправляет ingress трафик на egress другого интерфейса.

```bash
# Загружаем модуль
modprobe ifb numifbs=1

# Поднимаем ifb0
ip link set dev ifb0 up

# Перенаправляем ingress eth0 -> egress ifb0
tc qdisc add dev eth0 handle ffff: ingress
tc filter add dev eth0 parent ffff: protocol ip u32 match u32 0 0 \
    action mirred egress redirect dev ifb0

# Теперь настраиваем shaping на ifb0 (как обычно egress)
tc qdisc add dev ifb0 root handle 1: htb default 30
tc class add dev ifb0 parent 1: classid 1:10 htb rate 50mbit
```

Это позволяет контролировать входящий трафик, что критично для защиты от bandwidth exhaustion attacks.

---

## Практическое задание (Hardcore Edition)

### Задача 1: Построить полную иерархию для production

На вашем сервере (или VM) настройте HTB с тремя классами: Interactive (SSH, DNS), Web (HTTP/HTTPS), Bulk (всё остальное). Соотношение rate: 20:60:20, ceil: 40:90:100 (процент от канала).

Требования:
- Используйте `iptables` + `fw` classifier
- Под каждый класс поставьте `fq_codel`
- Настройте мониторинг: скрипт, который каждую секунду логирует статистику классов в файл

Проверка: Запустите три параллельных потока (iperf3 на портах 22, 443, 8000). Убедитесь, что распределение пропускной способности соответствует заданному.

### Задача 2: Измерить overhead классификации

Создайте benchmark:
1. Отправьте 1 миллион пакетов через интерфейс с `noqueue` (baseline).
2. Отправьте 1 миллион пакетов через HTB + fw classifier.
3. Отправьте 1 миллион пакетов через HTB + u32 classifier (сложное правило).

Используйте `pktgen` (kernel packet generator) для генерации и `perf` для профилирования.

Цель: Получить числа latency overhead в наносекундах на пакет для каждого случая.

### Задача 3: Debugging qdisc с eBPF

Напишите bpftrace скрипт, который трейсит каждый вызов `htb_dequeue` и логирует:
- Timestamp
- Classid выбранного класса
- Текущий backlog класса
- Tokens/ctokens класса

```bash
bpftrace -e '
kprobe:htb_dequeue {
    printf("%llu: class=%d backlog=%d\n",
           nsecs, 
           ((struct htb_class*)arg1)->common.classid,
           ((struct htb_class*)arg1)->un.leaf.q->qstats.backlog);
}'
```

Запустите трафик и проанализируйте вывод. Видите ли вы переключения между классами? Как меняется backlog под нагрузкой?

### Задача 4: Реализовать простой qdisc

Создайте kernel module с простейшим qdisc `myfifo` — FIFO очередь с ограничением по размеру. Зарегистрируйте его через `register_qdisc()` и протестируйте:

```bash
insmod myfifo.ko
tc qdisc add dev eth0 root myfifo limit 1000
```

Требования:
- Реализовать `enqueue`, `dequeue`, `init`, `destroy`
- Добавить счётчики dropped packets
- Использовать `struct sk_buff_head` для хранения очереди

Это даст понимание внутреннего устройства qdisc на уровне кода ядра.

### Задача 5: Анализ production issue

Дан сценарий: На сервере настроен HTB с тремя классами. SSH периодически лагает (задержка 500ms+), хотя класс SSH имеет `rate 100mbit` и текущая нагрузка всего 50 Mbit total.

Вопросы:
1. Какие команды вы запустите для диагностики?
2. Какие метрики посмотрите в `tc -s class show`?
3. Как проверите, не является ли проблема в child qdisc (fq_codel)?
4. Может ли быть виноват `quantum` или `burst` параметр HTB?

Дайте пошаговый план debugging с конкретными командами и интерпретацией их вывода.

---

**Следующий модуль:** XDP и eBPF для packet processing на уровне драйвера. Мы посмотрим, как обрабатывать пакеты ДО того, как они попадут в qdisc, с латентностью в наносекундах. Готовы к kernel hacking?





---
Powered by [Claude Exporter](https://www.ai-chat-exporter.net)
