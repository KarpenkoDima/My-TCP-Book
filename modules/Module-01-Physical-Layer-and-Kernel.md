# Модуль 1: Физика и Ядро (Где реально умирают пакеты)

> *«Прежде чем отлаживать TCP — убедись, что пакет вообще добрался до ядра.»*

Этот модуль посвящён самому фундаменту сетевого стека Linux: тому, что происходит с пакетом от момента попадания фотонов (или электрических сигналов) на сетевой адаптер до передачи данных в Socket API. Именно здесь — на уровне DMA-транзакций, аппаратных прерываний и SoftIRQ — теряется большинство пакетов под высокой нагрузкой, и именно здесь Senior-инженер должен уметь диагностировать проблемы.

Мы разберём путь пакета снизу вверх: от кольцевых буферов NIC через механизм NAPI и масштабирование по ядрам процессора до структуры `sk_buff` и технологий offloading. Завершим модуль обзором kernel bypass — подходов, которые появились именно потому, что классический сетевой стек ядра не справляется с десятками миллионов пакетов в секунду.

---

## Часть 1.1: Жизнь пакета до Socket API (The Landing)

### Что происходит до того, как ядро «узнает» о пакете

Когда Ethernet-фрейм приходит на порт сетевого адаптера (NIC), CPU об этом ещё ничего не знает. Пакет проходит следующий путь:

1. **PHY-уровень** — декодирование сигнала из среды (оптика, медь) в цифровые биты.
2. **MAC-уровень** — проверка FCS (Frame Check Sequence). Если контрольная сумма не совпадает, фрейм отбрасывается аппаратно — ядро его никогда не увидит.
3. **DMA-передача** — NIC записывает содержимое фрейма напрямую в оперативную память хоста через шину PCIe, минуя процессор.
4. **Аппаратное прерывание** — NIC сигнализирует CPU о том, что новые данные доступны.

Критически важно понять: **между шагами 3 и 4 пакет уже лежит в RAM**, но операционная система о нём не знает. Это окно — потенциальное место потерь.

### DMA и кольцевые буферы (Ring Buffers)

NIC не может записывать данные в произвольные адреса памяти. Вместо этого используется механизм **кольцевых буферов** (ring buffers, descriptor rings) — заранее выделенных областей памяти, о которых NIC и драйвер договариваются при инициализации.

**Концепция кольцевого буфера:**

```
    Кольцевой буфер RX (упрощённая схема)
    ========================================

    Дескриптор — это не сам пакет, а метаданные:
    адрес буфера в RAM, длина, статус.

    +--------+--------+--------+--------+--------+--------+
    | desc 0 | desc 1 | desc 2 | desc 3 | desc 4 | desc 5 |
    +--------+--------+--------+--------+--------+--------+
        |        |        |        |
        v        v        v        v
      [buf 0]  [buf 1]  [buf 2]  [buf 3]   ...
      (в RAM)  (в RAM)  (в RAM)  (в RAM)

    HEAD (запись NIC) ──────►  desc 2   (NIC записывает сюда следующий пакет)
    TAIL (чтение драйвера) ──► desc 0   (драйвер забирает отсюда)

    Когда HEAD догоняет TAIL — буфер переполнен,
    новые пакеты отбрасываются аппаратно (rx_missed_errors).
```

**Как это работает на уровне драйвера:**

При загрузке драйвер выделяет набор буферов в RAM и заполняет дескрипторный ринг адресами этих буферов. NIC знает физические адреса дескрипторов и использует DMA для записи данных пакетов в соответствующие буферы.

```c
/*
 * Выделение кольцевого буфера RX (упрощённый пример
 * на основе драйверов семейства Intel e1000e/ixgbe).
 *
 * Реальный код находится в drivers/net/ethernet/intel/
 */

/* Структура одного RX-дескриптора (формат зависит от NIC) */
struct rx_desc {
    __le64 buffer_addr;    /* Физический адрес буфера в RAM (для DMA) */
    __le64 status_error;   /* Статус: пакет записан? Ошибка CRC? */
    __le16 length;         /* Длина полученных данных */
    __le16 vlan_tag;       /* VLAN-тег (если есть) */
};

/*
 * Инициализация RX-ринга.
 * Вызывается при открытии сетевого интерфейса (ndo_open).
 */
static int setup_rx_ring(struct adapter *adapter)
{
    struct rx_ring *ring = &adapter->rx_ring;
    int i;

    /* Выделяем память под массив дескрипторов.
     * dma_alloc_coherent() гарантирует, что память доступна
     * и CPU, и устройству через DMA без дополнительных барьеров.
     * ring->dma — физический адрес, который мы сообщим NIC. */
    ring->desc = dma_alloc_coherent(&adapter->pdev->dev,
                                    ring->count * sizeof(struct rx_desc),
                                    &ring->dma,
                                    GFP_KERNEL);
    if (!ring->desc)
        return -ENOMEM;

    /* Для каждого дескриптора выделяем страницу памяти
     * и записываем её физический адрес в дескриптор. */
    for (i = 0; i < ring->count; i++) {
        struct page *page = alloc_page(GFP_KERNEL);
        if (!page)
            goto err_free;

        /* Получаем физический адрес страницы для DMA */
        dma_addr_t dma_addr = dma_map_page(&adapter->pdev->dev,
                                           page, 0, PAGE_SIZE,
                                           DMA_FROM_DEVICE);

        ring->desc[i].buffer_addr = cpu_to_le64(dma_addr);
        ring->buffer_info[i].page = page;
        ring->buffer_info[i].dma = dma_addr;
    }

    /* Сообщаем NIC адрес ринга и его размер
     * через запись в регистры устройства (MMIO). */
    writel(lower_32_bits(ring->dma), adapter->hw_addr + RX_DESC_BASE);
    writel(upper_32_bits(ring->dma), adapter->hw_addr + RX_DESC_BASE + 4);
    writel(ring->count, adapter->hw_addr + RX_DESC_LEN);

    /* Устанавливаем TAIL в последний дескриптор,
     * сообщая NIC, что все буферы доступны для записи. */
    writel(ring->count - 1, adapter->hw_addr + RX_DESC_TAIL);

    return 0;

err_free:
    /* Обработка ошибок: освобождаем уже выделенные буферы */
    cleanup_rx_ring(adapter);
    return -ENOMEM;
}
```

Обратите внимание на ключевой момент: `dma_alloc_coherent()` выделяет память, видимую одновременно процессору и устройству. Это DMA-когерентная память — CPU и NIC работают с одними и теми же физическими адресами. Когда NIC записывает пакет в буфер через DMA, CPU может прочитать данные без дополнительных операций синхронизации кэша (для когерентных аллокаций).

### Размер кольцевого буфера — почему это важно

Размер RX-ринга по умолчанию — обычно 256 или 512 дескрипторов. Для высоконагруженных серверов этого может быть недостаточно. Если ядро не успевает забирать пакеты (например, SoftIRQ заблокирован другой работой), ринг переполняется, и NIC начинает отбрасывать пакеты аппаратно.

```bash
# Просмотр текущего и максимального размера ринга
ethtool -g eth0

# Типичный вывод:
# Ring parameters for eth0:
# Pre-set maximums:
# RX:     4096
# TX:     4096
# Current hardware settings:
# RX:     256      <-- по умолчанию может быть мало!
# TX:     256

# Увеличение размера ринга
ethtool -G eth0 rx 4096 tx 4096
```

### Диагностика: где искать потери на уровне NIC

```bash
# Счётчики ошибок NIC (специфичны для драйвера и модели)
ethtool -S eth0 | grep -E 'rx_missed|rx_dropped|rx_no_buffer|rx_errors'

# rx_missed_errors  — NIC отбросил пакет, потому что ринг был полон.
#                     Это АППАРАТНАЯ потеря. Пакет не дошёл до ядра.
#
# rx_no_buffer_count — то же самое на некоторых драйверах Intel.
#
# rx_errors          — аппаратные ошибки (CRC, runt frames и т.д.)
#
# rx_dropped         — может означать разное в зависимости от драйвера;
#                     иногда это потери на уровне NIC, иногда — ядра.
```

**Правило:** если `rx_missed_errors` растёт — ядро не успевает забирать пакеты. Решения:

1. Увеличить размер RX-ринга (`ethtool -G`).
2. Проверить, что NAPI работает корректно (часть 1.2).
3. Проверить распределение прерываний по CPU (часть 1.3).
4. Если ничего не помогает — рассматривать kernel bypass (часть 1.6).

---

## Часть 1.2: Hard IRQ и NAPI

### Проблема штормов прерываний (Interrupt Storm)

В наивной модели обработки каждый входящий пакет вызывает аппаратное прерывание (Hard IRQ). CPU прерывает текущую работу, сохраняет контекст, выполняет обработчик прерывания, восстанавливает контекст и возвращается к прерванной задаче. На скорости 100 Мбит/с с пакетами минимального размера (64 байта) это примерно 150 000 прерываний в секунду — терпимо.

На скорости 10 Гбит/с с пакетами минимального размера это уже **~14,88 миллионов прерываний в секунду**. На 100 Гбит/с — почти 150 миллионов. При таких числах CPU проводит всё время в обработчиках прерываний, не успевая выполнять полезную работу — это и есть **interrupt storm** (шторм прерываний), также известный как **livelock**.

### NAPI: гибридный подход

**NAPI** (New API) — механизм, появившийся в ядре Linux ещё в версии 2.5/2.6, который решает проблему interrupt storm через гибридную модель «прерывание + опрос»:

1. **Приходит пакет** — NIC генерирует аппаратное прерывание (Hard IRQ).
2. **Hard IRQ обработчик** — выполняет минимум работы: **отключает прерывания от NIC** и **регистрирует NAPI poll** для обработки в SoftIRQ контексте.
3. **SoftIRQ (NET_RX_SOFTIRQ)** — вызывает NAPI poll-функцию, которая **опрашивает** (polling) ринг буфер и обрабатывает пакеты пачками (до `budget` штук за один вызов).
4. **Если все пакеты обработаны** — NAPI завершает polling и **включает прерывания обратно**.
5. **Если пакетов больше, чем budget** — NAPI возвращает управление (чтобы не монополизировать CPU), и SoftIRQ вызовет poll снова.

```
    Временная шкала обработки пакетов с NAPI
    ==========================================

    Пакет 1   Пакеты 2,3,4,5 приходят пока идёт polling
    │         │ │ │ │
    ▼         ▼ ▼ ▼ ▼
    ┌──────┐  ┌──────────────────────────────────┐  ┌──────┐
    │HardIRQ│  │  SoftIRQ: NAPI poll              │  │HardIRQ│
    │(быстр)│  │  Обрабатываем пакеты 1,2,3,4,5  │  │(следу-│
    │Откл.  │──►  из ринга пачкой.                │──►ющий  │
    │прерыв.│  │  Прерывания ОТКЛЮЧЕНЫ.           │  │цикл) │
    └──────┘  │  NIC копирует через DMA молча.    │  └──────┘
               └──────────────────────────────────┘
               ▲                                    ▲
               │                                    │
          1 прерывание                         1 прерывание
          на всю пачку!                        (а не 5)
```

Ключевое преимущество: **вместо одного прерывания на каждый пакет — одно прерывание на целую пачку**. При высокой нагрузке это снижает число прерываний на порядки.

### Hard IRQ обработчик в коде

```c
/*
 * Обработчик аппаратного прерывания NIC.
 * Должен выполняться максимально быстро —
 * мы находимся в контексте, где все прерывания
 * на данном CPU отключены.
 *
 * Упрощено на основе drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
 */
static irqreturn_t nic_irq_handler(int irq, void *data)
{
    struct queue_vector *qv = data;
    struct adapter *adapter = qv->adapter;

    /*
     * Проверяем, действительно ли прерывание от нашего устройства.
     * На shared IRQ линиях это критически важно.
     */
    u32 icr = readl(adapter->hw_addr + IRQ_CAUSE_REG);
    if (!(icr & IRQ_RX_BIT))
        return IRQ_NONE;  /* Прерывание не от нас */

    /*
     * КЛЮЧЕВОЙ МОМЕНТ: отключаем прерывания от NIC.
     * Пока мы будем обрабатывать пакеты в polling-режиме,
     * NIC будет молча складывать новые пакеты в ринг через DMA,
     * не дёргая CPU прерываниями.
     */
    writel(0, adapter->hw_addr + IRQ_MASK_REG);

    /*
     * Планируем NAPI poll. Эта функция добавляет наш
     * napi_struct в per-CPU список на обработку
     * и поднимает флаг NET_RX_SOFTIRQ.
     *
     * napi_schedule_prep() проверяет, не запланирован ли
     * poll уже (защита от повторного планирования).
     */
    if (napi_schedule_prep(&qv->napi))
        __napi_schedule(&qv->napi);

    return IRQ_HANDLED;
}
```

### NAPI poll-функция

```c
/*
 * NAPI poll-функция — «рабочая лошадка» приёма пакетов.
 * Вызывается из контекста SoftIRQ (NET_RX_SOFTIRQ).
 *
 * @napi   — указатель на napi_struct (зарегистрирован при инициализации)
 * @budget — максимальное число пакетов для обработки за один вызов.
 *           По умолчанию 64 (sysctl net.core.netdev_budget_usecs / weight).
 *           Это предотвращает монополизацию CPU одним интерфейсом.
 *
 * Возвращает: число реально обработанных пакетов.
 */
static int driver_napi_poll(struct napi_struct *napi, int budget)
{
    struct queue_vector *qv = container_of(napi, struct queue_vector, napi);
    struct rx_ring *ring = qv->rx_ring;
    int work_done = 0;

    /* Цикл обработки: забираем пакеты из ринга, пока есть что забирать
     * и пока не исчерпали budget. */
    while (work_done < budget) {
        struct rx_desc *desc = &ring->desc[ring->next_to_clean];

        /*
         * Проверяем статусный бит DD (Descriptor Done) —
         * NIC устанавливает его после записи пакета через DMA.
         * rmb() — барьер чтения памяти, гарантирует, что мы
         * видим актуальные данные после DMA-записи NIC.
         */
        rmb();
        if (!(desc->status_error & RX_DESC_STATUS_DD))
            break;  /* Больше готовых пакетов нет */

        /* Получаем данные пакета и длину */
        struct page *page = ring->buffer_info[ring->next_to_clean].page;
        unsigned int len = le16_to_cpu(desc->length);

        /*
         * Создаём sk_buff (подробно — в части 1.4)
         * и копируем/маппим данные из DMA-буфера.
         */
        struct sk_buff *skb = napi_alloc_skb(napi, len);
        if (!skb)
            break;  /* Нет памяти — выходим, попробуем позже */

        /* Заполняем sk_buff данными из DMA-буфера */
        memcpy(skb_put(skb, len),
               page_address(page), len);

        /* Устанавливаем протокол (ethernet) */
        skb->protocol = eth_type_trans(skb, napi->dev);

        /*
         * Передаём пакет вверх по сетевому стеку.
         * napi_gro_receive() пытается агрегировать пакеты
         * одного потока (GRO — подробнее в части 1.5).
         */
        napi_gro_receive(napi, skb);

        /* Подготавливаем дескриптор для повторного использования:
         * выделяем новый буфер и обновляем дескриптор. */
        alloc_new_rx_buffer(ring, ring->next_to_clean);

        ring->next_to_clean = (ring->next_to_clean + 1) % ring->count;
        work_done++;
    }

    /*
     * Если обработали меньше, чем budget — значит,
     * ринг пуст, все пакеты забраны.
     * Завершаем NAPI polling и ВКЛЮЧАЕМ прерывания обратно.
     */
    if (work_done < budget) {
        napi_complete_done(napi, work_done);

        /* Включаем прерывания от NIC.
         * Следующий пришедший пакет снова вызовет Hard IRQ. */
        writel(IRQ_RX_BIT, adapter->hw_addr + IRQ_MASK_REG);

        /*
         * ВАЖНЫЙ НЮАНС: между napi_complete_done() и writel()
         * мог прийти пакет. Проверяем ринг ещё раз,
         * чтобы не потерять его (race condition).
         */
        rmb();
        if (ring->desc[ring->next_to_clean].status_error & RX_DESC_STATUS_DD) {
            /* Пакет пришёл в зазор — перепланируем NAPI */
            if (napi_schedule_prep(napi))
                __napi_schedule(napi);
        }
    }

    return work_done;
}
```

Обратите внимание на проверку race condition в конце функции. Это классическая проблема: между моментом, когда мы решили «пакетов больше нет», и моментом, когда мы включили прерывания, NIC мог успеть записать новый пакет через DMA. Без этой проверки пакет бы «завис» в ринге до следующего прерывания.

### Interrupt Coalescing (Объединение прерываний)

Помимо NAPI, сами NIC поддерживают **interrupt coalescing** — аппаратную задержку прерываний:

```bash
# Просмотр текущих настроек coalescing
ethtool -c eth0

# Типичный вывод:
# rx-usecs: 3           — задержка в микросекундах перед генерацией прерывания
# rx-frames: 0          — или по количеству накопленных фреймов
# adaptive-rx: off      — адаптивный режим (NIC сам подбирает)

# Настройка: генерировать прерывание не чаще чем раз в 50 мкс
# или при накоплении 64 фреймов (что наступит раньше)
ethtool -C eth0 rx-usecs 50 rx-frames 64

# Адаптивный режим — NIC автоматически подбирает параметры
# в зависимости от текущей нагрузки
ethtool -C eth0 adaptive-rx on
```

**Компромисс:** coalescing снижает число прерываний (хорошо для throughput), но увеличивает латентность (плохо для latency-sensitive приложений). Для trading-систем и real-time приложений coalescing обычно минимизируют или отключают.

### ksoftirqd: когда SoftIRQ не справляется

SoftIRQ обработка (включая NAPI poll) выполняется в контексте `do_softirq()`, который вызывается при выходе из Hard IRQ. Но если SoftIRQ накапливаются быстрее, чем обрабатываются, ядро ограничивает их обработку (по времени и количеству итераций), чтобы не оставить userspace без CPU.

В этом случае оставшаяся работа передаётся потоку ядра **`ksoftirqd/N`** (по одному на каждый CPU):

```c
/*
 * Из kernel/softirq.c — упрощённая логика.
 *
 * После обработки SoftIRQ в контексте прерывания,
 * если остались необработанные SoftIRQ —
 * пробуждаем ksoftirqd.
 */
asmlinkage __visible void __softirq_entry __do_softirq(void)
{
    /* ... обработка SoftIRQ ... */

    pending = local_softirq_pending();
    if (pending) {
        /*
         * MAX_SOFTIRQ_RESTART (обычно 10) итераций пройдено,
         * а SoftIRQ всё ещё есть. Пробуждаем ksoftirqd,
         * чтобы не морить голодом userspace-процессы.
         */
        if (time_before(jiffies, end) && !need_resched() &&
            --max_restart)
            goto restart;  /* Ещё одна итерация */

        wakeup_softirqd();  /* Передаём работу ksoftirqd */
    }
}
```

**Диагностика:** если `ksoftirqd` потребляет значительный процент CPU (видно в `top` или `htop`), это сигнал о том, что сетевая нагрузка слишком высока для текущей конфигурации. Решения: масштабирование по CPU (часть 1.3), увеличение бюджета NAPI, или kernel bypass (часть 1.6).

```bash
# Параметры бюджета NAPI
sysctl net.core.netdev_budget        # Макс. пакетов за один цикл SoftIRQ (по умолчанию 300)
sysctl net.core.netdev_budget_usecs  # Макс. время на один цикл SoftIRQ в мкс (по умолчанию 2000)

# Увеличение бюджета (если ksoftirqd загружен)
sysctl -w net.core.netdev_budget=600
sysctl -w net.core.netdev_budget_usecs=4000
```

---

## Часть 1.3: Масштабирование (Scaling)

### Зачем масштабировать обработку пакетов

На современном сервере может быть 32, 64 или даже 128 ядер CPU. Без специальных механизмов масштабирования все пакеты одного NIC обрабатываются **одним** CPU (тем, на который приходит прерывание). При скоростях 10+ Гбит/с одно ядро не способно обработать весь поток пакетов — оно упирается в потолок производительности, в то время как остальные ядра простаивают.

Linux предоставляет несколько механизмов для распределения нагрузки:

### RSS — Receive Side Scaling (аппаратное)

**RSS** — наиболее эффективный механизм масштабирования, реализованный в аппаратуре NIC. NIC вычисляет хеш от заголовков пакета (обычно 4-tuple: src IP, dst IP, src port, dst port) и направляет пакет в одну из нескольких **аппаратных RX-очередей**. Каждая очередь привязана к своему прерыванию, а значит — к своему CPU.

```
    RSS: аппаратное распределение по очередям
    ===========================================

    Входящие пакеты:
    ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
    │Пакет│ │Пакет│ │Пакет│ │Пакет│ │Пакет│ │Пакет│
    │ A:1 │ │ B:2 │ │ A:1 │ │ C:3 │ │ B:2 │ │ A:1 │
    └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘
       │       │       │       │       │       │
       ▼       ▼       ▼       ▼       ▼       ▼
    ┌──────────────────────────────────────────────┐
    │           NIC: hash = toeplitz(4-tuple)      │
    │           queue = indirection_table[hash]    │
    └──────┬──────────┬──────────┬─────────────────┘
           │          │          │
           ▼          ▼          ▼
       ┌───────┐  ┌───────┐  ┌───────┐
       │Queue 0│  │Queue 1│  │Queue 2│
       │A:1×3  │  │B:2×2  │  │C:3×1  │
       └───┬───┘  └───┬───┘  └───┬───┘
           │          │          │
           ▼          ▼          ▼
        CPU 0      CPU 1      CPU 2

    Пакеты одного потока (A:1) всегда попадают
    в одну очередь → обрабатываются одним CPU
    → гарантия порядка внутри потока.
```

**Хеш-функция:** большинство NIC используют **Toeplitz hash** — алгоритм, дающий хорошее распределение при использовании случайного ключа.

```c
/*
 * Toeplitz hash — упрощённая реализация.
 * Используется в RSS для маппинга 4-tuple → номер очереди.
 *
 * Реальная реализация — в аппаратуре NIC; в ядре
 * используется для RPS (программная эмуляция RSS).
 *
 * Из include/linux/netdevice.h и net/core/flow_dissector.c
 */
static inline u32 toeplitz_hash(const u8 *input, int len,
                                const u8 *key)
{
    u32 hash = 0;
    int i, bit;

    for (i = 0; i < len; i++) {
        for (bit = 7; bit >= 0; bit--) {
            if (input[i] & (1 << bit)) {
                /* XOR с 32-битным окном из ключа,
                 * сдвинутым на текущую позицию */
                hash ^= get_unaligned_be32(key + (i * 8 + (7 - bit)) / 8);
            }
        }
    }

    return hash;
}

/*
 * Indirection table: маппинг hash → очередь.
 * Таблица обычно имеет 128 или 512 записей.
 * Позволяет гибко распределять нагрузку (например,
 * направить 2/3 трафика на мощные ядра).
 */
static inline int rss_hash_to_queue(u32 hash,
                                     const u16 *indir_table,
                                     int table_size)
{
    /* Младшие биты хеша — индекс в таблице */
    return indir_table[hash & (table_size - 1)];
}
```

**Настройка RSS:**

```bash
# Просмотр indirection table
ethtool -x eth0

# Настройка: распределить по первым 8 очередям
ethtool -X eth0 equal 8

# Или задать вручную (пример: очереди 0,1 получают больше)
ethtool -X eth0 weight 3 3 1 1 1 1 1 1

# Просмотр и настройка хеш-ключа
ethtool -x eth0 --show-hash-key

# Какие поля участвуют в хешировании
ethtool -n eth0 rx-flow-hash tcp4
# Обычно: sdfn = src/dst IP + src/dst port (4-tuple)
```

### RPS — Receive Packet Steering (программное)

**RPS** — программная эмуляция RSS для NIC без аппаратной поддержки множественных очередей (или если очередей меньше, чем CPU). Работает в ядре: после NAPI poll вычисляет хеш пакета и передаёт `sk_buff` на целевой CPU через Inter-Processor Interrupt (**IPI**).

```c
/*
 * Из net/core/dev.c — логика RPS.
 * Вызывается из netif_receive_skb() / __netif_receive_skb().
 */
static int get_rps_cpu(struct net_device *dev,
                       struct sk_buff *skb,
                       struct rps_dev_flow **rflowp)
{
    struct rps_map *map;
    struct rps_sock_flow_table *sock_flow_table;
    u32 hash;
    int cpu = -1;

    /* Получаем RPS map для данной RX-очереди */
    map = rcu_dereference(rxqueue->rps_map);
    if (!map)
        return -1;  /* RPS не настроен */

    /*
     * Вычисляем хеш пакета. Если NIC уже вычислил хеш
     * (через RSS), используем его — не тратим CPU повторно.
     */
    hash = skb_get_hash(skb);
    if (!hash)
        return -1;

    /*
     * Проверяем RFS (Receive Flow Steering) — если настроен,
     * пытаемся направить пакет на CPU, где приложение
     * вызывает recv(). Подробнее — ниже.
     */
    sock_flow_table = rcu_dereference(rps_sock_flow_table);
    if (sock_flow_table) {
        /* RFS-логика: определяем CPU по потоку */
        cpu = rfs_select_cpu(sock_flow_table, hash, rflowp);
        if (cpu >= 0)
            return cpu;
    }

    /* Фолбэк: маппинг hash → CPU из RPS map */
    cpu = map->cpus[reciprocal_scale(hash, map->len)];

    return cpu;
}

/*
 * После определения целевого CPU пакет ставится
 * в per-CPU backlog queue и отправляется IPI.
 *
 * IPI (Inter-Processor Interrupt) — прерывание,
 * которое один CPU отправляет другому, чтобы тот
 * обработал входящую очередь пакетов.
 * Это дополнительные накладные расходы RPS по сравнению с RSS.
 */
```

**Настройка RPS:**

```bash
# Включить RPS для очереди 0 интерфейса eth0:
# маска CPU в шестнадцатеричном формате.
# ff = CPU 0-7, ffff = CPU 0-15

echo "ff" > /sys/class/net/eth0/queues/rx-0/rps_cpus

# Размер backlog-очереди (на каждый CPU)
sysctl -w net.core.netdev_max_backlog=5000
```

**Важно:** RPS добавляет накладные расходы IPI (порядка 1-2 мкс на передачу). Если NIC поддерживает RSS, всегда предпочитайте RSS — нулевые накладные расходы, распределение происходит в аппаратуре.

### RFS — Receive Flow Steering

**RFS** — расширение RPS, которое направляет пакет не просто на «какой-нибудь CPU по хешу», а на тот CPU, где приложение **последний раз вызывало `recv()/recvmsg()`** на данном сокете. Это улучшает **cache locality** — данные пакета уже в L1/L2 кэше того CPU, где приложение будет их читать.

```bash
# Включение RFS: размер таблицы потоков
# Рекомендация: >= ожидаемого числа одновременных потоков
sysctl -w net.core.rps_sock_flow_entries=32768

# Для каждой очереди
echo 2048 > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt
```

### XPS — Transmit Packet Steering

**XPS** — механизм для отправки (TX). Определяет, какую TX-очередь использовать при отправке пакета с данного CPU. Оптимизирует cache locality и уменьшает contention на блокировках TX-очередей.

```bash
# Привязка TX-очереди 0 к CPU 0, TX-очереди 1 к CPU 1, и т.д.
echo 1  > /sys/class/net/eth0/queues/tx-0/xps_cpus
echo 2  > /sys/class/net/eth0/queues/tx-1/xps_cpus
echo 4  > /sys/class/net/eth0/queues/tx-2/xps_cpus
echo 8  > /sys/class/net/eth0/queues/tx-3/xps_cpus
```

### Сводная таблица механизмов масштабирования

| Механизм | Уровень | Направление | Что делает | Накладные расходы |
|----------|---------|-------------|------------|-------------------|
| **RSS**  | Аппаратура | RX | Hash → HW queue → CPU | Нулевые (в NIC) |
| **RPS**  | Ядро (Software) | RX | Hash → enqueue → IPI → CPU | IPI ~1-2 мкс |
| **RFS**  | Ядро (Software) | RX | Направляет на CPU приложения | IPI + lookup |
| **XPS**  | Ядро (Software) | TX | CPU → оптимальная TX queue | Минимальные |

---

## Часть 1.4: sk_buff — Тяжёлая ноша ядра

### Зачем нам структура в 200+ байт на каждый пакет

`struct sk_buff` (socket buffer) — центральная структура сетевого стека Linux. **Каждый** пакет, обрабатываемый ядром, представлен экземпляром `sk_buff`. Эта структура содержит не только указатели на данные пакета, но и обширные метаданные: информацию о сетевом интерфейсе, протокольных заголовках, маршрутизации, временных метках, информации о VLAN, метках conntrack и многом другом.

Проблема: `struct sk_buff` занимает **более 200 байт** (в зависимости от конфигурации ядра и версии — от 232 до 320 байт). При обработке 10 миллионов пакетов в секунду (10 Mpps, типичная нагрузка для 10 GbE с мелкими пакетами) это означает:

- **10 000 000 аллокаций/деаллокаций `sk_buff` в секунду**
- **~2.3 ГБ/с** только на метаданные (при 240 байтах на структуру)
- Огромная нагрузка на SLAB/SLUB аллокатор ядра
- Постоянное давление на CPU кэши

Именно эта стоимость — одна из главных причин появления технологий kernel bypass (DPDK, XDP).

### Ключевые поля struct sk_buff

```c
/*
 * Упрощённая выборка из include/linux/skbuff.h
 * Показаны наиболее важные поля для понимания
 * обработки пакетов.
 *
 * Полная структура: ~50 полей, 200-320 байт.
 */
struct sk_buff {
    /*
     * === Связный список ===
     * sk_buff'ы объединяются в двусвязные списки
     * (например, очередь сокета, backlog CPU).
     */
    struct sk_buff      *next;
    struct sk_buff      *prev;

    /*
     * === Привязка к сокету и устройству ===
     */
    struct sock         *sk;        /* Сокет-владелец (NULL для входящих
                                     * пакетов до lookup'а сокета) */
    struct net_device   *dev;       /* Сетевой интерфейс, через который
                                     * пакет пришёл или будет отправлен */

    /*
     * === Временные метки ===
     * Используются для SO_TIMESTAMP, TCP RTT и т.д.
     */
    ktime_t             tstamp;

    /*
     * === Указатели на данные пакета ===
     *
     *   head        data              tail         end
     *    │           │                  │            │
     *    ▼           ▼                  ▼            ▼
     *    ┌───────────┬──────────────────┬────────────┐
     *    │ headroom  │   packet data    │  tailroom  │
     *    │           │ (заголовки+тело) │            │
     *    └───────────┴──────────────────┴────────────┘
     *
     *    headroom — резерв для добавления заголовков
     *              (например, при инкапсуляции).
     *    tailroom — резерв для роста данных.
     *
     *    При передаче вниз по стеку (от L4 к L2):
     *    skb_push() сдвигает data влево, добавляя заголовок.
     *
     *    При передаче вверх (от L2 к L4):
     *    skb_pull() сдвигает data вправо, «снимая» заголовок.
     */
    unsigned char       *head;      /* Начало выделенного буфера */
    unsigned char       *data;      /* Начало текущих данных пакета */
    unsigned int        tail;       /* Конец текущих данных */
    unsigned int        end;        /* Конец выделенного буфера */

    /*
     * === Длина данных ===
     */
    unsigned int        len;        /* Полная длина данных пакета
                                     * (включая фрагменты в shared info) */
    unsigned int        data_len;   /* Длина данных в paged-фрагментах
                                     * (len - data_len = линейные данные) */

    /*
     * === Протокольная информация ===
     */
    __be16              protocol;   /* Протокол L3 (ETH_P_IP, ETH_P_IPV6...) */
    __u16               transport_header;  /* Смещение транспортного заголовка
                                            * (TCP/UDP) от head */
    __u16               network_header;    /* Смещение сетевого заголовка
                                            * (IP) от head */
    __u16               mac_header;        /* Смещение MAC-заголовка
                                            * (Ethernet) от head */

    /*
     * === Хеш для RSS/RPS ===
     */
    __u32               hash;       /* Хеш пакета (от NIC или sw) */

    /*
     * === Битовые поля (компактно упакованные) ===
     */
    __u8                pkt_type:3; /* Тип: PACKET_HOST, PACKET_BROADCAST... */
    __u8                ip_summed:2;/* Статус контрольной суммы:
                                     * CHECKSUM_NONE — не проверена
                                     * CHECKSUM_UNNECESSARY — NIC проверил
                                     * CHECKSUM_COMPLETE — NIC предоставил */

    /*
     * === Queue mapping ===
     */
    __u16               queue_mapping; /* TX-очередь (для XPS) */

    /*
     * === Информация о фрагментах (для scatter/gather I/O) ===
     * Хранится в конце буфера (skb_shared_info).
     * Позволяет sk_buff ссылаться на несколько страниц памяти
     * без копирования (zero-copy).
     */
    /* ... skb_shared_info доступен через skb_shinfo(skb) ... */
};
```

### Операции с указателями данных

```c
/*
 * Как протокольные уровни работают с данными пакета
 * через манипуляцию указателями sk_buff.
 *
 * Это нулевое копирование (zero-copy) в пределах ядра:
 * данные не перемещаются, сдвигаются только указатели.
 */

/* === Приём пакета (снизу вверх) === */

/* Драйвер заполнил sk_buff данными Ethernet-фрейма.
 * data указывает на начало Ethernet-заголовка. */

/* L2 → L3: «снимаем» Ethernet-заголовок */
skb_pull(skb, ETH_HLEN);          /* data сдвигается вправо на 14 байт */
skb_reset_network_header(skb);     /* network_header = data */

/* L3 → L4: «снимаем» IP-заголовок */
struct iphdr *iph = ip_hdr(skb);   /* Читаем IP-заголовок */
int ip_hdr_len = iph->ihl * 4;
skb_pull(skb, ip_hdr_len);         /* data сдвигается вправо */
skb_reset_transport_header(skb);    /* transport_header = data */

/* Теперь data указывает на TCP/UDP payload.
 * Все заголовки ниже доступны по сохранённым смещениям. */


/* === Отправка пакета (сверху вниз) === */

/* Приложение записало данные. data указывает на payload. */

/* L4: добавляем TCP-заголовок */
struct tcphdr *th = skb_push(skb, sizeof(struct tcphdr));
/* data сдвинулся ВЛЕВО. th указывает на начало TCP-заголовка. */
skb_reset_transport_header(skb);

/* L3: добавляем IP-заголовок */
struct iphdr *iph = skb_push(skb, sizeof(struct iphdr));
skb_reset_network_header(skb);

/* L2: добавляем Ethernet-заголовок */
struct ethhdr *eth = skb_push(skb, ETH_HLEN);
skb_reset_mac_header(skb);

/* Теперь data указывает на начало Ethernet-фрейма,
 * и пакет готов к передаче драйверу. */
```

### Стоимость аллокации и SLAB

```bash
# Статистика SLAB/SLUB аллокатора для sk_buff
# (покажет число аллокаций, размер объекта, использование кэша)
cat /proc/slabinfo | head -2
cat /proc/slabinfo | grep skbuff

# Или через slabtop для динамического мониторинга
sudo slabtop -o | grep skbuff

# Типичный вывод:
# skbuff_head_cache    12345  15000   256   16    1 : ...
#                      ^актив ^всего ^размер
```

При 10 Mpps аллокатор должен выделять и освобождать ~10 млн объектов `sk_buff` в секунду. Даже с оптимизациями SLUB (per-CPU free lists, batch allocation через `napi_alloc_skb()`) это создаёт существенную нагрузку. Именно поэтому XDP (часть 1.6) обрабатывает пакеты **до** создания `sk_buff`, а DPDK обходит ядро целиком.

---

## Часть 1.5: Offloading — Как железо делает работу за нас

### TSO — TCP Segmentation Offload (передача)

При отправке данных TCP разбивает поток на сегменты размером MSS (Maximum Segment Size, обычно 1460 байт для MTU 1500). Без TSO ядро создаёт отдельный `sk_buff` для каждого сегмента, добавляет IP и TCP заголовки, вычисляет контрольные суммы — и всё это на CPU.

**С TSO** ядро передаёт NIC один большой «суперпакет» (до 64 КБ), а NIC **аппаратно** разбивает его на сегменты с правильными заголовками и контрольными суммами:

```
    Без TSO (CPU делает всю работу):
    =================================
    Приложение: write(socket, data, 64000)

    Ядро TCP:
    ┌──────┐ ┌──────┐ ┌──────┐     ┌──────┐
    │1460 B│ │1460 B│ │1460 B│ ... │1460 B│  ← 44 сегмента!
    │+hdrs │ │+hdrs │ │+hdrs │     │+hdrs │  ← 44 набора заголовков!
    └──┬───┘ └──┬───┘ └──┬───┘     └──┬───┘
       │        │        │             │
       ▼        ▼        ▼             ▼
      NIC отправляет 44 отдельных фрейма


    С TSO (NIC разбивает):
    =================================
    Приложение: write(socket, data, 64000)

    Ядро TCP:
    ┌────────────────────────────────────────┐
    │  Один «суперпакет» ~64000 B + 1 заголовок │
    └───────────────────┬────────────────────┘
                        │
                        ▼
    NIC аппаратно разбивает на 44 фрейма
    с правильными seq numbers и checksums
```

Выигрыш: вместо 44 вызовов стека (аллокация `sk_buff`, формирование заголовков, вычисление checksums) — один. Это радикально снижает нагрузку на CPU.

**Почему Wireshark показывает «гигантские» пакеты:**

Когда вы захватываете трафик на отправляющей стороне, tcpdump/Wireshark видит пакеты **до** аппаратной сегментации. Поэтому вы видите TCP-сегменты по 64 КБ при MTU 1500. Это не ошибка — это TSO в действии. Чтобы увидеть реальные фреймы, нужно захватывать на принимающей стороне или на промежуточном устройстве.

### GSO — Generic Segmentation Offload (программный TSO)

**GSO** — программный аналог TSO. Если NIC не поддерживает TSO (или пакет проходит через виртуальный интерфейс, туннель, netfilter и т.д.), сегментация выполняется в ядре, но **откладывается до последнего момента** — прямо перед передачей драйверу:

```c
/*
 * Упрощённая логика GSO-сегментации.
 * Из net/core/skbuff.c и net/ipv4/tcp_offload.c
 *
 * Вызывается из dev_hard_start_xmit(), если NIC
 * не поддерживает аппаратную сегментацию для
 * данного типа пакетов.
 */
struct sk_buff *tcp_gso_segment(struct sk_buff *skb,
                                netdev_features_t features)
{
    struct tcphdr *th;
    struct sk_buff *segs = NULL;
    struct sk_buff *tail = NULL;
    unsigned int mss = skb_shinfo(skb)->gso_size; /* MSS из метаданных */
    unsigned int seq = ntohl(th->seq);
    unsigned int delta;

    /*
     * Разбиваем большой sk_buff на цепочку маленьких.
     * Каждый сегмент получает свой TCP-заголовок
     * с правильным sequence number.
     */
    while (skb->len > mss) {
        struct sk_buff *nseg;

        /* Выделяем новый sk_buff для сегмента */
        nseg = alloc_skb(hdr_len + mss, GFP_ATOMIC);
        if (!nseg)
            goto err;

        /* Копируем заголовки */
        skb_copy_bits(skb, 0, skb_put(nseg, hdr_len + mss),
                      hdr_len + mss);

        /* Устанавливаем корректный sequence number */
        th = tcp_hdr(nseg);
        th->seq = htonl(seq);
        seq += mss;

        /* Пересчитываем IP total length */
        ip_hdr(nseg)->tot_len = htons(hdr_len + mss - ETH_HLEN);

        /*
         * Пересчитываем контрольные суммы.
         * Это CPU-интенсивная операция — именно поэтому
         * аппаратный TSO предпочтительнее.
         */
        tcp_gso_csum(nseg);

        /* Добавляем в цепочку сегментов */
        if (!segs)
            segs = nseg;
        else
            tail->next = nseg;
        tail = nseg;

        /* Отрезаем обработанную часть от исходного sk_buff */
        skb_pull(skb, mss);
    }

    /* Последний сегмент (остаток) */
    /* ... обработка аналогична ... */

    return segs;

err:
    kfree_skb_list(segs);
    return ERR_PTR(-ENOMEM);
}
```

### LRO и GRO — агрегация на приёме

**LRO (Large Receive Offload)** — аппаратная агрегация входящих пакетов одного TCP-потока в один большой «суперпакет». Обратная операция к TSO. Проблема LRO: NIC слепо объединяет пакеты, теряя информацию об отдельных заголовках. **Это ломает маршрутизацию и bridging**, потому что промежуточное устройство должно пересылать пакеты в исходном виде.

**GRO (Generic Receive Offload)** — программная замена LRO, реализованная в ядре. GRO умнее: она агрегирует пакеты, **сохраняя возможность восстановить** исходные фреймы, если это необходимо (например, для форвардинга).

```bash
# Проверка статуса offloading
ethtool -k eth0

# Типичный вывод (сокращённо):
# tcp-segmentation-offload: on     ← TSO
# generic-segmentation-offload: on ← GSO
# generic-receive-offload: on      ← GRO
# large-receive-offload: off       ← LRO (часто выключен по умолчанию)
# rx-checksumming: on              ← Checksum offload (RX)
# tx-checksumming: on              ← Checksum offload (TX)

# Отключить LRO (если включён на маршрутизаторе!)
ethtool -K eth0 lro off

# Отключить GRO (для отладки, чтобы видеть реальные пакеты)
ethtool -K eth0 gro off

# Отключить TSO (для отладки)
ethtool -K eth0 tso off
```

**Правило:** на маршрутизаторах и бриджах **никогда** не используйте аппаратный LRO. GRO безопасен для маршрутизации, потому что `skb_gro_receive()` сохраняет информацию о фрагментах.

### Checksum Offload

NIC может вычислять и проверять контрольные суммы IP, TCP, UDP аппаратно, освобождая CPU. Это приводит к характерному артефакту при захвате трафика:

```
    Почему Wireshark показывает «Bad checksum»
    ============================================

    При ОТПРАВКЕ:
    1. Ядро заполняет sk_buff данными.
    2. Ядро НЕ вычисляет контрольную сумму TCP/UDP,
       а ставит ip_summed = CHECKSUM_PARTIAL.
    3. tcpdump захватывает пакет ДО передачи NIC —
       поле checksum содержит мусор или 0.
    4. NIC вычисляет правильную checksum аппаратно
       и подставляет в пакет перед отправкой в сеть.

    Wireshark видит шаг 3, NIC делает шаг 4 — отсюда
    «ложное» предупреждение о неверной контрольной сумме.

    При ПРИЁМЕ:
    1. NIC проверяет checksum аппаратно.
    2. Если верна — ставит ip_summed = CHECKSUM_UNNECESSARY.
    3. Ядро доверяет NIC и НЕ проверяет повторно.
    4. Если NIC не поддерживает — ip_summed = CHECKSUM_NONE,
       и ядро проверяет программно.
```

```bash
# Отключение checksum offload для отладки (чтобы ядро считало само
# и tcpdump показывал правильные значения)
ethtool -K eth0 rx off tx off
```

---

## Часть 1.6: Kernel Bypass

### Почему ядро — узкое место

Подведём итоги того, что происходит с каждым пакетом в классическом стеке:

1. **Hard IRQ** — переключение контекста, сохранение регистров.
2. **NAPI poll** — создание `sk_buff` (240+ байт аллокация).
3. **Netfilter/iptables** — прохождение через цепочки правил.
4. **Протокольная обработка** — IP lookup, TCP state machine.
5. **Копирование в userspace** — `copy_to_user()` при `recv()`.

При 14.88 Mpps (line rate 10 GbE, 64-байтные пакеты) на обработку одного пакета остаётся **~67 наносекунд**. Классический стек ядра тратит на порядок больше. Отсюда — три подхода к обходу ядра.

### DPDK — Data Plane Development Kit

**DPDK** — полный обход ядра. Драйвер NIC работает в userspace, используя UIO или VFIO для прямого доступа к регистрам устройства и DMA-памяти.

**Ключевые принципы DPDK:**

```
    Классический стек           DPDK
    ==================          ==================
    ┌──────────────┐            ┌──────────────┐
    │  Application │            │  Application │
    ├──────────────┤            │  + DPDK libs │
    │  Socket API  │            │  (TCP/UDP —  │
    ├──────────────┤            │   сами!)     │
    │  TCP/IP      │            ├──────────────┤
    ├──────────────┤            │  PMD         │
    │  Netfilter   │            │  (Poll Mode  │
    ├──────────────┤            │   Driver)    │
    │  Driver      │            └──────┬───────┘
    ├──────────────┤                   │
    │  NIC         │            ┌──────┴───────┐
    └──────────────┘            │  NIC (VFIO)  │
                                └──────────────┘

    Всё, что между Application и NIC
    в классическом стеке — УБРАНО.
    Цена: вы сами реализуете TCP/IP.
```

**Архитектурные решения DPDK:**

- **PMD (Poll Mode Driver)** — драйвер работает в режиме постоянного опроса (busy-polling). Выделенное ядро CPU крутит бесконечный цикл `while(true) { poll_rx_queue(); }`. Никаких прерываний, никаких переключений контекста. CPU загружен на 100% даже без трафика — но латентность обработки минимальна.

- **Hugepages** — DPDK использует страницы памяти по 2 МБ или 1 ГБ вместо стандартных 4 КБ. Это критически важно: при работе с миллионами буферов пакетов обычные страницы создают огромное давление на TLB (Translation Lookaside Buffer), что приводит к TLB miss'ам и потере производительности.

- **NUMA-awareness** — на серверах с несколькими процессорами (NUMA nodes) DPDK выделяет память и привязывает потоки к тому же NUMA-узлу, к которому подключён NIC через PCIe. Обращение к памяти «чужого» NUMA-узла добавляет ~100 нс латентности.

```bash
# Настройка hugepages для DPDK
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
mount -t hugetlbfs nodev /dev/hugepages

# Привязка NIC к VFIO (отвязка от ядерного драйвера)
dpdk-devbind.py --bind=vfio-pci 0000:03:00.0

# После этого NIC НЕВИДИМ для ядра:
# ip link show — не покажет интерфейс
# ethtool — не будет работать
# tcpdump — не может захватывать
```

**Когда использовать DPDK:**
- NFV (Network Function Virtualization) — виртуальные маршрутизаторы, файрволы.
- Высокопроизводительные балансировщики нагрузки.
- Телеком-оборудование.
- Любые сценарии, где нужна обработка >10 Mpps на сервер.

**Цена:** полная изоляция от сетевого стека ядра. Ни iptables, ни tcpdump, ни стандартные сокеты. Нужно реализовать собственный протокольный стек или использовать существующие (mTCP, F-Stack, Seastar).

### XDP и eBPF — обработка на уровне драйвера

**XDP (eXpress Data Path)** — компромиссный подход. Программа eBPF загружается в ядро и выполняется **в контексте NAPI poll, до создания `sk_buff`**. Это даёт возможность принимать решения о судьбе пакета (отбросить, перенаправить, пропустить в стек) с минимальными накладными расходами.

```
    Путь пакета с XDP
    ==================

    NIC → DMA → Ring Buffer
                    │
                    ▼
              ┌───────────┐
              │ XDP program│ ← eBPF-программа, без sk_buff!
              │ (в NAPI)   │    Работает с xdp_buff (лёгкий).
              └─────┬─────┘
                    │
          ┌─────────┼──────────┬──────────┐
          ▼         ▼          ▼          ▼
      XDP_DROP  XDP_PASS   XDP_TX    XDP_REDIRECT
      (отброс)  (в стек)   (отправ.  (на другой
                            обратно)  интерфейс)
                    │
                    ▼
              Создание sk_buff
              (обычный путь ядра)
```

**Пример XDP-программы:**

```c
/*
 * Простая XDP-программа: подсчёт пакетов по протоколу
 * и отбрасывание пакетов с определённого IP.
 *
 * Компилируется с помощью clang -target bpf.
 * Загружается через ip link set dev eth0 xdp obj prog.o
 * или через libbpf / bpftool.
 */

#include <linux/bpf.h>
#include <linux/if_ether.h>
#include <linux/ip.h>
#include <bpf/bpf_helpers.h>

/* BPF-карта для хранения счётчиков пакетов по протоколу.
 * Ключ: номер IP-протокола (TCP=6, UDP=17, ...).
 * Значение: количество пакетов. */
struct {
    __uint(type, BPF_MAP_TYPE_HASH);
    __uint(max_entries, 256);
    __type(key, __u8);           /* IP protocol number */
    __type(value, __u64);        /* Счётчик пакетов */
} packet_count SEC(".maps");

/* IP-адрес, который будем блокировать (в network byte order).
 * 192.168.1.100 = 0x6401A8C0 в little-endian */
#define BLOCKED_IP __constant_htonl(0xC0A80164)

SEC("xdp")
int xdp_filter_and_count(struct xdp_md *ctx)
{
    /*
     * ctx->data и ctx->data_end — указатели на начало
     * и конец данных пакета в DMA-буфере.
     * Здесь НЕТ sk_buff — мы работаем напрямую
     * с данными в ring buffer.
     */
    void *data = (void *)(long)ctx->data;
    void *data_end = (void *)(long)ctx->data_end;

    /* Парсим Ethernet-заголовок.
     * ОБЯЗАТЕЛЬНАЯ проверка границ — верификатор eBPF
     * не пропустит программу без неё. */
    struct ethhdr *eth = data;
    if ((void *)(eth + 1) > data_end)
        return XDP_PASS;  /* Пакет слишком короткий */

    /* Нас интересует только IPv4 */
    if (eth->h_proto != __constant_htons(ETH_P_IP))
        return XDP_PASS;

    /* Парсим IP-заголовок */
    struct iphdr *iph = (void *)(eth + 1);
    if ((void *)(iph + 1) > data_end)
        return XDP_PASS;

    /*
     * Блокируем пакеты с определённого IP-адреса.
     * XDP_DROP отбрасывает пакет ДО создания sk_buff —
     * это в 4-5 раз быстрее, чем iptables DROP,
     * потому что iptables работает после создания sk_buff
     * и прохождения через весь протокольный стек L2/L3.
     */
    if (iph->saddr == BLOCKED_IP)
        return XDP_DROP;

    /* Обновляем счётчик для данного протокола */
    __u8 proto = iph->protocol;
    __u64 *count = bpf_map_lookup_elem(&packet_count, &proto);
    if (count) {
        /* Атомарный инкремент — программа может выполняться
         * параллельно на разных CPU */
        __sync_fetch_and_add(count, 1);
    } else {
        __u64 init_val = 1;
        bpf_map_update_elem(&packet_count, &proto, &init_val, BPF_ANY);
    }

    /* XDP_PASS — передаём пакет в обычный сетевой стек ядра.
     * Дальше создаётся sk_buff, и пакет идёт по стандартному пути. */
    return XDP_PASS;
}

/* Лицензия обязательна для eBPF-программ, использующих
 * GPL-only хелперы ядра. */
char _license[] SEC("license") = "GPL";
```

```bash
# Компиляция XDP-программы
clang -O2 -target bpf -c xdp_filter.c -o xdp_filter.o

# Загрузка в ядро (native mode — лучшая производительность,
# требует поддержки драйвером)
ip link set dev eth0 xdp obj xdp_filter.o sec xdp

# Или generic mode (работает с любым драйвером,
# но медленнее — после NAPI, до создания sk_buff)
ip link set dev eth0 xdpgeneric obj xdp_filter.o sec xdp

# Просмотр счётчиков из BPF-карты
bpftool map dump name packet_count

# Удаление XDP-программы
ip link set dev eth0 xdp off
```

### AF_XDP — гибридный подход

**AF_XDP** — специальный тип сокета, позволяющий передавать пакеты из XDP напрямую в userspace через shared memory (UMEM), минуя большую часть сетевого стека ядра. Это компромисс между DPDK (полный обход) и обычными сокетами (полный стек):

```
    AF_XDP: архитектура
    ====================

    ┌─────────────────────────────────┐
    │        Userspace Application    │
    │   ┌───────────────────────┐     │
    │   │  AF_XDP Socket        │     │
    │   │  (RX ring, TX ring,   │     │
    │   │   Fill ring, Comp ring)│    │
    │   └───────────┬───────────┘     │
    │               │ mmap (zero-copy)│
    ├───────────────┼─────────────────┤
    │   UMEM        │  (shared memory)│ ← Общая память
    ├───────────────┼─────────────────┤  между ядром и
    │ Kernel:       │                 │  приложением
    │   XDP program │                 │
    │   (XDP_REDIRECT │               │
    │    → xsk)     │                 │
    ├───────────────┼─────────────────┤
    │   NIC DMA     │                 │
    └───────────────┴─────────────────┘
```

- **Быстрее обычных сокетов** — нет `sk_buff`, нет протокольной обработки.
- **Проще DPDK** — NIC остаётся под управлением ядра, часть очередей работает как обычно.
- **Гибкость** — можно обрабатывать только определённый трафик через AF_XDP, остальной пойдёт по стандартному пути.

### Когда что использовать

| Подход | Производительность | Сложность | Совместимость со стеком | Применение |
|--------|-------------------|-----------|------------------------|------------|
| **Обычные сокеты** | ~1-2 Mpps | Низкая | Полная | Большинство приложений |
| **XDP/eBPF** | ~10-24 Mpps | Средняя | Частичная (фильтрация) | DDoS-защита, балансировка, мониторинг |
| **AF_XDP** | ~10-20 Mpps | Средняя | Частичная | Специализированная обработка потоков |
| **DPDK** | ~40-80 Mpps | Высокая | Нулевая | NFV, телеком, HFT |

---

## Практическое задание

### Задание 1: Анализ распределения прерываний

Проверьте, как прерывания от NIC распределены по CPU. Неравномерное распределение — частая причина потерь пакетов: один CPU перегружен, остальные простаивают.

```bash
# Показать прерывания от сетевого интерфейса
# Каждая строка — один вектор прерываний (обычно = одна RX/TX очередь).
# Столбцы — количество прерываний на каждом CPU.
cat /proc/interrupts | grep eth0

# Пример вывода:
#            CPU0       CPU1       CPU2       CPU3
#  45:    1234567         12          5          3   eth0-rx-0
#  46:          8    1234891          2          7   eth0-rx-1
#  47:          3          6    1234555         11   eth0-rx-2
#  48:          5          9          8    1234333   eth0-rx-3
#
# Хорошо: прерывания равномерно распределены (RSS работает).
# Плохо: все прерывания на одном CPU (RSS не настроен или 1 очередь).

# Проверить привязку прерываний (affinity)
for irq in $(grep eth0 /proc/interrupts | awk '{print $1}' | tr -d ':'); do
    echo "IRQ $irq: $(cat /proc/irq/$irq/smp_affinity_list)"
done
```

### Задание 2: Мониторинг SoftIRQ

Наблюдайте за счётчиками SoftIRQ в реальном времени. Если NET_RX растёт значительно быстрее на одном CPU — это bottleneck.

```bash
# Наблюдение в реальном времени (обновление каждую секунду)
watch -n 1 "cat /proc/softirqs | grep -E 'NET_RX|NET_TX'"

# Пример вывода:
#                    CPU0       CPU1       CPU2       CPU3
#       NET_TX:   1234567    1234891    1234555    1234333
#       NET_RX:  45678901   45679012   45678888   45679100
#
# Если числа сильно различаются между CPU —
# масштабирование работает плохо.

# Мониторинг загрузки ksoftirqd
# Если ksoftirqd потребляет >10% CPU — пакетов слишком много
ps aux | grep ksoftirqd
top -p $(pgrep -d',' ksoftirqd)
```

### Задание 3: Статистика драйвера NIC

Найдите потери пакетов на уровне NIC. Эти потери невидимы для `ifconfig`/`ip -s link` — только через `ethtool -S`.

```bash
# Поиск потерь и ошибок в статистике драйвера
ethtool -S eth0 | grep -E 'drop|miss|err|discard|no_buf'

# Ключевые счётчики (названия зависят от драйвера):
#
# rx_missed_errors    — NIC отбросил пакет (ринг полон)
# rx_no_buffer_count  — вариант rx_missed для Intel
# rx_dropped          — пакет отброшен (может быть NIC или ядро)
# rx_errors           — CRC, runt, oversize ошибки
# tx_dropped          — TX-очередь заполнена
#
# Если rx_missed_errors растёт:
#   1. ethtool -G eth0 rx 4096   (увеличить ринг)
#   2. ethtool -C eth0 adaptive-rx on  (coalescing)
#   3. Проверить RSS/RPS (часть 1.3)

# Сравните с «видимой» статистикой ядра
ip -s link show eth0
# Поле "RX dropped" в ip/ifconfig — это НЕ аппаратные потери NIC.
# Это потери на уровне протокольного стека или netfilter.
```

### Задание 4: Проверка и настройка Offloading

```bash
# Полный список offloading-возможностей интерфейса
ethtool -k eth0

# Проверить, что критически важные offload'ы включены:
ethtool -k eth0 | grep -E 'tcp-segmentation|generic-segmentation|generic-receive|checksumming'

# Если вы маршрутизатор или бридж — убедитесь, что LRO выключен:
ethtool -k eth0 | grep large-receive-offload
# Должно быть: large-receive-offload: off

# Если LRO включен на маршрутизаторе — выключите:
ethtool -K eth0 lro off
```

### Задание 5 (продвинутое): Написание XDP-программы для подсчёта пакетов

Напишите простую XDP-программу, которая подсчитывает общее количество пакетов и байтов, проходящих через интерфейс:

```c
/* Файл: xdp_counter.c
 * Компиляция: clang -O2 -target bpf -c xdp_counter.c -o xdp_counter.o
 * Загрузка:   ip link set dev eth0 xdp obj xdp_counter.o sec xdp
 * Просмотр:   bpftool map dump name stats
 * Удаление:   ip link set dev eth0 xdp off
 */

#include <linux/bpf.h>
#include <bpf/bpf_helpers.h>

/* Индексы в массиве статистики */
#define STAT_PACKETS 0
#define STAT_BYTES   1

/* Per-CPU массив для счётчиков.
 * Per-CPU — чтобы избежать атомарных операций
 * при параллельном выполнении на разных CPU.
 * Значения суммируются при чтении из userspace. */
struct {
    __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
    __uint(max_entries, 2);         /* [0]=пакеты, [1]=байты */
    __type(key, __u32);
    __type(value, __u64);
} stats SEC(".maps");

SEC("xdp")
int xdp_count(struct xdp_md *ctx)
{
    __u32 key;
    __u64 *value;

    /* Подсчитываем размер пакета */
    __u64 pkt_len = (__u64)(ctx->data_end - ctx->data);

    /* Инкрементируем счётчик пакетов */
    key = STAT_PACKETS;
    value = bpf_map_lookup_elem(&stats, &key);
    if (value)
        *value += 1;  /* Per-CPU — атомарность не нужна */

    /* Инкрементируем счётчик байтов */
    key = STAT_BYTES;
    value = bpf_map_lookup_elem(&stats, &key);
    if (value)
        *value += pkt_len;

    /* Пропускаем пакет дальше в стек */
    return XDP_PASS;
}

char _license[] SEC("license") = "GPL";
```

```bash
# После загрузки — просмотр статистики:
bpftool map dump name stats

# Для удобного чтения можно написать userspace-утилиту
# на Python с использованием библиотеки bcc или libbpf,
# которая будет периодически читать карту и выводить
# rate (пакетов/с, байтов/с).
```

---

## Итоги модуля

В этом модуле мы проследили путь пакета от физического уровня до создания `sk_buff`:

1. **NIC** принимает фрейм, проверяет CRC, записывает в RAM через **DMA** в заранее выделенный **кольцевой буфер**.
2. **Hard IRQ** обработчик отключает прерывания и планирует **NAPI poll**.
3. **NAPI poll** (в контексте SoftIRQ) забирает пакеты из ринга пачками (до `budget`), создаёт **`sk_buff`** для каждого, передаёт вверх по стеку.
4. **RSS/RPS/RFS** распределяют нагрузку по CPU.
5. **Offloading** (TSO/GSO/GRO) снижает нагрузку на CPU, агрегируя пакеты.
6. Когда стек ядра — узкое место, **XDP/eBPF** обрабатывают пакеты до `sk_buff`, а **DPDK** обходит ядро полностью.

Каждый из этих уровней — потенциальное место потери пакетов. Умение диагностировать, на каком именно уровне происходят потери, отличает Senior-инженера от того, кто просто запускает `ping` и разводит руками.

В следующем модуле мы поднимемся выше: от `netif_receive_skb()` через протокольный стек IP/TCP до Socket API — и разберём, как ядро управляет TCP-соединениями.
