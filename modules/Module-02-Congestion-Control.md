# Модуль 2: Bufferbloat и Congestion Control (Война за очередь)

> *«Интернет работает не потому, что он хорошо спроектирован, а потому, что congestion control не даёт ему коллапсировать.»*
> — свободная интерпретация Van Jacobson, 1988

Этот модуль — центральная часть книги. Мы разберём, почему буферы в сетевых устройствах одновременно спасают и убивают производительность TCP; как ядро Linux принимает решение о скорости передачи данных; и почему переход с CUBIC на BBR может увеличить throughput на трансконтинентальных линках в 10–20 раз — или, наоборот, сломать всё.

Мы пройдём полный путь: от закона Литтла через математику CUBIC и BBR до реального кода AQM-дисциплин в ядре. Завершим модуль практическим case study — бэкап 500 ГБ из Амстердама в Нью-Йорк — и набором заданий, которые можно воспроизвести в лабораторной среде из Module 0.

---

## Часть 2.1: Анатомия Bufferbloat

### Зачем вообще существуют буферы

Любое сетевое устройство — маршрутизатор, коммутатор, NIC, даже программный TC-qdisk — содержит очередь (буфер) на каждом исходящем интерфейсе. Буфер нужен для **абсорбции микровсплесков** (microbursts): если два пакета приходят одновременно, один из них должен подождать, пока первый передаётся в линк. Без буфера второй пакет был бы немедленно отброшен.

Представьте кассу в супермаркете. Без очереди каждый клиент, пришедший в момент, когда кассир занят, разворачивается и уходит. С очередью на 3–5 человек — система работает гладко. Но если очередь на 500 человек — время ожидания становится невыносимым, хотя ни один клиент не «потерян».

Именно это и есть **bufferbloat** — патологическое состояние, при котором избыточные буферы в сети создают огромную задержку (latency) без какого-либо выигрыша в пропускной способности (throughput).

### Исторический контекст: как мы здесь оказались

В начале 2000-х память стала дешёвой. Производители сетевого оборудования начали устанавливать буферы на десятки и сотни мегабайт «на всякий случай». Логика была простой: «больше буфер — меньше потерь — лучше». Эта логика оказалась катастрофически неверной.

Проблему впервые детально описал Jim Gettys (один из создателей X Window System) в 2010–2011 годах, дав ей название **bufferbloat**. Он обнаружил, что его домашний DSL-модем имел буфер на несколько секунд трафика, из-за чего ping до ближайшего маршрутизатора составлял 3+ секунды при активной загрузке.

### Закон Литтла (Little's Law)

Закон Литтла — фундаментальная теорема теории очередей, связывающая три величины:

```
L = λ × W

где:
  L — среднее число элементов в системе (длина очереди)
  λ — средняя скорость поступления (arrival rate)
  W — среднее время пребывания в системе (sojourn time)
```

Для сетевого буфера это переформулируется так:

```
                Queue_Size
  Latency  =  ────────────
               Service_Rate

  или, в более привычных единицах:

                Buffer_Size [байт]
  Delay [с] = ─────────────────────
               Link_Rate [байт/с]
```

Это **фундаментально важная** формула. Она говорит: задержка, вносимая буфером, определяется **только** размером буфера и скоростью линка. Она не зависит от протокола, от количества потоков, от погоды — только буфер и линк.

### Математический пример: 10 МБ буфер на 10 Mbps линке

Рассмотрим типичный домашний маршрутизатор с DSL-аплинком:

```
Дано:
  Buffer_Size = 10 MB = 10 × 1024 × 1024 байт = 10 485 760 байт
  Link_Rate   = 10 Mbps = 10 000 000 бит/с = 1 250 000 байт/с

Решение:
                10 485 760 байт
  Delay [с] = ──────────────────  =  8.39 секунды
                1 250 000 байт/с

Результат: ≈ 8 секунд дополнительной задержки!
```

Это означает, что когда буфер полон (а TCP с CUBIC быстро его заполнит — см. Часть 2.2), **каждый пакет** проведёт в очереди ~8 секунд. Для интерактивных приложений (VoIP, SSH, онлайн-игры) это катастрофа.

Для сравнения — задержки, приемлемые для различных приложений:

```
┌──────────────────────────────┬───────────────────────┐
│ Приложение                   │ Допустимая задержка   │
├──────────────────────────────┼───────────────────────┤
│ VoIP (голос)                 │ < 150 мс              │
│ Видеоконференция             │ < 200 мс              │
│ Онлайн-игры (FPS)           │ < 30 мс               │
│ Веб-браузинг (перцептивно)  │ < 100 мс              │
│ SSH (интерактивный)          │ < 200 мс              │
│ Бэкап (bulk transfer)       │ не критично            │
└──────────────────────────────┴───────────────────────┘

8 секунд очевидно превышают ВСЕ пороги.
```

### BDP — Bandwidth-Delay Product

Ещё одна ключевая величина, которая будет всплывать на протяжении всего модуля:

```
BDP = Bandwidth × RTT

Пример:
  Bandwidth = 10 Gbps = 1 250 000 000 байт/с
  RTT       = 80 мс   = 0.08 с

  BDP = 1 250 000 000 × 0.08 = 100 000 000 байт = 100 МБ
```

BDP — это объём данных «в полёте» (in-flight), который нужен для полной утилизации линка. Если отправитель держит в сети меньше BDP байт — линк недоиспользуется. Если больше — избыток оседает в буферах, создавая задержку.

**Оптимальная точка работы** congestion control — держать inflight ≈ BDP: линк загружен на 100%, буферы пусты. Именно к этой точке стремится BBR (Часть 2.3).

### Визуализация проблемы

```
    Throughput
    ▲
    │           ┌─────────────────────────  ← пропускная способность линка
    │          ╱│
    │         ╱ │
    │        ╱  │
    │       ╱   │
    │      ╱    │
    │     ╱     │
    │    ╱      │
    │   ╱       │
    │  ╱        │
    │ ╱         │
    │╱          │
    ├───────────┼───────────────────────────► inflight данных
    │           │
    │    BDP    │    буфер заполняется →
    │  (оптимум)│    задержка растёт,
    │           │    throughput НЕ растёт!
    │           │

    Latency
    ▲
    │                                    ╱
    │                                   ╱
    │                                  ╱  ← задержка растёт
    │                                 ╱     линейно с буфером
    │                                ╱
    │                               ╱
    │                              ╱
    │         ┌───────────────────╱
    │         │  RTprop (минимальный RTT)
    ├─────────┼──────────────────────────► inflight данных
    │         │
    │   BDP   │
```

Вывод: увеличение inflight сверх BDP **не увеличивает** throughput, но **линейно увеличивает** задержку. Это центральная проблема, которую решают все алгоритмы congestion control.

---

## Часть 2.2: CUBIC — рабочая лошадка Интернета

### Историческая справка

CUBIC — алгоритм congestion control, который является **дефолтным в Linux с ядра 2.6.19** (2006 год) и используется на подавляющем большинстве серверов в Интернете. Он был разработан в Университете Северной Каролины (Injong Rhee, Lisong Xu) как улучшение BIC-TCP.

До CUBIC стандартом был Reno/NewReno с линейным ростом окна (AIMD — Additive Increase, Multiplicative Decrease). CUBIC заменил линейный рост **кубической функцией**, что значительно лучше подходит для высокоскоростных линков с большим RTT.

### Кубическая функция роста окна

Центральная формула CUBIC:

```
W(t) = C × (t - K)³ + W_max

где:
  W(t)   — размер окна congestion window (cwnd) в момент времени t
  C      — константа масштабирования (в Linux: C = 0.4)
  t      — время, прошедшее с последнего события потери
  K      — точка перегиба, вычисляемая как:

           K = ³√(W_max × β / C)

  W_max  — размер окна в момент последней потери
  β      — коэффициент мультипликативного уменьшения (в Linux: β = 0.7,
           т.е. при потере окно уменьшается до 0.7 × W_max,
           а не до 0.5 как в Reno)
```

### Графическое поведение

```
    cwnd
    ▲
    │
    │                                          ╱
    │                                        ╱
    │                                      ╱     ← агрессивный рост
    │                                    ╱        (convex region)
    │                                  ╱
W_max├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ╱─
    │                            ╱╱    ← плато вокруг W_max
    │                          ╱╱       (concave → convex)
    │                        ╱╱
    │                     ╱╱╱
    │                ╱╱╱╱╱
    │           ╱╱╱╱╱
    │      ╱╱╱╱╱
    │ ╱╱╱╱╱
0.7×W_max├╱ ← окно после потери (β = 0.7)
    │
    ├─────────┬───────────────────────────► время
    │    loss event
    │         ├── K ──┤
    │         (время до достижения W_max)
```

Ключевые свойства кубической функции:

1. **Быстрый начальный рост** — после потери окно быстро возвращается к W_max (вогнутая часть, concave region).
2. **Осторожность вокруг W_max** — рост замедляется, т.к. W_max — последнее известное безопасное значение.
3. **Выпуклый рост выше W_max** — если потери нет, CUBIC начинает агрессивно зондировать новую ёмкость (convex region).
4. **Независимость от RTT** — функция W(t) зависит от **реального времени**, а не от количества RTT. Это критическое отличие от Reno, где рост привязан к RTT и длинные линки страдают от RTT-fairness.

### Упрощённый код ядра: `bictcp_cong_avoid()`

Рассмотрим упрощённую версию основной функции CUBIC в ядре Linux. Реальный код находится в `net/ipv4/tcp_cubic.c`.

```c
/*
 * Упрощённая версия bictcp_cong_avoid() из net/ipv4/tcp_cubic.c
 *
 * Эта функция вызывается при получении каждого ACK в состоянии
 * congestion avoidance (т.е. когда cwnd >= ssthresh).
 *
 * Исходный код: Linux kernel 6.x, Injong Rhee, Lisong Xu
 */

/* Константы CUBIC */
#define BICTCP_BETA_SCALE   1024        /* Масштаб для β */
#define BICTCP_B            717         /* β = 717/1024 ≈ 0.7 */
#define BICTCP_C            410         /* C = 410/1024 ≈ 0.4 */
#define BICTCP_HZ           10          /* Гранулярность таймера: 10 Гц */

/*
 * Структура состояния CUBIC для каждого TCP-соединения.
 * Хранится в поле icsk_ca_priv структуры inet_connection_sock.
 */
struct bictcp {
    u32 cnt;            /* Коэффициент роста: увеличивать cwnd на 1 каждые cnt ACK */
    u32 last_max_cwnd;  /* W_max: cwnd при последней потере */
    u32 last_cwnd;      /* Последнее значение cwnd (для кэширования) */
    u32 last_time;      /* Время последнего вычисления */
    u32 epoch_start;    /* Начало текущей эпохи (момент последней потери) */
    u32 origin_point;   /* W_max или cwnd при старте эпохи */
    u32 K;              /* Время до достижения origin_point (в единицах HZ) */
    u32 ack_cnt;        /* Счётчик ACK с момента последнего увеличения cwnd */
    u32 tcp_cwnd;       /* Значение cwnd по формуле Reno (для совместимости) */
};

/*
 * bictcp_update() — вычисление нового целевого cwnd по кубической функции.
 *
 * Вызывается из bictcp_cong_avoid() при каждом ACK.
 */
static void bictcp_update(struct bictcp *ca, u32 cwnd, u32 acked)
{
    u32 delta, bic_target, max_cnt;
    u64 offs, t;

    /* Если cwnd не изменился с прошлого раза — используем кэш */
    ca->ack_cnt += acked;

    if (ca->last_cwnd == cwnd &&
        (s32)(tcp_jiffies32 - ca->last_time) <= HZ / 32)
        return;

    ca->last_cwnd = cwnd;
    ca->last_time = tcp_jiffies32;

    /* Начало новой эпохи (после потери) */
    if (ca->epoch_start == 0) {
        ca->epoch_start = tcp_jiffies32;
        ca->ack_cnt = acked;
        ca->tcp_cwnd = cwnd;

        /*
         * Определяем origin_point и K.
         * Если текущий cwnd < W_max, то мы возвращаемся к W_max:
         *   origin_point = W_max
         *   K = cubic_root(W_max - cwnd) / C)
         * Если cwnd >= W_max (быстрая сходимость — fast convergence):
         *   origin_point = cwnd
         *   K = 0
         */
        if (cwnd < ca->last_max_cwnd) {
            ca->K = cubic_root(cube_factor *
                               (ca->last_max_cwnd - cwnd));
            ca->origin_point = ca->last_max_cwnd;
        } else {
            ca->K = 0;
            ca->origin_point = cwnd;
        }
    }

    /*
     * Вычисляем t — время с начала эпохи.
     * Формула CUBIC: W(t) = C * (t - K)³ + origin_point
     */
    t = (s32)(tcp_jiffies32 - ca->epoch_start);
    t += usecs_to_jiffies(ca->delay_min);  /* Компенсация RTT */
    t <<= BICTCP_HZ;                       /* Масштабирование */
    t /= HZ;

    /* offs = |t - K| — расстояние от точки перегиба */
    if (t < ca->K)
        offs = ca->K - t;
    else
        offs = t - ca->K;

    /* delta = C × offs³ (с учётом масштабирования) */
    delta = (cube_rtt_scale * offs * offs * offs) >> (10 + 3 * BICTCP_HZ);

    /* bic_target = W(t): целевое значение cwnd */
    if (t < ca->K)
        bic_target = ca->origin_point - delta;
    else
        bic_target = ca->origin_point + delta;

    /*
     * cnt — определяет скорость роста cwnd.
     * cwnd увеличивается на 1 сегмент каждые cnt ACK.
     * Чем больше разница (bic_target - cwnd), тем меньше cnt,
     * тем быстрее растёт cwnd.
     */
    if (bic_target > cwnd)
        ca->cnt = cwnd / (bic_target - cwnd);
    else
        ca->cnt = 100 * cwnd;  /* Очень медленный рост */
}

/*
 * bictcp_cong_avoid() — главная точка входа CUBIC при congestion avoidance.
 *
 * Вызывается TCP-стеком при получении ACK, если мы НЕ в состоянии
 * slow start и НЕ в состоянии loss/recovery.
 */
static void bictcp_cong_avoid(struct sock *sk, u32 ack, u32 acked)
{
    struct tcp_sock *tp = tcp_sk(sk);
    struct bictcp *ca = inet_csk_ca(sk);

    /* Стандартная проверка: не превышаем ли мы receiver window */
    if (!tcp_is_cwnd_limited(sk))
        return;

    /* Slow Start: используем стандартный TCP slow start */
    if (tcp_in_slow_start(tp)) {
        tcp_slow_start(tp, acked);
        return;
    }

    /* Congestion Avoidance: вычисляем новый cwnd по CUBIC */
    bictcp_update(ca, tcp_snd_cwnd(tp), acked);

    /*
     * Увеличиваем cwnd на 1 сегмент каждые ca->cnt ACK.
     * Это эквивалент «дробного» увеличения:
     * cwnd += 1/cnt при каждом ACK.
     */
    tcp_cong_avoid_ai(tp, ca->cnt, acked);
}

/*
 * bictcp_recalc_ssthresh() — реакция на потерю пакета.
 *
 * При обнаружении потери (тройной дупликат ACK или RTO):
 *   1. Запоминаем текущий cwnd как W_max
 *   2. Устанавливаем ssthresh = β × cwnd (β ≈ 0.7)
 *   3. Сбрасываем эпоху
 */
static u32 bictcp_recalc_ssthresh(struct sock *sk)
{
    const struct tcp_sock *tp = tcp_sk(sk);
    struct bictcp *ca = inet_csk_ca(sk);

    ca->epoch_start = 0;  /* Сброс эпохи — начнём заново */

    /* Fast convergence: если W_max снижается, уменьшаем его быстрее */
    if (tcp_snd_cwnd(tp) < ca->last_max_cwnd)
        ca->last_max_cwnd = (tcp_snd_cwnd(tp) * (BICTCP_BETA_SCALE + BICTCP_B))
                            / (2 * BICTCP_BETA_SCALE);
    else
        ca->last_max_cwnd = tcp_snd_cwnd(tp);

    /* ssthresh = β × cwnd, где β ≈ 0.7 */
    return max((tcp_snd_cwnd(tp) * BICTCP_B) / BICTCP_BETA_SCALE, 2U);
}
```

### Главная слабость CUBIC: слепота к задержке

CUBIC **полностью слеп к задержке**. Он узнаёт о перегрузке **только** из факта потери пакетов. Это означает:

1. **CUBIC заполняет все буферы** — он будет увеличивать cwnd до тех пор, пока не произойдёт потеря. Если буфер на 10 МБ — CUBIC заполнит 10 МБ, добавив 8 секунд задержки на 10 Mbps линке.
2. **Пилообразное поведение** — cwnd растёт до потери, падает на 30%, растёт снова. Очередь постоянно колеблется от «почти пусто» до «переполнено».
3. **На длинных линках — катастрофически медленная сходимость.** На линке 10 Gbps × 80 мс RTT, BDP = 100 МБ. После потери cwnd падает до 0.7 × W_max. Возврат к W_max может занять **минуты**, в течение которых линк недоиспользуется.

### HyStart — ускорение выхода из Slow Start

Стандартный TCP Slow Start удваивает cwnd каждый RTT, пока не произойдёт потеря. На линке с большим BDP это может привести к массивному overshoot: cwnd перескакивает BDP и вливает огромный всплеск пакетов в сеть.

**HyStart** (Hybrid Slow Start) — дополнение к CUBIC, которое пытается обнаружить приближение к ёмкости линка **до** потери, используя два эвристических детектора:

```
1. ACK Train Detector:
   Если разница между временами прихода первого и последнего ACK
   в рамках одного RTT слишком велика → очередь начала расти → выход.

2. Delay Increase Detector:
   Если минимальный RTT в текущем окне значительно больше базового
   минимального RTT → очередь начала расти → выход.

Порог по умолчанию: увеличение RTT > 2 мс
(sysctl net.ipv4.tcp_hystart_low_window = 16)
(sysctl net.ipv4.tcp_hystart_ack_delta_us)
```

HyStart неидеален — он может срабатывать слишком рано (особенно при jitter) или слишком поздно, но существенно снижает вероятность массивного overshoot при slow start.

---

## Часть 2.3: BBR — Bottleneck Bandwidth and Round-trip propagation time

### Философский сдвиг

BBR, разработанный в Google (Neal Cardwell, Yuchung Cheng, Van Jacobson), представляет **фундаментальный сдвиг** в подходе к congestion control:

```
CUBIC:  «Сеть перегружена, когда пакеты теряются»
BBR:    «Сеть перегружена, когда задержка растёт»
```

BBR стремится работать в точке Kleinrock — оптимуме, где throughput максимален, а задержка минимальна. Для этого BBR непрерывно оценивает два параметра:

```
BtlBw   — Bottleneck Bandwidth: максимальная пропускная способность
           узкого места (bottleneck) на пути.
           Оценивается как max(delivery_rate) за последние ~10 RTT.

RTprop  — Round-trip propagation time: минимальная задержка на пути
           (когда буферы пусты).
           Оценивается как min(RTT) за последние 10 секунд.

Целевой inflight = BDP = BtlBw × RTprop
```

### Оценка BtlBw: `bbr_update_bw()`

BBR вычисляет delivery rate для каждого ACK — скорость, с которой данные доставляются получателю. Максимум delivery rate за скользящее окно ≈ 10 RTT принимается за оценку BtlBw.

```c
/*
 * Упрощённая версия bbr_update_bw() из net/ipv4/tcp_bbr.c
 *
 * Вызывается при получении каждого ACK.
 * Обновляет оценку bottleneck bandwidth.
 */
static void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)
{
    struct tcp_sock *tp = tcp_sk(sk);
    struct bbr *bbr = inet_csk_ca(sk);
    u64 bw;

    /*
     * Пропускаем некорректные измерения:
     * - rs->delivered <= 0: ACK не подтвердил новые данные
     * - rs->interval_us <= 0: некорректный интервал
     * - rs->is_app_limited: приложение не передавало данные
     *   с максимальной скоростью, поэтому delivery_rate
     *   занижен и не отражает реальную ёмкость линка.
     */
    if (rs->delivered <= 0 || rs->interval_us <= 0)
        return;

    /*
     * Вычисляем delivery_rate = delivered_bytes / interval.
     *
     * delivery_rate — это скорость, с которой ACK-и подтверждают
     * доставку данных. В отсутствие потерь и при полной загрузке
     * линка это значение стремится к BtlBw.
     */
    bw = div64_long((u64)rs->delivered * BW_UNIT, rs->interval_us);

    /*
     * Обновляем скользящий максимум BtlBw.
     *
     * minmax_running_max() поддерживает максимум за скользящее окно
     * размером bbr_bw_rtts (по умолчанию 10 RTT).
     *
     * Почему максимум, а не среднее? Потому что delivery_rate
     * может быть занижен (из-за задержки ACK, агрегации и т.д.),
     * но не может быть завышен выше реальной ёмкости линка.
     * Поэтому max(delivery_rate) — наилучшая оценка BtlBw.
     *
     * minmax_running_max() использует структуру minmax из
     * lib/win_minmax.c — эффективный O(1) алгоритм для
     * скользящего min/max с окном фиксированного размера.
     */
    if (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {
        minmax_running_max(&bbr->bw, bbr_bw_rtts,
                           bbr->rtt_cnt, bw);
    }
}

/*
 * bbr_max_bw() — возвращает текущую оценку BtlBw.
 */
static u32 bbr_max_bw(const struct sock *sk)
{
    const struct bbr *bbr = inet_csk_ca(sk);

    /* minmax_get() возвращает текущий максимум из скользящего окна */
    return minmax_get(&bbr->bw);
}
```

**Ключевой момент — фильтр `is_app_limited`:**

Если приложение не посылает данные так быстро, как позволяет cwnd (например, сервер читает файл с диска медленнее, чем может отправить), delivery_rate занижен. BBR **игнорирует** такие измерения при обновлении BtlBw (но допускает их, если они выше текущей оценки — это безопасно).

### Оценка RTprop: `bbr_update_min_rtt()`

BBR поддерживает оценку минимального RTT — задержки при пустых буферах:

```c
/*
 * Упрощённая версия bbr_update_min_rtt() из net/ipv4/tcp_bbr.c
 *
 * Обновляет оценку RTprop — минимального RTT на пути.
 * RTprop сбрасывается каждые 10 секунд, чтобы отслеживать
 * изменения маршрута.
 */
static void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)
{
    struct tcp_sock *tp = tcp_sk(sk);
    struct bbr *bbr = inet_csk_ca(sk);
    bool filter_expired;

    /*
     * rs->rtt_us — RTT, измеренный для данного ACK.
     * Используем только корректные измерения (> 0).
     */
    if (rs->rtt_us <= 0)
        return;

    /*
     * Проверяем, не истёк ли 10-секундный фильтр.
     *
     * Почему 10 секунд? Это компромисс:
     * - Слишком короткое окно → BBR будет видеть inflated RTT
     *   (когда сам же заполнил буферы) и занижать RTprop.
     * - Слишком длинное окно → BBR не заметит изменение маршрута
     *   (например, при переключении на более длинный путь).
     *
     * 10 секунд — эмпирически подобранное значение, достаточное
     * для прохождения всех фаз BBR state machine.
     */
    filter_expired = after(tcp_jiffies32,
                           bbr->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);

    /*
     * Обновляем min_rtt, если:
     * 1. Новое измерение меньше текущего min_rtt, ИЛИ
     * 2. Фильтр истёк (прошло 10 секунд без обновления).
     *
     * В случае 2 мы принимаем текущее значение RTT как новый min_rtt,
     * даже если оно больше предыдущего. Это необходимо для
     * корректного отслеживания увеличения RTprop (смена маршрута).
     */
    if (rs->rtt_us < bbr->min_rtt_us || filter_expired) {
        bbr->min_rtt_us = rs->rtt_us;
        bbr->min_rtt_stamp = tcp_jiffies32;
    }

    /*
     * Если фильтр истёк, инициируем переход в PROBE_RTT.
     *
     * В состоянии PROBE_RTT BBR уменьшает inflight до 4 пакетов
     * на 200 мс, чтобы «осушить» буферы и получить чистое
     * измерение RTprop. Это единственный способ убедиться,
     * что мы видим реальный RTprop, а не inflated RTT.
     */
    if (filter_expired && !bbr->idle_restart && bbr->mode != BBR_PROBE_RTT) {
        bbr->mode = BBR_PROBE_RTT;
        bbr->probe_rtt_done_stamp = 0;
        /* Сохраняем cwnd для восстановления после PROBE_RTT */
        bbr->prior_cwnd = tcp_snd_cwnd(tp);
    }
}
```

### Pacing — контроль скорости отправки

В отличие от CUBIC, который полагается только на cwnd (и отправляет burst пакетов сразу после получения ACK), BBR использует **pacing** — равномерное распределение пакетов по времени.

```
Pacing rate = pacing_gain × BtlBw

                   1 сегмент (MSS)
pacing_interval = ─────────────────
                    pacing_rate

Пример:
  BtlBw = 1 Gbps = 125 000 000 байт/с
  MSS   = 1460 байт (стандартный для Ethernet)
  pacing_gain = 1.0 (в PROBE_BW steady state)

  pacing_rate = 1.0 × 125 000 000 = 125 000 000 байт/с

                      1460
  pacing_interval = ────────────── = 0.00001168 с = 11.68 мкс
                    125 000 000

  → один пакет каждые ~11.7 микросекунд
```

**Почему pacing критически важен:**

Без pacing TCP отправляет burst пакетов при получении ACK (так называемый ACK clocking). Burst из 10–100 пакетов может мгновенно заполнить буфер промежуточного маршрутизатора, вызвав потери даже при умеренной общей нагрузке.

Pacing «размазывает» пакеты по времени, значительно снижая вероятность переполнения буферов.

**Требование к qdisc: почему необходим fq (Fair Queue):**

BBR вычисляет pacing_rate и передаёт его ядру через `sk->sk_pacing_rate`. Но **реализация** pacing — задача не congestion control, а **qdisc** (queueing discipline) на исходящем интерфейсе.

```bash
# По умолчанию в Linux используется pfifo_fast — FIFO без pacing.
# BBR требует fq или fq_codel для реализации pacing!

# Установка fq qdisc (рекомендуется для серверов с BBR):
tc qdisc replace dev eth0 root fq

# Проверка текущего qdisc:
tc qdisc show dev eth0

# fq qdisc считывает sk_pacing_rate из сокета и задерживает
# отправку пакетов до нужного момента, реализуя pacing.
```

Без fq BBR всё равно будет работать (cwnd-based ограничение остаётся), но **без pacing**: пакеты будут отправляться burst-ами, что сводит на нет одно из главных преимуществ BBR.

### Машина состояний BBR

BBR работает как конечный автомат с четырьмя состояниями:

```
    ┌──────────────────────────────────────────────────────────┐
    │                   BBR State Machine                      │
    │                                                          │
    │  ┌──────────┐                                            │
    │  │ STARTUP  │  pacing_gain = 2.89 (= 2/ln2)             │
    │  │          │  cwnd_gain   = 2.89                        │
    │  │ Цель:    │  Быстро найти BtlBw.                      │
    │  │ найти    │  Удваиваем скорость каждый RTT             │
    │  │ BtlBw    │  (аналог slow start, но с pacing).         │
    │  └────┬─────┘                                            │
    │       │                                                  │
    │       │ BtlBw перестал расти 3 RTT подряд                │
    │       ▼                                                  │
    │  ┌──────────┐                                            │
    │  │  DRAIN   │  pacing_gain = 1/2.89 ≈ 0.35              │
    │  │          │  cwnd_gain   = 2.89                        │
    │  │ Цель:    │  Осушить очередь, которую создал STARTUP.  │
    │  │ осушить  │  Скорость падает, inflight уменьшается     │
    │  │ очередь  │  до BDP.                                   │
    │  └────┬─────┘                                            │
    │       │                                                  │
    │       │ inflight <= BDP (estimated)                      │
    │       ▼                                                  │
    │  ┌──────────────────────────────────────────────┐        │
    │  │              PROBE_BW                         │        │
    │  │                                               │        │
    │  │  Основное устойчивое состояние.               │        │
    │  │  Циклически меняет pacing_gain:               │        │
    │  │                                               │        │
    │  │  Фаза:  [5/4, 3/4, 1, 1, 1, 1, 1, 1]        │        │
    │  │          ^^^  ^^^  ^^^^^^^^^^^^^^^^^          │        │
    │  │          UP   DOWN    CRUISE                  │        │
    │  │                                               │        │
    │  │  UP (1.25): зондируем — есть ли доп. ёмкость? │        │
    │  │  DOWN (0.75): дренируем созданную очередь.    │        │
    │  │  CRUISE (1.0): передаём со скоростью BtlBw.  │        │
    │  │                                               │        │
    │  │  cwnd_gain = 2.0 (позволяем burst до 2×BDP)  │        │
    │  └──────────┬───────────────────────────────────┘        │
    │             │                                            │
    │             │ min_rtt_stamp + 10 сек (фильтр истёк)      │
    │             ▼                                            │
    │  ┌──────────┐                                            │
    │  │PROBE_RTT │  pacing_gain = 1.0                         │
    │  │          │  cwnd = 4 пакета (минимум!)                │
    │  │ Цель:    │                                            │
    │  │ измерить │  Длительность: 200 мс                      │
    │  │ RTprop   │  Осушаем ВСЕ буферы, чтобы увидеть        │
    │  │          │  реальный RTprop.                           │
    │  └────┬─────┘                                            │
    │       │                                                  │
    │       │ 200 мс прошло                                    │
    │       ▼                                                  │
    │  Возврат в PROBE_BW (или STARTUP, если BtlBw не найден) │
    │                                                          │
    └──────────────────────────────────────────────────────────┘
```

### BBR v1 vs BBR v2: проблема fairness

BBR v1 (представлен в Linux 4.9, 2016) имел серьёзные проблемы при конкуренции с потоками на базе loss-based алгоритмов (CUBIC):

```
Проблема BBR v1:
─────────────────
1. BBR v1 отправляет со скоростью BtlBw × pacing_gain, игнорируя потери.
2. CUBIC реагирует на потери, уменьшая cwnd.
3. Результат: BBR v1 «выдавливает» CUBIC из канала.

В тесте с одним BBR v1 потоком и одним CUBIC потоком на общем
10 Mbps линке:
  - BBR v1: ~8 Mbps (80%)
  - CUBIC:  ~2 Mbps (20%)

Это несправедливо и, что хуже, может вызвать retransmission storm
у CUBIC-потоков.

Дополнительная проблема: BBR v1 при RTprop PROBE создавал
периодические просадки для всех потоков.
```

**BBR v2** (TCP BBRv2, в разработке с 2019, staging в ядре с 5.x) решает эти проблемы:

```
Основные изменения в BBR v2:
─────────────────────────────
1. Учёт потерь: BBR v2 реагирует на потери, снижая inflight_hi.
   Это делает его более дружелюбным к CUBIC/Reno.

2. Учёт ECN: BBR v2 использует ECN-маркировку как сигнал перегрузки,
   не дожидаясь потерь.

3. PROBE_BW переработан:
   - Вместо фиксированного цикла [5/4, 3/4, 1×6] используется
     более адаптивная схема с фазами UP, DOWN, CRUISE, REFILL.
   - UP: зондирует, но ограничен inflight_hi.
   - DOWN: дренирует.
   - CRUISE: не превышает estimated BDP.

4. inflight_hi / inflight_lo:
   - inflight_hi — верхняя граница inflight, при которой
     начинаются потери/ECN.
   - inflight_lo — нижняя граница, ниже которой потерь нет.
   - BBR v2 держит inflight между ними.

Результат fairness:
  BBR v2 + CUBIC на общем 10 Mbps линке:
  - BBR v2: ~5.2 Mbps (~52%)
  - CUBIC:  ~4.8 Mbps (~48%)
  Значительно более справедливо!
```

---

## Часть 2.4: DCTCP и ECN — Congestion Control для датацентров

### Проблема Incast

В типичном датацентре существует специфический паттерн трафика, называемый **incast**:

```
           ┌──────┐
           │Client│ (запрашивает данные одновременно)
           └──┬───┘
              │
         ┌────┴────┐
         │ ToR     │  ← буфер коммутатора:
         │ Switch  │     типично 100–500 КБ на порт
         └─┬─┬─┬─┬─┘    (shallow buffers!)
           │ │ │ │
     ┌─────┘ │ │ └─────┐
     │       │ │       │
  ┌──┴──┐┌──┴─┴┐┌──┴──┐┌──┴──┐
  │Srv 1││Srv 2││Srv 3││Srv N│  (все отвечают одновременно)
  └─────┘└─────┘└─────┘└─────┘

Incast: N серверов одновременно отправляют данные одному клиенту.
Пример: MapReduce — mapper'ы отправляют результаты reducer'у.

Результат: буфер ToR-коммутатора переполняется мгновенно.
N = 20 серверов × 64 КБ начального окна = 1280 КБ > 500 КБ буфера.
→ Массивные потери → TCP RTO (200+ мс) → деградация throughput.
```

### ECN — Explicit Congestion Notification

ECN решает проблему **до** переполнения буфера:

```
Без ECN (Drop Tail):
  Буфер полон → пакет отбрасывается → отправитель узнаёт через timeout/dup-ACK.

С ECN:
  Буфер приближается к заполнению → маршрутизатор МАРКИРУЕТ пакет
  (устанавливает CE-бит в IP-заголовке) → получатель отражает маркировку
  в ACK (ECE-флаг) → отправитель снижает скорость.

  Пакет НЕ отбрасывается! Данные доставлены, но отправитель
  предупреждён о перегрузке.

Биты ECN в IP-заголовке (поле DSCP/ECN, 2 бита):
  00 — Not-ECT: отправитель не поддерживает ECN
  01 — ECT(1):  отправитель поддерживает ECN
  10 — ECT(0):  отправитель поддерживает ECN
  11 — CE:      Congestion Experienced (установлен маршрутизатором)
```

### DCTCP — Data Center TCP

DCTCP (RFC 8257) использует ECN не как бинарный сигнал («есть перегрузка / нет перегрузки»), а как **пропорциональный**:

```
DCTCP вычисляет α — долю ECN-маркированных пакетов:

  α = (1 - g) × α + g × F

  где:
    g — вес EWMA (обычно 1/16)
    F — доля маркированных пакетов в последнем окне

  Реакция на перегрузку:

  cwnd = cwnd × (1 - α/2)

  Примеры:
    α = 0   → cwnd не изменяется (нет перегрузки)
    α = 0.1 → cwnd *= 0.95 (лёгкая перегрузка: уменьшаем на 5%)
    α = 0.5 → cwnd *= 0.75 (средняя перегрузка: уменьшаем на 25%)
    α = 1.0 → cwnd *= 0.5  (полная перегрузка: уменьшаем вдвое)

В отличие от CUBIC (всегда cwnd *= 0.7 при потере):
  DCTCP реагирует ПРОПОРЦИОНАЛЬНО степени перегрузки.
  Это позволяет держать очередь коммутатора очень короткой
  (< 20 КБ), обеспечивая низкую задержку И высокую утилизацию.
```

**Порог маркировки ECN (K):**

```
Для DCTCP рекомендуемый порог маркировки на коммутаторе:

  K = (BDP × (1 + 1/(2×g))) / N  ← для N потоков

  На практике: K ≈ 20–65 пакетов (30–97.5 КБ при MSS=1500)

  Включение ECN на коммутаторе (пример для Memory-Mapped коммутатора):
  # Помечать пакеты ECN, когда очередь > 65 пакетов
  # На Memory-Mapped (например Memory-Mapped Memory-Mapped):
  # Используется RED/WRED с ECN на конкретных портах
```

**Включение DCTCP в Linux:**

```bash
# На ОБОИХ концах соединения:
sysctl -w net.ipv4.tcp_congestion_control=dctcp
sysctl -w net.ipv4.tcp_ecn=1

# На коммутаторе: настроить ECN-маркировку с порогом K
# (зависит от вендора и модели)
```

**Важное ограничение**: DCTCP предназначен **только для датацентров** с контролируемой средой. В публичном Интернете DCTCP будет конкурировать с CUBIC/BBR потоками и проигрывать, т.к. они не реагируют на ECN так же агрессивно.

---

## Часть 2.5: AQM — Active Queue Management

### Проблема Drop Tail

Drop Tail — простейшая стратегия управления очередью: «если буфер полон — отбрось новый пакет». Несмотря на простоту, Drop Tail создаёт несколько серьёзных проблем.

**Проблема 1: Global Synchronization**

```
    cwnd потоков при Drop Tail
    ▲
    │   ╱╲    ╱╲    ╱╲    ╱╲      ← Поток 1
    │  ╱  ╲  ╱  ╲  ╱  ╲  ╱  ╲
    │ ╱    ╲╱    ╲╱    ╲╱    ╲
    │╱╲    ╱╲    ╱╲    ╱╲    ╱╲   ← Поток 2
    │  ╲  ╱  ╲  ╱  ╲  ╱  ╲  ╱
    │   ╲╱    ╲╱    ╲╱    ╲╱      ← Поток 3
    ├──────────────────────────────► время

    Все потоки растут одновременно, одновременно заполняют буфер,
    одновременно получают потери, одновременно снижают cwnd.

    Результат: пульсация throughput. Линк то загружен на 100%,
    то простаивает, пока все потоки одновременно восстанавливаются.
```

**Проблема 2: Lock-out (голодание потоков)**

```
Когда буфер полон, потоки с бóльшим burst имеют больше шансов
«захватить» место в очереди. Потоки с малым cwnd (новые соединения,
потоки после потери) могут систематически не попадать в буфер.

Это нарушает fairness и особенно опасно для коротких HTTP-запросов,
которые не успевают выйти из slow start.
```

### RED — Random Early Detection

RED (Floyd & Jacobson, 1993) — первая попытка решить проблемы Drop Tail:

```
Принцип RED:
  1. Вычисляем среднюю длину очереди (EWMA): avg
  2. Если avg < min_th: не отбрасываем
  3. Если min_th < avg < max_th: отбрасываем с вероятностью p,
     линейно растущей от 0 до max_p
  4. Если avg > max_th: отбрасываем все пакеты

  Вероятность отбрасывания:
    p
    ▲
max_p├─────────────────────┐
    │                   ╱  │
    │                 ╱    │
    │               ╱      │
    │             ╱        │
    │           ╱          │
    │         ╱            │
    │       ╱              │
    0├─────╱               │
    ├─────┼────────────────┼──► avg queue
         min_th          max_th
```

**Почему RED — «кошмар настройки»:**

RED имеет **четыре** взаимозависимых параметра: `min_th`, `max_th`, `max_p`, `wq` (вес EWMA). Оптимальные значения зависят от:
- количества потоков
- скорости линка
- RTT потоков
- размера буфера
- характера трафика

```bash
# Пример настройки RED в Linux:
tc qdisc add dev eth0 root red \
    limit 500000 \    # Максимальный размер очереди (байт)
    min 50000 \       # min_th (байт)
    max 150000 \      # max_th (байт)
    avpkt 1000 \      # Средний размер пакета (для расчёта EWMA)
    burst 55 \        # Количество пакетов для расчёта avg
    ecn \             # Использовать ECN вместо drop
    probability 0.02  # max_p

# Эти параметры оптимальны для КОНКРЕТНОГО сценария.
# Стоит изменить скорость линка или количество потоков —
# и нужно перенастраивать.
```

На практике администраторы либо не настраивают RED вовсе, либо используют неоптимальные параметры, которые работают хуже, чем Drop Tail. По этой причине RED не получил широкого распространения, и отрасль перешла к **CoDel**.

### CoDel — Controlled Delay

CoDel (Nichols & Jacobson, 2012) решает проблему настройки радикально: у него **нет** параметров, требующих ручной настройки (на практике два параметра, но с хорошими дефолтами).

**Ключевая идея CoDel**: важен не размер очереди, а **время пребывания пакета в очереди** (sojourn time). Если пакет провёл в очереди больше `target` (по умолчанию 5 мс), значит очередь «стоячая» (standing queue) и её нужно сокращать.

```c
/*
 * Упрощённая версия codel_dequeue() из net/sched/sch_codel.c
 *
 * CoDel работает на стороне DEQUEUE (извлечения из очереди),
 * а не ENQUEUE. Это принципиально: мы смотрим на РЕАЛЬНОЕ
 * время ожидания пакета, а не на оценку длины очереди.
 *
 * Параметры (с хорошими дефолтами):
 *   target   = 5 мс   — целевая максимальная задержка в очереди
 *   interval = 100 мс — интервал между сбросами пакетов
 */

/*
 * Структура состояния CoDel.
 */
struct codel_vars {
    u32 count;          /* Количество сброшенных пакетов в текущем цикле */
    u32 dropping;       /* Флаг: мы сейчас в режиме сброса? */
    u32 first_above;    /* Время, когда sojourn_time впервые превысил target */
    codel_time_t drop_next; /* Время следующего сброса */
    codel_time_t ldelay;    /* Задержка последнего dequeued пакета */
};

/*
 * control_law() — вычисляет момент следующего сброса.
 *
 * Интервал между сбросами уменьшается пропорционально 1/sqrt(count):
 *
 *   drop_next = now + interval / sqrt(count)
 *
 * Это обеспечивает нарастающее давление: чем дольше очередь
 * остаётся выше target, тем чаще сбрасываются пакеты.
 *
 * Формула 1/sqrt(count) — не случайность. Она обеспечивает
 * линейное уменьшение throughput каждого потока, что приводит
 * к справедливому распределению полосы.
 */
static codel_time_t control_law(codel_time_t t,
                                codel_time_t interval,
                                u32 count)
{
    /* interval / sqrt(count) с использованием Newton's method */
    return t + reciprocal_divide(interval, int_sqrt(count));
}

/*
 * codel_should_drop() — определяет, нужно ли сбросить пакет.
 *
 * Возвращает true, если пакет провёл в очереди больше target.
 */
static bool codel_should_drop(const struct sk_buff *skb,
                              struct codel_vars *vars,
                              const struct codel_params *p,
                              codel_time_t now)
{
    codel_time_t sojourn_time;

    /*
     * Вычисляем sojourn_time — время пребывания пакета в очереди.
     * codel_get_enqueue_time(skb) — момент, когда пакет был помещён
     * в очередь (сохраняется в skb при enqueue).
     */
    sojourn_time = now - codel_get_enqueue_time(skb);
    vars->ldelay = sojourn_time;

    /* Если задержка в пределах target — не сбрасываем */
    if (codel_time_before(sojourn_time, p->target))
        return false;

    /*
     * sojourn_time > target.
     *
     * Но мы НЕ сбрасываем сразу! Мы ждём interval (100 мс),
     * чтобы убедиться, что это не кратковременный burst,
     * а устойчивая перегрузка.
     */
    if (vars->first_above == 0) {
        /* Первое превышение — запоминаем время */
        vars->first_above = now;
        return false;
    }

    /* Если прошло больше interval с момента первого превышения */
    if (codel_time_after(now, vars->first_above + p->interval))
        return true;

    return false;
}

/*
 * codel_dequeue() — основная функция CoDel.
 *
 * Вызывается при каждом извлечении пакета из очереди.
 */
static struct sk_buff *codel_dequeue(struct Qdisc *sch,
                                     struct codel_vars *vars,
                                     struct codel_params *p)
{
    struct sk_buff *skb = __qdisc_dequeue_head(&sch->q);
    codel_time_t now;
    bool drop;

    if (!skb) {
        /* Очередь пуста — сбрасываем состояние */
        vars->dropping = false;
        vars->first_above = 0;
        return NULL;
    }

    now = codel_get_time();
    drop = codel_should_drop(skb, vars, p, now);

    if (vars->dropping) {
        /*
         * Мы в режиме сброса (dropping state).
         * Проверяем, не пора ли сбросить следующий пакет.
         */
        if (!drop) {
            /*
             * sojourn_time < target → очередь рассосалась.
             * Выходим из режима сброса.
             */
            vars->dropping = false;
        } else if (codel_time_after_eq(now, vars->drop_next)) {
            /*
             * Пора сбросить. Увеличиваем count, вычисляем
             * время следующего сброса по control_law().
             */
            while (codel_time_after_eq(now, vars->drop_next) && vars->dropping) {
                vars->count++;

                /* Сбрасываем пакет */
                qdisc_drop(skb, sch);

                /* Извлекаем следующий пакет */
                skb = __qdisc_dequeue_head(&sch->q);
                if (!skb) {
                    vars->dropping = false;
                    return NULL;
                }

                /* Проверяем новый пакет */
                drop = codel_should_drop(skb, vars, p, now);
                if (!drop) {
                    vars->dropping = false;
                    break;
                }

                /* Вычисляем время следующего сброса */
                vars->drop_next = control_law(vars->drop_next,
                                              p->interval,
                                              vars->count);
            }
        }
    } else if (drop) {
        /*
         * Мы НЕ в режиме сброса, но пакет нужно сбросить.
         * Входим в режим сброса.
         */
        qdisc_drop(skb, sch);

        skb = __qdisc_dequeue_head(&sch->q);
        if (!skb) {
            vars->dropping = false;
            return NULL;
        }

        vars->dropping = true;

        /*
         * Если мы недавно были в режиме сброса, «вспоминаем»
         * count, чтобы не начинать с нуля. Это ускоряет реакцию
         * на повторную перегрузку.
         */
        if (codel_time_before(now - vars->drop_next,
                              16 * p->interval)) {
            vars->count = max(vars->count - 1, 1U);
        } else {
            vars->count = 1;
        }

        vars->drop_next = control_law(now, p->interval, vars->count);
    }

    return skb;
}
```

**Почему CoDel работает без ручной настройки:**

1. **target = 5 мс** — универсально хорошее значение. Это достаточно мало для интерактивных приложений и достаточно велико, чтобы не мешать burst-ам.
2. **interval = 100 мс** — типичный worst-case RTT. CoDel ждёт interval перед началом сброса, чтобы не реагировать на кратковременные burst-ы.
3. **1/sqrt(count)** — автоматически адаптируется к количеству потоков и степени перегрузки.

### FQ_CoDel — Fair Queuing + CoDel

FQ_CoDel (Fair Queue Controlled Delay) — комбинация **fair queuing** и **CoDel**, являющаяся **дефолтным qdisc в Linux** с ядра 3.6.

```
Архитектура FQ_CoDel:

                     Входящие пакеты
                          │
                          ▼
                  ┌───────────────┐
                  │  Классификация │
                  │  (hash flow)  │
                  └───────┬───────┘
                          │
            Хэш по (src_ip, dst_ip, src_port, dst_port, proto)
                          │
        ┌─────────────────┼─────────────────┐
        ▼                 ▼                 ▼
    ┌───────┐         ┌───────┐         ┌───────┐
    │Queue 0│         │Queue 1│   ...   │Queue  │
    │(CoDel)│         │(CoDel)│         │ 1023  │
    │       │         │       │         │(CoDel)│
    └───┬───┘         └───┬───┘         └───┬───┘
        │                 │                 │
        └─────────────────┼─────────────────┘
                          │
                  ┌───────────────┐
                  │  Round-Robin  │  ← DRR (Deficit Round Robin)
                  │  планировщик  │
                  └───────┬───────┘
                          │
                          ▼
                   Исходящий линк
```

**Структура данных FQ_CoDel в ядре:**

```c
/*
 * Из net/sched/sch_fq_codel.c
 *
 * Основная структура данных FQ_CoDel.
 */
struct fq_codel_sched_data {
    struct tcf_proto __rcu *filter_list; /* Список фильтров tc */
    struct tcf_block *block;

    u32 flows_cnt;           /* Количество потоков (очередей).
                              * По умолчанию: 1024.
                              * Каждый поток — отдельная очередь
                              * с собственным экземпляром CoDel. */

    u32 quantum;             /* Квант DRR: сколько байт из каждого потока
                              * отправляется за один round-robin цикл.
                              * По умолчанию: MTU (1514 байт). */

    u32 drop_batch_size;     /* Пакетов за один drop-цикл (по умолчанию: 64) */
    u32 memory_limit;        /* Лимит общей памяти (по умолчанию: 32 МБ) */

    struct codel_params cparams; /* Параметры CoDel (target, interval) */

    u32 perturbation;        /* Случайный ключ для хэш-функции.
                              * Перемешивается периодически, чтобы избежать
                              * постоянных коллизий. */

    struct fq_codel_flow *flows;  /* Массив из flows_cnt структур потоков */
    u32 *backlogs;               /* Размер backlog каждого потока */

    /*
     * Два списка для планирования:
     * new_flows — потоки, начавшие передачу недавно (приоритет!)
     * old_flows — потоки, передающие давно
     *
     * Новые потоки получают приоритет: их пакеты обслуживаются
     * раньше. Это критично для коротких HTTP-запросов и DNS.
     */
    struct list_head new_flows;
    struct list_head old_flows;
};

/*
 * Структура одного потока (очереди) внутри FQ_CoDel.
 */
struct fq_codel_flow {
    struct sk_buff *head;        /* Голова очереди пакетов */
    struct sk_buff *tail;        /* Хвост очереди */
    struct list_head flowchain;  /* Связь в списке new_flows/old_flows */
    int deficit;                 /* Дефицит DRR (байт) */
    struct codel_vars cvars;     /* Состояние CoDel для ЭТОГО потока */
    u32 dropped;                 /* Счётчик сброшенных пакетов */
};
```

**Как FQ_CoDel решает проблемы:**

```
1. Fair Queuing (1024 хэш-корзины):
   - Каждый TCP-поток попадает в свою очередь.
   - Один «жирный» bulk-поток не может вытеснить тонкие
     интерактивные потоки (SSH, DNS, VoIP).
   - DRR гарантирует, что каждый поток получает справедливую
     долю полосы.

2. CoDel на каждую очередь:
   - Каждый поток имеет свой экземпляр CoDel.
   - Bufferbloat контролируется per-flow, а не глобально.

3. Приоритет новых потоков:
   - Новые потоки (new_flows) обслуживаются раньше старых.
   - Короткий HTTP-запрос не ждёт в очереди за bulk-загрузкой.

Результат: FQ_CoDel одновременно обеспечивает:
  ✓ Низкую задержку (CoDel target ≈ 5 мс)
  ✓ Fairness между потоками
  ✓ Приоритет коротких потоков
  ✓ Без ручной настройки
```

### CAKE — Common Applications Kept Enhanced

CAKE (разработан Dave Täht и Jonathan Morton, в ядре с 5.x) — развитие идей FQ_CoDel, оптимизированное для **домашних маршрутизаторов и edge-устройств**.

**Чем CAKE лучше FQ_CoDel:**

```
1. NAT-aware (распознаёт потоки за NAT):
   FQ_CoDel хэширует по (src_ip, dst_ip, src_port, dst_port).
   За NAT все внутренние хосты имеют один src_ip — все попадают
   в одну корзину!

   CAKE использует host-fairness (Triple Isolate):
   сначала делит полосу между хостами, затем между потоками хоста.

2. ATM/DOCSIS compensation:
   DSL-линки используют ATM-инкапсуляцию, которая добавляет
   overhead (AAL5 padding до 48 байт). FQ_CoDel не знает об этом
   и недооценивает реальный размер пакета → bufferbloat остаётся.
   CAKE корректно учитывает ATM/DOCSIS overhead.

3. Встроенный shaping:
   CAKE включает собственный шейпер — не нужно комбинировать
   с HTB/TBF. Один tc-вызов вместо сложной иерархии.

4. 8 приоритетных тинов (tins):
   CAKE может классифицировать трафик по приоритетам
   (Diffserv-aware), выделяя больше полосы для интерактивного
   трафика и меньше для bulk.
```

**Triple Isolate — ключевая инновация CAKE:**

```
Triple Isolate:
  1. Внешняя изоляция: полоса делится поровну между внутренними хостами
  2. Внутренняя изоляция: полоса каждого хоста делится между его потоками
  3. Приоритетная изоляция: tins (Best Effort, Bulk, Voice) изолированы

Пример:
  Линк: 100 Mbps, NAT, 3 внутренних хоста:
    Host A: 1 торрент (100 потоков) + VoIP
    Host B: 5 HTTP-загрузок
    Host C: 1 видеозвонок

  FQ_CoDel: торрент A захватывает ~95 Mbps (100 из 106 потоков).
  CAKE:     каждый хост получает ~33 Mbps,
            внутри каждого хоста — fair queuing.
            VoIP Host A не страдает от его же торрента.
```

**Практическая настройка CAKE:**

```bash
# ─── Типичная настройка CAKE для домашнего маршрутизатора ───

# Upstream (upload): ограничиваем до 95% от реальной скорости аплинка
# (см. Часть 2.6 — почему 95%)
tc qdisc replace dev eth0 root cake \
    bandwidth 95mbit \         # Ограничение скорости
    nat \                      # NAT-aware: учитывать внутренние IP
    wash \                     # Очищать DSCP-маркировку от пользователей
    diffserv4 \                # 4 приоритетных класса
    atm \                      # ATM-компенсация (для DSL)
    overhead 44 \              # ATM overhead в байтах
    ack-filter \               # Фильтрация избыточных TCP ACK
    split-gso                  # Разделение GSO-суперпакетов

# Downstream (download): на интерфейсе к LAN, ingress shaping
tc qdisc replace dev ifb0 root cake \
    bandwidth 450mbit \
    nat \
    wash \
    diffserv4 \
    ingress                    # Режим ingress

# ─── Проверка статистики CAKE ───
tc -s qdisc show dev eth0

# Пример вывода:
# qdisc cake 8001: root refcnt 2 bandwidth 95Mbit diffserv4 nat wash
#  Sent 1234567890 bytes 987654 pkt (dropped 12, overlimits 345 requeues 0)
#  backlog 0b 0p requeues 0
#  memory used: 234567b of 4Mb
#  capacity estimate: 95Mbit
#
#                 Bulk  Best Effort    Video    Voice
#   thresh      5Mbit      85Mbit   90Mbit   95Mbit
#   target       5ms         5ms      5ms      5ms
#   interval    100ms       100ms    100ms    100ms
#   pk_delay     2ms         1ms      0ms      0ms
#   av_delay     0ms         0ms      0ms      0ms
#   sp_delay     0ms         0ms      0ms      0ms
#   pkts         1234        5678     910      112
#   bytes      567890     2345678   45678     5678
#   way_inds        0           2       0        0
#   way_miss        1           5       1        0
#   way_cols        0           0       0        0
#   drops           8           3       1        0
#   marks           2          10       0        0
#   ack_drop        5           0       0        0
#   sp_flows        0           1       0        0
#   bk_flows        3           2       1        1
#   un_flows        0           0       0        0
#   max_len      1514        1514    1000      200
```

---

## Часть 2.6: Искусственное ограничение скорости (Shaping)

### Перемещение bottleneck на ваш «умный» qdisc

Центральная идея шейпинга в контексте AQM:

```
Без шейпинга:
  Bottleneck = ISP-оборудование (неконтролируемый буфер!)
  ┌────────┐     ┌─────────────┐     ┌────────┐
  │ Ваш    │────►│ ISP router  │────►│Internet│
  │ сервер │     │ (буфер 10МБ)│     │        │
  └────────┘     │ Drop Tail!  │     └────────┘
                 └─────────────┘
                       ↑
                Вы НЕ контролируете этот буфер.
                Он создаёт bufferbloat.

С шейпингом:
  Bottleneck = ваш qdisc (CAKE/FQ_CoDel)!
  ┌────────┐  ┌──────────┐  ┌─────────────┐  ┌────────┐
  │ Ваш    │─►│ CAKE     │─►│ ISP router  │─►│Internet│
  │ сервер │  │ 95% rate │  │ (буфер пуст)│  │        │
  └────────┘  │ AQM!     │  │             │  └────────┘
              └──────────┘  └─────────────┘
                   ↑               ↑
              Вы контролируете  Буфер ISP ПУСТ,
              этот qdisc!       т.к. мы шлём
              FQ + CoDel +      медленнее, чем
              shaping работают  линк может
              здесь.            передать.
```

Ограничивая скорость отправки ниже ёмкости ISP-линка, мы **перемещаем bottleneck** с неконтролируемого буфера ISP на наш qdisc, где работает AQM.

### Почему 95%, а не 100%

```
Почему мы ограничиваем скорость до 95% (а не 99% или 100%)?

1. Непостоянство скорости ISP-линка:
   Реальная скорость DSL/кабеля колеблется ±5% из-за
   перекрёстных помех, температуры, нагрузки на DSLAM.
   Если мы шейпим на 100% от номинала, а линк просел на 2%,
   наш шейпер пропускает больше, чем может ISP → буфер ISP
   начинает заполняться → bufferbloat возвращается.

2. Overhead протоколов:
   ATM-инкапсуляция, PPPoE-заголовки, VLAN-теги — всё это
   занимает полосу, невидимую для tc. Даже с overhead-компенсацией
   CAKE возможна погрешность.

3. Burst допуск:
   5% запас позволяет кратковременным burst-ам (TCP ACK storm,
   ARP) не вызывать заполнение ISP-буфера.

Практическое правило:
  - DSL/Cable: шейпить на 90–95% от измеренной скорости
  - FTTH (оптика): 95–98% (более стабильная скорость)
  - Ethernet (датацентр): 98–99% (стабильный линк)

Как измерить реальную скорость:
  # Несколько тестов в разное время:
  speedtest-cli --simple
  # Или:
  iperf3 -c speedtest.server.com -t 30 -P 4
  # Взять минимальное значение из серии тестов.
```

### Симбиоз BBR + AQM

BBR и AQM (CoDel/FQ_CoDel/CAKE) работают на **разных уровнях**, но **синергетически**:

```
                        Где работает
                        ────────────
  BBR:    На отправителе (sender-side congestion control)
          Контролирует pacing_rate и cwnd.
          Стремится к inflight = BDP.

  AQM:    На промежуточном узле (router/qdisc)
          Контролирует очередь.
          Сигнализирует о перегрузке (drop/ECN).

Синергия:
  1. BBR минимизирует заполнение буферов (pacing + BDP targeting).
  2. AQM ловит случаи, когда BBR (или другие потоки) всё же
     заполняют буфер → быстро сигнализирует.
  3. FQ компонент AQM изолирует потоки → один «сломанный»
     поток не убивает остальные.

Оптимальная комбинация:
  - Сервер: BBR v2 + fq qdisc
  - Домашний маршрутизатор: CAKE (shaping + AQM + FQ)
  - Датацентр: DCTCP + ECN (или BBR v2 + fq)

Антипаттерн:
  - CUBIC + Drop Tail = максимальный bufferbloat
  - BBR v1 + pfifo_fast = BBR без pacing (burst-ы!)
```

---

## Case Study: Бэкап Амстердам → Нью-Йорк

### Условия задачи

```
Задача: передать 500 ГБ данных из Амстердам в Нью-Йорк.

Параметры линка:
  Bandwidth:  10 Gbps (выделенный канал)
  RTT:        80 мс (типичный для AMS ↔ NYC)
  Потери:     0.1% (случайные, не congestion-related)

BDP = 10 Gbps × 80 мс = 10 × 10⁹ бит/с × 0.08 с
    = 800 000 000 бит = 100 000 000 байт = 100 МБ

Время передачи при полной утилизации:
  500 ГБ / 10 Gbps = 500 × 8 / 10 = 400 секунд ≈ 6.7 минут

Вопрос: какую реальную скорость мы получим?
```

### Сценарий 1: CUBIC (дефолтный)

```
CUBIC с 0.1% потерь на длинном RTT:

Формула Mathis для TCP throughput:
                    MSS × C
  Throughput ≈ ──────────────
                RTT × √(loss)

  где C ≈ 1.22 (константа для стандартного TCP)

  MSS = 1460 байт = 11680 бит
  RTT = 80 мс = 0.08 с
  loss = 0.001 (0.1%)

               11680 × 1.22
  Throughput ≈ ────────────── = 564 365 бит/с ≈ 564 Kbps (!!!)
               0.08 × √0.001

Подождите, это для Reno. CUBIC лучше, но не кардинально:

Для CUBIC с потерями формула сложнее, но эмпирически:
  - При 0.1% потерь на 10 Gbps × 80 мс линке
  - CUBIC достигает ~300–500 Mbps (3–5% от ёмкости)
  - Время передачи: 500 ГБ / 400 Mbps ≈ 2.8 часа

Почему так плохо?

1. CUBIC реагирует на КАЖДУЮ потерю снижением cwnd.
   При 0.1% потерь и cwnd = 100 МБ потери происходят
   примерно каждые 1000 пакетов.

2. После каждой потери: cwnd *= 0.7
   Восстановление до W_max на линке 80 мс RTT занимает
   десятки секунд (кубическая функция медленна вокруг W_max).

3. Средний cwnd ≈ 20–30 МБ (20–30% от BDP).
   → Линк утилизирован на 20–30%.

Пилообразная диаграмма cwnd (CUBIC):
  cwnd
  ▲
100МБ├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ BDP
  │                    ╱╲
  │                  ╱    ╲  loss!
 70МБ│              ╱      ╲
  │            ╱╱╱╱╱        ╲
  │         ╱╱╱              ╲
 50МБ│     ╱╱                  ╲
  │    ╱╱                      ╲  loss!
 35МБ│ ╱╱                       ╲
  │╱╱                            ╲╱╱╱╱╱...
  ├────────────────────────────────────────► время
  │    ~30 сек      ~15 сек

  Средний cwnd ≈ 40–50 МБ → ~400 Mbps
```

### Сценарий 2: BBR

```
BBR с 0.1% потерь на длинном RTT:

BBR НЕ РЕАГИРУЕТ на потери напрямую (BBR v1).
Он ориентируется на delivery_rate и RTprop.

1. STARTUP: BBR быстро обнаруживает BtlBw = 10 Gbps за ~6 RTT
   (6 × 80 мс = 480 мс).

2. DRAIN: осушает очередь за 1–2 RTT (160 мс).

3. PROBE_BW: устанавливает pacing_rate ≈ 10 Gbps.
   cwnd ≈ 2 × BDP = 200 МБ.

   0.1% потерь? BBR видит delivery_rate ≈ 9.2–9.8 Gbps
   (потери незначительно снижают effective delivery rate).

   BBR v1 ИГНОРИРУЕТ потери → поддерживает ~9.2 Gbps.
   (BBR v2 слегка снизит скорость, но останется выше 8 Gbps.)

Результат:
  BBR:    ~9.2 Gbps → 500 ГБ за ~7.3 минуты
  CUBIC:  ~400 Mbps → 500 ГБ за ~2.8 часа

  Ускорение: ~23x !!!

BBR cwnd:
  cwnd
  ▲
200МБ├── ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ 2×BDP
  │  ┌─────────────────────────────────────────────
  │  │                                    (стабильно!)
100МБ├──┤
  │  │ BDP
  │  │
  │  │
  ├──┴────────────────────────────────────────────► время
  │  │
  STARTUP+DRAIN (~0.7 сек)
```

### Диагностика с `ss -ti`

Утилита `ss` (socket statistics) — основной инструмент диагностики TCP в Linux. Флаг `-ti` показывает внутреннее состояние TCP-соединений:

```bash
# Запуск на отправителе (Амстердам) во время передачи:
ss -ti dst 198.51.100.1

# ─── Пример вывода для CUBIC ───
ESTAB 0 1234567
    cubic wscale:7,7 rto:280 rtt:80.5/0.3 ato:40 mss:1460
    pmtu:1500 rcvmss:1460 advmss:1460
    cwnd:2048 ssthresh:1820
    bytes_sent:214748364800 bytes_acked:214745678900
    bytes_retrans:2147483 segs_out:147034567 segs_in:73500000
    data_segs_out:147000000 data_segs_in:100
    delivery_rate 48125000bps          ← ~385 Mbps (далеко от 10 Gbps!)
    app_limited                        ← НЕТ (приложение шлёт на полную)
    busy:600000ms
    retrans:0/147000                   ← 147000 ретрансмиссий
    lost:15                            ← пакетов в состоянии lost
    pacing_rate 51200000bps            ← ~410 Mbps (pacing бесполезен
                                          без fq, но значение показывает)

# ─── Пример вывода для BBR ───
ESTAB 0 5678901
    bbr wscale:7,7 rto:280 rtt:80.2/0.1 ato:40 mss:1460
    pmtu:1500 rcvmss:1460 advmss:1460
    cwnd:85000 ssthresh:68000
    bytes_sent:500000000000 bytes_acked:499999500000
    bytes_retrans:500000 segs_out:342465753 segs_in:171230000
    data_segs_out:342460000 data_segs_in:100
    delivery_rate 1150000000bps        ← ~9.2 Gbps (≈92% линка!)
    app_limited                        ← НЕТ
    busy:440000ms
    retrans:0/342460                   ← ретрансмиссии есть, но BBR не снижает
    pacing_rate 1187500000bps          ← ~9.5 Gbps (pacing_gain × BtlBw)
    bbr:(bw:1150000000bps,mrtt:80.1,   ← BBR-specific: BtlBw и min_rtt
         pacing_gain:1.25,
         cwnd_gain:2)
```

**Ключевые поля для диагностики:**

```
┌────────────────┬───────────────────────────────────────────────┐
│ Поле           │ Что означает                                  │
├────────────────┼───────────────────────────────────────────────┤
│ delivery_rate  │ Фактическая скорость доставки данных.         │
│                │ Если << пропускной способности линка →        │
│                │ проблема с congestion control или потерями.   │
├────────────────┼───────────────────────────────────────────────┤
│ pacing_rate    │ Скорость pacing (только с BBR/fq).            │
│                │ Должна быть ≈ delivery_rate × pacing_gain.    │
├────────────────┼───────────────────────────────────────────────┤
│ app_limited    │ Если «да» — приложение не шлёт данные         │
│                │ достаточно быстро. Bottleneck НЕ в сети,      │
│                │ а в приложении/диске/CPU.                     │
├────────────────┼───────────────────────────────────────────────┤
│ cwnd           │ Congestion window (в MSS-сегментах).          │
│                │ Для BDP=100МБ: cwnd должен быть ≈ 68493       │
│                │ (100МБ / 1460).                               │
├────────────────┼───────────────────────────────────────────────┤
│ rtt            │ Smoothed RTT / RTT variance.                  │
│                │ Если RTT >> base RTT → очереди заполнены.     │
├────────────────┼───────────────────────────────────────────────┤
│ retrans        │ current_retrans / total_retrans.              │
│                │ Высокое значение при CUBIC → cwnd collapse.   │
│                │ При BBR — допустимо (BBR толерантен).         │
├────────────────┼───────────────────────────────────────────────┤
│ bbr:(bw,mrtt)  │ Только для BBR: текущие оценки BtlBw и       │
│                │ RTprop. Если bw << ёмкости линка → проблема   │
│                │ с измерением или app_limited.                 │
└────────────────┴───────────────────────────────────────────────┘
```

**Алгоритм диагностики:**

```
1. delivery_rate << ёмкости линка?
   ├── Да → app_limited?
   │        ├── Да → Bottleneck в приложении (диск? CPU? sendfile?)
   │        └── Нет → Продолжаем ↓
   └── Нет → Всё в порядке.

2. Какой алгоритм? (первое поле после ESTAB)
   ├── cubic → cwnd маленький? ssthresh маленький?
   │           → Потери! Переключитесь на BBR.
   └── bbr   → bw маленький? mrtt >> ожидаемого?
               → Проблема с маршрутом или AQM слишком агрессивен.

3. retrans высокий?
   ├── При CUBIC → это ПРИЧИНА низкого throughput.
   └── При BBR   → это СЛЕДСТВИЕ, не причина. Проверьте delivery_rate.

4. rtt >> ожидаемого? (например, 800 мс вместо 80 мс)
   → Bufferbloat! Установите AQM (CAKE/FQ_CoDel) на bottleneck.
```

---

## Практическое задание

### Требования к лабораторной среде

Все задания выполняются в лабораторной среде из Module 0 (Lab-Setup.md). Минимальная конфигурация: 3 network namespace, связанных veth-парами с управляемыми задержками и потерями через `tc netem`.

```
┌──────────┐     10 Gbps     ┌──────────┐     10 Gbps     ┌──────────┐
│  ns-src  │◄───────────────►│ ns-router│◄───────────────►│  ns-dst  │
│(отправ.) │  veth-sr        │(маршрут.)│  veth-rd        │(получат.)│
│          │                 │ netem:   │                 │          │
│          │                 │ delay,   │                 │          │
│          │                 │ loss,    │                 │          │
│          │                 │ rate     │                 │          │
└──────────┘                 └──────────┘                 └──────────┘
```

---

### Задание 1: BBR v2 vs CUBIC — гонка на трансатлантическом линке

**Цель:** Воспроизвести результаты Case Study и измерить разницу между CUBIC и BBR.

```bash
# ─── Подготовка ───

# В ns-router: эмулируем трансатлантический линк
ip netns exec ns-router tc qdisc replace dev veth-rd root netem \
    delay 40ms \        # 40 мс в одну сторону (80 мс RTT)
    loss 0.1% \         # 0.1% случайных потерь
    rate 10gbit         # 10 Gbps

# В ns-dst: запускаем iperf3 сервер
ip netns exec ns-dst iperf3 -s -p 5201

# ─── Тест 1: CUBIC ───
ip netns exec ns-src sysctl -w net.ipv4.tcp_congestion_control=cubic
ip netns exec ns-src iperf3 -c <dst_ip> -p 5201 -t 120 -C cubic

# Запишите: средний throughput, retransmissions, RTT.
# Ожидаемый результат: 300–500 Mbps.

# ─── Тест 2: BBR ───
ip netns exec ns-src sysctl -w net.ipv4.tcp_congestion_control=bbr
# ВАЖНО: установите fq qdisc для pacing!
ip netns exec ns-src tc qdisc replace dev veth-sr root fq
ip netns exec ns-src iperf3 -c <dst_ip> -p 5201 -t 120 -C bbr

# Запишите: средний throughput, retransmissions, RTT.
# Ожидаемый результат: 8–9.5 Gbps.

# ─── Тест 3: BBR без fq ───
ip netns exec ns-src tc qdisc replace dev veth-sr root pfifo_fast
ip netns exec ns-src iperf3 -c <dst_ip> -p 5201 -t 120 -C bbr

# Вопрос: как отсутствие fq влияет на BBR throughput?
# Наблюдайте за retransmissions — они должны вырасти (burst-ы!).
```

**Вопросы для анализа:**
1. Во сколько раз BBR быстрее CUBIC при 0.1% потерь?
2. Как изменится картина при 1% потерь? При 0.01%?
3. Как fq влияет на BBR: throughput, retransmissions, RTT jitter?

---

### Задание 2: CAKE — бинарный поиск оптимальной скорости шейпинга

**Цель:** Найти оптимальное значение `bandwidth` для CAKE, при котором bufferbloat минимизируется, а throughput максимизируется.

```bash
# ─── Подготовка ───

# Эмулируем ISP-линк 100 Mbps с большим буфером (bufferbloat!)
ip netns exec ns-router tc qdisc replace dev veth-rd root netem \
    delay 10ms \
    rate 100mbit \
    limit 1000          # Большой буфер: 1000 пакетов ≈ 1.5 МБ

# ─── Методика бинарного поиска ───

# Шаг 1: Измерьте базовый RTT без нагрузки
ip netns exec ns-src ping -c 10 <dst_ip>
# Запишите: min RTT = base_rtt (ожидаем ~20 мс)

# Шаг 2: Запустите bulk-поток БЕЗ CAKE и измерьте RTT под нагрузкой
ip netns exec ns-src tc qdisc replace dev veth-sr root pfifo_fast
# В одном терминале: bulk-поток
ip netns exec ns-src iperf3 -c <dst_ip> -t 60
# В другом: ping
ip netns exec ns-src ping -c 60 <dst_ip>
# Запишите: RTT под нагрузкой (ожидаем ~200+ мс — bufferbloat!)

# Шаг 3: Бинарный поиск bandwidth для CAKE
# Начинаем с low=50, high=100 (Mbps)

for BW in 95 90 85 80 75; do
    echo "=== Testing bandwidth=${BW}mbit ==="
    ip netns exec ns-src tc qdisc replace dev veth-sr root cake \
        bandwidth ${BW}mbit nat diffserv4

    # Одновременно: bulk + ping
    ip netns exec ns-src iperf3 -c <dst_ip> -t 30 &
    ip netns exec ns-src ping -c 30 <dst_ip>
    wait

    echo "=== Throughput и RTT записаны для ${BW}mbit ==="
done

# Заполните таблицу:
# ┌──────────┬────────────┬──────────┬──────────────┐
# │ CAKE BW  │ Throughput │ Avg RTT  │ RTT increase │
# │ (Mbps)   │ (Mbps)     │ (мс)    │ vs base      │
# ├──────────┼────────────┼──────────┼──────────────┤
# │ Без CAKE │ ~95        │ ~200+    │ +180 мс      │
# │ 95       │ ~90        │ ~25      │ +5 мс        │
# │ 90       │ ~85        │ ~22      │ +2 мс        │
# │ 85       │ ~80        │ ~21      │ +1 мс        │
# │ ...      │ ...        │ ...      │ ...          │
# └──────────┴────────────┴──────────┴──────────────┘
#
# Найдите точку, где RTT increase < 5 мс при максимальном throughput.
```

**Вопросы для анализа:**
1. Какое значение `bandwidth` оптимально (trade-off throughput vs latency)?
2. Что происходит, если `bandwidth` > реальной скорости линка?
3. Как CAKE ведёт себя при значении `bandwidth` ровно 100 Mbps (= скорость линка)?

---

### Задание 3: Анализ статистики CAKE

**Цель:** Научиться читать и интерпретировать вывод `tc -s qdisc show`.

```bash
# ─── Подготовка ───

# Настройте CAKE с рабочей конфигурацией из Задания 2
ip netns exec ns-src tc qdisc replace dev veth-sr root cake \
    bandwidth 90mbit nat wash diffserv4

# Запустите смешанную нагрузку (одновременно):
# Терминал 1: bulk-поток (торрент-симуляция)
ip netns exec ns-src iperf3 -c <dst_ip> -t 120 -P 10

# Терминал 2: интерактивный трафик (ping)
ip netns exec ns-src ping -c 120 -i 0.1 <dst_ip>

# Терминал 3: короткие HTTP-подобные потоки
for i in $(seq 1 100); do
    ip netns exec ns-src iperf3 -c <dst_ip> -t 1 -p 5202 &
    sleep 1
done

# ─── Сбор статистики ───

# Каждые 10 секунд:
watch -n 10 "ip netns exec ns-src tc -s qdisc show dev veth-sr"

# Запишите и проанализируйте следующие метрики по каждому тину:
# - drops vs marks: ECN-маркировка vs отбрасывание
# - sp_flows (sparse flows): количество коротких потоков
# - bk_flows (bulk flows): количество длинных потоков
# - pk_delay / av_delay / sp_delay: пиковая / средняя /
#   задержка для sparse-потоков
```

**Вопросы для анализа:**
1. В какой тин (Bulk, Best Effort, Video, Voice) попадает iperf3-трафик?
2. Каково соотношение drops vs marks? Почему?
3. Как sp_delay (задержка sparse-потоков) соотносится с av_delay (средняя задержка)?
4. Что происходит с pk_delay при увеличении количества параллельных bulk-потоков с 1 до 10?

---

### Задание 4: FQ_CoDel vs CAKE с NAT

**Цель:** Продемонстрировать проблему FQ_CoDel с NAT-трафиком и показать, как CAKE решает её.

```bash
# ─── Подготовка: эмуляция NAT ───

# Создаём 3 «внутренних» namespace за NAT:
ip netns add ns-lan1
ip netns add ns-lan2
ip netns add ns-lan3

# ns-router выполняет роль NAT-маршрутизатора
# (все внутренние хосты выходят через один IP)
ip netns exec ns-router iptables -t nat -A POSTROUTING \
    -o veth-rd -j MASQUERADE
ip netns exec ns-router sysctl -w net.ipv4.ip_forward=1

# Ограничиваем исходящий линк до 50 Mbps
ip netns exec ns-router tc qdisc replace dev veth-rd root netem \
    delay 5ms rate 50mbit limit 1000

# ─── Тест 1: FQ_CoDel ───
ip netns exec ns-router tc qdisc replace dev veth-rd root fq_codel

# Запускаем:
# ns-lan1: 20 параллельных потоков (симуляция торрента)
ip netns exec ns-lan1 iperf3 -c <dst_ip> -t 60 -P 20 &

# ns-lan2: 1 поток (обычный пользователь)
ip netns exec ns-lan2 iperf3 -c <dst_ip> -t 60 -p 5202 &

# ns-lan3: ping (интерактивный пользователь)
ip netns exec ns-lan3 ping -c 60 <dst_ip>

# Запишите throughput каждого хоста и RTT ping.
# Ожидание: lan1 получает ~45 Mbps, lan2 ~2-3 Mbps (unfair!)

# ─── Тест 2: CAKE с NAT ───
ip netns exec ns-router tc qdisc replace dev veth-rd root cake \
    bandwidth 48mbit nat wash diffserv4

# Повторите тот же тест.
# Ожидание: ~16 Mbps на каждый хост (fair!), RTT < 10 мс.
```

**Вопросы для анализа:**
1. Какое распределение полосы между хостами при FQ_CoDel? Почему?
2. Как CAKE с `nat` меняет распределение?
3. Что произойдёт, если убрать `nat` из конфигурации CAKE?
4. Как влияет `wash` на трафик, если ns-lan1 устанавливает DSCP-маркировку?

---

### Задание 5: eBPF-трассировка BBR state machine

**Цель:** Использовать eBPF (bpftrace) для наблюдения за переходами между состояниями BBR в реальном времени.

```bash
# ─── Требования ───
# - Ядро >= 5.x с CONFIG_BPF=y
# - Установлен bpftrace (apt install bpftrace)

# ─── Подготовка ───
ip netns exec ns-src sysctl -w net.ipv4.tcp_congestion_control=bbr
ip netns exec ns-src tc qdisc replace dev veth-sr root fq

# ─── eBPF-скрипт: трассировка BBR ───
# Сохраните как bbr_trace.bt:

cat > /tmp/bbr_trace.bt << 'EBPF'
#!/usr/bin/env bpftrace

/*
 * bbr_trace.bt — трассировка состояний BBR state machine.
 *
 * Отслеживает вызовы bbr_set_state() в ядре,
 * показывая переходы между STARTUP, DRAIN, PROBE_BW, PROBE_RTT.
 *
 * Использование: bpftrace bbr_trace.bt
 */

/* Определения состояний BBR (из net/ipv4/tcp_bbr.c) */
/* BBR_STARTUP = 0, BBR_DRAIN = 1, BBR_PROBE_BW = 2, BBR_PROBE_RTT = 3 */

/*
 * Точка трассировки: kprobe на bbr_main() — основная функция BBR,
 * вызываемая при каждом ACK.
 */
kprobe:bbr_main
{
    $sk = (struct sock *)arg0;
    $tp = (struct tcp_sock *)$sk;

    /*
     * Считываем BBR-состояние из icsk_ca_priv.
     * Смещение зависит от структуры — может потребоваться
     * корректировка для конкретной версии ядра.
     */
    $bbr = (uint8 *)($sk + sizeof(struct sock));
    $mode = *($bbr + offsetof(struct bbr, mode));

    /* Фильтруем по порту, чтобы видеть только наш поток */
    $sport = $tp->inet_conn.icsk_inet.inet_sport;
    $dport = $tp->inet_conn.icsk_inet.inet_dport;

    if ($dport == htons(5201)) {
        /*
         * Выводим: время, состояние, BtlBw, min_rtt, cwnd, inflight.
         */
        $cwnd = $tp->snd_cwnd;

        printf("%-12llu sport=%-5d mode=%-10s cwnd=%-8u\n",
            nsecs / 1000000,
            ntohs($sport),
            $mode == 0 ? "STARTUP" :
            $mode == 1 ? "DRAIN" :
            $mode == 2 ? "PROBE_BW" :
            $mode == 3 ? "PROBE_RTT" : "UNKNOWN",
            $cwnd);
    }
}

/*
 * Трассировка переходов PROBE_RTT.
 * PROBE_RTT — особенно интересен, т.к. он уменьшает cwnd до 4.
 */
kprobe:bbr_update_min_rtt
{
    $sk = (struct sock *)arg0;
    $tp = (struct tcp_sock *)$sk;
    $dport = $tp->inet_conn.icsk_inet.inet_dport;

    if ($dport == htons(5201)) {
        $cwnd = $tp->snd_cwnd;

        /* Если cwnd <= 4 — мы в PROBE_RTT */
        if ($cwnd <= 4) {
            printf("*** PROBE_RTT ACTIVE *** cwnd=%u\n", $cwnd);
        }
    }
}
EBPF

# ─── Запуск трассировки ───

# Терминал 1: запуск bpftrace (от root)
bpftrace /tmp/bbr_trace.bt

# Терминал 2: запуск iperf3
ip netns exec ns-src iperf3 -c <dst_ip> -t 120 -C bbr

# ─── Ожидаемый вывод ───
# 0            sport=44000 mode=STARTUP    cwnd=10
# 80           sport=44000 mode=STARTUP    cwnd=20
# 160          sport=44000 mode=STARTUP    cwnd=40
# 240          sport=44000 mode=STARTUP    cwnd=80
# 320          sport=44000 mode=DRAIN      cwnd=80
# 400          sport=44000 mode=DRAIN      cwnd=80
# 480          sport=44000 mode=PROBE_BW   cwnd=68493
# ...
# 10480        sport=44000 mode=PROBE_RTT  cwnd=4
# *** PROBE_RTT ACTIVE *** cwnd=4
# 10680        sport=44000 mode=PROBE_BW   cwnd=68493
# ...

# ─── Альтернатива: использование tcp_probe (устаревший, но простой) ───
# Для старых ядер без BPF:
modprobe tcp_probe port=5201
cat /proc/net/tcpprobe
# Формат: timestamp src dst length snd_nxt snd_una snd_cwnd ssthresh snd_wnd
```

**Вопросы для анализа:**
1. Сколько RTT занимает фаза STARTUP до перехода в DRAIN?
2. Как часто происходит PROBE_RTT? Совпадает ли с 10-секундным интервалом?
3. Какое значение cwnd в PROBE_BW? Соответствует ли 2 × BDP?
4. Как выглядит переход PROBE_BW → PROBE_RTT → PROBE_BW в терминах cwnd? Какое влияние на throughput?
5. Модифицируйте скрипт для отслеживания pacing_rate и delivery_rate. Как они соотносятся в каждом состоянии?

---

## Итоги модуля

```
Ключевые выводы:

1. Bufferbloat — корень всех зол задержки в Интернете.
   Формула: Delay = Buffer_Size / Link_Rate. Запомните навсегда.

2. CUBIC — надёжный, но слепой к задержке.
   Заполняет все буферы, создаёт пилу cwnd, медленно сходится
   на длинных линках с потерями.

3. BBR — paradigm shift. Ориентируется на BtlBw и RTprop.
   Требует fq qdisc для pacing. BBR v2 решает проблемы fairness.

4. DCTCP + ECN — для датацентров.
   Пропорциональная реакция на перегрузку. Не для Интернета.

5. AQM эволюция: Drop Tail → RED (failed) → CoDel → FQ_CoDel → CAKE.
   CAKE — лучший выбор для edge. FQ_CoDel — хороший дефолт.

6. Шейпинг перемещает bottleneck на ваш qdisc.
   95% от скорости линка. Без этого AQM бесполезен.

7. BBR + AQM = синергия. Используйте оба.

8. ss -ti — ваш главный инструмент диагностики TCP.
   delivery_rate, pacing_rate, app_limited — три кита.
```

```
Переход к следующему модулю:

  Модуль 3: TCP State Machine и жизненный цикл соединения.
  Разберём: 3-way handshake, TIME_WAIT, SYN flood,
  tcp_tw_reuse, и почему SO_LINGER — опасное оружие.
```

---

> *«Congestion control — это не алгоритм. Это общественный договор между миллиардами TCP-потоков о том, как делить конечный ресурс. BBR — первый алгоритм, который пытается делить его разумно, а не по принципу "кто громче кричит".»*
